{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5219a437",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "# PySpark SQL ä¿¡ç”¨å¡äº¤æ˜“è³‡æ–™åˆ†æå¯¦æˆ°\n",
    "\n",
    "ä½œè€…ï¼šQChoice AI æ•™å­¸åœ˜éšŠ\n",
    "æ—¥æœŸï¼š2025-01-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç’°å¢ƒè¨­å®šèˆ‡å¥—ä»¶è¼‰å…¥\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# å»ºç«‹ Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CreditCardAnalysis\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"./spark-warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"âœ… Sparkç‰ˆæœ¬: {spark.version}\")\n",
    "print(f\"âœ… ç’°å¢ƒè¨­å®šå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7056407e",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## ğŸ“š å–®å…ƒä¸€ï¼šå»ºç«‹ä¿¡ç”¨å¡äº¤æ˜“å‡è³‡æ–™\n",
    "\n",
    "### 1.1 ä½¿ç”¨è€…è³‡æ–™å»ºç«‹\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "ä½¿ç”¨ Numpy å’Œ Pandas å»ºç«‹ä½¿ç”¨è€…åŸºæœ¬è³‡æ–™ï¼ŒåŒ…å«ä½¿ç”¨è€…IDã€å§“åã€å¹´é½¡ã€è·æ¥­ç­‰è³‡è¨Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1657b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å®šéš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿çµæœå¯é‡ç¾\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# å»ºç«‹ä½¿ç”¨è€…è³‡æ–™\n",
    "n_users = 1000\n",
    "\n",
    "# å§“æ°å’Œåå­—åˆ—è¡¨ï¼ˆå°ç£å¸¸è¦‹å§“åï¼‰\n",
    "last_names = ['é™³', 'æ—', 'é»ƒ', 'å¼µ', 'æ', 'ç‹', 'å³', 'åŠ‰', 'è”¡', 'æ¥Š', 'è¨±', 'é„­', 'è¬', 'éƒ­', 'æ´ª']\n",
    "first_names = ['æ€¡å›', 'å¿—æ˜', 'é›…å©·', 'å»ºåœ‹', 'æ·‘èŠ¬', 'ä¿Šå‚‘', 'ç¾ç²', 'å®¶è±ª', 'è©©æ¶µ', 'å† å®‡', 'ä½³ç©', 'å®—ç¿°', 'ç­±æ¶µ', 'æ‰¿æ©', 'é›…ç­‘']\n",
    "\n",
    "users_data = {\n",
    "    'user_id': [f'U{str(i).zfill(6)}' for i in range(1, n_users + 1)],\n",
    "    'user_name': [random.choice(last_names) + random.choice(first_names) for _ in range(n_users)],\n",
    "    'age': np.random.randint(20, 70, n_users),\n",
    "    'gender': np.random.choice(['M', 'F'], n_users),\n",
    "    'occupation': np.random.choice(['ä¸Šç­æ—', 'å­¸ç”Ÿ', 'è‡ªç”±æ¥­', 'é€€ä¼‘', 'å®¶ç®¡', 'å…¬å‹™å“¡', 'é†«ç™‚', 'æ•™è‚²'], n_users),\n",
    "    'city': np.random.choice(['å°åŒ—', 'æ–°åŒ—', 'æ¡ƒåœ’', 'å°ä¸­', 'å°å—', 'é«˜é›„', 'æ–°ç«¹', 'åŸºéš†'], n_users),\n",
    "    'annual_income': np.random.randint(300000, 2000000, n_users)\n",
    "}\n",
    "\n",
    "users_df = pd.DataFrame(users_data)\n",
    "print(\"âœ… ä½¿ç”¨è€…è³‡æ–™å»ºç«‹å®Œæˆ\")\n",
    "users_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd1b09f",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 1.1: å°‡ Pandas DataFrame è½‰æ›ç‚º PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5cd670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è½‰æ›ç‚º PySpark DataFrame\n",
    "users_spark = spark.createDataFrame(users_df)\n",
    "\n",
    "# è¨»å†Šç‚ºè‡¨æ™‚è¡¨æ ¼\n",
    "users_spark.createOrReplaceTempView(\"users\")\n",
    "\n",
    "print(\"ğŸ“Š ä½¿ç”¨è€…è³‡æ–™è¡¨çµæ§‹ï¼š\")\n",
    "users_spark.printSchema()\n",
    "print(f\"\\nç¸½ä½¿ç”¨è€…æ•¸ï¼š{users_spark.count()}\")\n",
    "users_spark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9231a1",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 1.2 ä¿¡ç”¨å¡æŒå¡äººè³‡æ–™å»ºç«‹\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "æ¯ä½ä½¿ç”¨è€…å¯èƒ½æŒæœ‰å¤šå¼µä¿¡ç”¨å¡ï¼Œå»ºç«‹ä¿¡ç”¨å¡åŸºæœ¬è³‡è¨Šï¼ŒåŒ…å«å¡è™Ÿã€å¡åˆ¥ã€é¡åº¦ç­‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca45c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ä¿¡ç”¨å¡è³‡æ–™\n",
    "n_cards = 1500\n",
    "\n",
    "card_types = ['é‡‘å¡', 'ç™½é‡‘å¡', 'éˆ¦é‡‘å¡', 'æ™®å¡', 'å•†å‹™å¡']\n",
    "card_brands = ['VISA', 'MasterCard', 'JCB', 'American Express']\n",
    "banks = ['å°æ–°éŠ€è¡Œ', 'åœ‹æ³°ä¸–è¯', 'ä¸­ä¿¡éŠ€è¡Œ', 'ç‰å±±éŠ€è¡Œ', 'å¯Œé‚¦éŠ€è¡Œ', 'ç¬¬ä¸€éŠ€è¡Œ']\n",
    "\n",
    "# éš¨æ©Ÿåˆ†é…ä¿¡ç”¨å¡çµ¦ä½¿ç”¨è€…ï¼ˆæœ‰äº›ä½¿ç”¨è€…æœ‰å¤šå¼µå¡ï¼‰\n",
    "card_owners = np.random.choice(users_df['user_id'].values, n_cards)\n",
    "\n",
    "cards_data = {\n",
    "    'card_id': [f'C{str(i).zfill(8)}' for i in range(1, n_cards + 1)],\n",
    "    'user_id': card_owners,\n",
    "    'card_number': [f'{random.randint(1000, 9999)}-{random.randint(1000, 9999)}-{random.randint(1000, 9999)}-{random.randint(1000, 9999)}' for _ in range(n_cards)],\n",
    "    'card_type': np.random.choice(card_types, n_cards, p=[0.15, 0.25, 0.10, 0.40, 0.10]),\n",
    "    'card_brand': np.random.choice(card_brands, n_cards, p=[0.40, 0.35, 0.15, 0.10]),\n",
    "    'bank': np.random.choice(banks, n_cards),\n",
    "    'credit_limit': np.random.choice([50000, 100000, 200000, 300000, 500000, 800000], n_cards),\n",
    "    'issue_date': pd.to_datetime([datetime(2020, 1, 1) + timedelta(days=random.randint(0, 1825)) for _ in range(n_cards)]),\n",
    "    'card_status': np.random.choice(['active', 'suspended', 'closed'], n_cards, p=[0.85, 0.10, 0.05])\n",
    "}\n",
    "\n",
    "cards_df = pd.DataFrame(cards_data)\n",
    "print(\"âœ… ä¿¡ç”¨å¡è³‡æ–™å»ºç«‹å®Œæˆ\")\n",
    "cards_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b4e689",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 1.2: è½‰æ›ä¿¡ç”¨å¡è³‡æ–™ç‚º Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è½‰æ›ç‚º PySpark DataFrame\n",
    "cards_spark = spark.createDataFrame(cards_df)\n",
    "\n",
    "# è¨»å†Šç‚ºè‡¨æ™‚è¡¨æ ¼\n",
    "cards_spark.createOrReplaceTempView(\"cards\")\n",
    "\n",
    "print(\"ğŸ“Š ä¿¡ç”¨å¡è³‡æ–™è¡¨çµæ§‹ï¼š\")\n",
    "cards_spark.printSchema()\n",
    "print(f\"\\nç¸½ä¿¡ç”¨å¡æ•¸ï¼š{cards_spark.count()}\")\n",
    "cards_spark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b79e1d",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 1.3 ä¿¡ç”¨å¡åˆ·å¡äº¤æ˜“ç´€éŒ„å»ºç«‹\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "å»ºç«‹ä¿¡ç”¨å¡äº¤æ˜“æ˜ç´°ï¼ŒåŒ…å«äº¤æ˜“æ™‚é–“ã€é‡‘é¡ã€å•†åº—é¡åˆ¥ã€äº¤æ˜“ç‹€æ…‹ç­‰è³‡è¨Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a202c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹äº¤æ˜“è³‡æ–™\n",
    "n_transactions = 50000\n",
    "\n",
    "# åªä½¿ç”¨ç‹€æ…‹ç‚º active çš„ä¿¡ç”¨å¡\n",
    "active_cards = cards_df[cards_df['card_status'] == 'active']['card_id'].values\n",
    "\n",
    "# å•†åº—é¡åˆ¥\n",
    "merchant_categories = [\n",
    "    'è¶…å¸‚', 'é¤å»³', 'åŠ æ²¹ç«™', 'ç™¾è²¨å…¬å¸', 'ç¶²è³¼', \n",
    "    'é›»å½±é™¢', 'æ›¸åº—', 'è—¥å±€', 'å’–å•¡å»³', 'æœé£¾åº—',\n",
    "    '3Cè³£å ´', 'ä¾¿åˆ©å•†åº—', 'æ—…éŠ', 'é£¯åº—', 'é†«ç™‚'\n",
    "]\n",
    "\n",
    "# äº¤æ˜“åœ°é»\n",
    "locations = ['å°åŒ—', 'æ–°åŒ—', 'æ¡ƒåœ’', 'å°ä¸­', 'å°å—', 'é«˜é›„', 'æ–°ç«¹', 'åŸºéš†', 'åœ‹å¤–']\n",
    "\n",
    "# ç”Ÿæˆäº¤æ˜“ç´€éŒ„\n",
    "transactions_data = {\n",
    "    'transaction_id': [f'T{str(i).zfill(10)}' for i in range(1, n_transactions + 1)],\n",
    "    'card_id': np.random.choice(active_cards, n_transactions),\n",
    "    'transaction_date': pd.to_datetime([\n",
    "        datetime(2024, 1, 1) + timedelta(\n",
    "            days=random.randint(0, 365),\n",
    "            hours=random.randint(0, 23),\n",
    "            minutes=random.randint(0, 59)\n",
    "        ) for _ in range(n_transactions)\n",
    "    ]),\n",
    "    'merchant_name': [f\"{random.choice(merchant_categories)}{random.randint(1, 100)}è™Ÿåº—\" for _ in range(n_transactions)],\n",
    "    'merchant_category': np.random.choice(merchant_categories, n_transactions),\n",
    "    'amount': np.random.gamma(2, 500, n_transactions).astype(int),  # ä½¿ç”¨ Gamma åˆ†å¸ƒæ¨¡æ“¬çœŸå¯¦äº¤æ˜“é‡‘é¡\n",
    "    'location': np.random.choice(locations, n_transactions, p=[0.25, 0.20, 0.15, 0.15, 0.10, 0.08, 0.05, 0.01, 0.01]),\n",
    "    'transaction_status': np.random.choice(['approved', 'declined', 'pending'], n_transactions, p=[0.90, 0.08, 0.02]),\n",
    "    'is_online': np.random.choice([True, False], n_transactions, p=[0.35, 0.65])\n",
    "}\n",
    "\n",
    "transactions_df = pd.DataFrame(transactions_data)\n",
    "\n",
    "# ç¢ºä¿é‡‘é¡ç‚ºæ­£æ•¸ä¸”åˆç†\n",
    "transactions_df['amount'] = transactions_df['amount'].clip(lower=10, upper=100000)\n",
    "\n",
    "print(\"âœ… äº¤æ˜“è³‡æ–™å»ºç«‹å®Œæˆ\")\n",
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e97f2",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 1.3: è½‰æ›äº¤æ˜“è³‡æ–™ç‚º Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cb5776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è½‰æ›ç‚º PySpark DataFrame\n",
    "transactions_spark = spark.createDataFrame(transactions_df)\n",
    "\n",
    "# è¨»å†Šç‚ºè‡¨æ™‚è¡¨æ ¼\n",
    "transactions_spark.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "print(\"ğŸ“Š äº¤æ˜“è³‡æ–™è¡¨çµæ§‹ï¼š\")\n",
    "transactions_spark.printSchema()\n",
    "print(f\"\\nç¸½äº¤æ˜“ç­†æ•¸ï¼š{transactions_spark.count()}\")\n",
    "transactions_spark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522c9866",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## ğŸ“š å–®å…ƒäºŒï¼šSQL åŸºç¤æŸ¥è©¢ç·´ç¿’\n",
    "\n",
    "### 2.1 SELECT èˆ‡ WHERE å­å¥\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "ä½¿ç”¨ SELECT é¸æ“‡æ¬„ä½ï¼ŒWHERE é€²è¡Œæ¢ä»¶ç¯©é¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c869b79",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 2.1: æŸ¥è©¢å¹´é½¡å¤§æ–¼40æ­²çš„ä½¿ç”¨è€…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6489d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT user_id, user_name, age, city, occupation\n",
    "FROM users\n",
    "WHERE age > 40\n",
    "ORDER BY age DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f48bd0",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 2.2: æŸ¥è©¢å°åŒ—åœ°å€çš„é«˜æ”¶å…¥ä½¿ç”¨è€…ï¼ˆå¹´æ”¶å…¥è¶…é100è¬ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8af99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT user_id, user_name, age, city, annual_income\n",
    "FROM users\n",
    "WHERE city = 'å°åŒ—' AND annual_income > 1000000\n",
    "ORDER BY annual_income DESC\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a660471",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 2.2 èšåˆå‡½æ•¸èˆ‡ GROUP BY\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "ä½¿ç”¨èšåˆå‡½æ•¸ï¼ˆCOUNT, SUM, AVG, MAX, MINï¼‰æ­é… GROUP BY é€²è¡Œè³‡æ–™çµ±è¨ˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26604b52",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 2.3: çµ±è¨ˆå„åŸå¸‚çš„ä½¿ç”¨è€…æ•¸é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ae8edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    city,\n",
    "    COUNT(*) as user_count,\n",
    "    AVG(age) as avg_age,\n",
    "    AVG(annual_income) as avg_income\n",
    "FROM users\n",
    "GROUP BY city\n",
    "ORDER BY user_count DESC\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c3b64b",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 2.4: çµ±è¨ˆå„å•†åº—é¡åˆ¥çš„äº¤æ˜“ç¸½é¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e72c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    merchant_category,\n",
    "    COUNT(*) as transaction_count,\n",
    "    SUM(amount) as total_amount,\n",
    "    AVG(amount) as avg_amount,\n",
    "    MAX(amount) as max_amount\n",
    "FROM transactions\n",
    "WHERE transaction_status = 'approved'\n",
    "GROUP BY merchant_category\n",
    "ORDER BY total_amount DESC\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3170ec",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## ğŸ“š å–®å…ƒä¸‰ï¼šJOIN é€£æ¥æŸ¥è©¢\n",
    "\n",
    "### 3.1 INNER JOIN\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "INNER JOIN å–å¾—å…©å€‹è¡¨æ ¼çš„äº¤é›†è³‡æ–™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27996607",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 3.1: æŸ¥è©¢ä½¿ç”¨è€…çš„ä¿¡ç”¨å¡è³‡è¨Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c789a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    u.user_id,\n",
    "    u.user_name,\n",
    "    u.city,\n",
    "    c.card_id,\n",
    "    c.card_type,\n",
    "    c.card_brand,\n",
    "    c.credit_limit\n",
    "FROM users u\n",
    "INNER JOIN cards c ON u.user_id = c.user_id\n",
    "WHERE c.card_status = 'active'\n",
    "ORDER BY u.user_id\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957aba2f",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 3.2: æŸ¥è©¢äº¤æ˜“ç´€éŒ„èˆ‡æŒå¡äººè³‡è¨Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb8355",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    t.transaction_id,\n",
    "    t.transaction_date,\n",
    "    t.amount,\n",
    "    t.merchant_category,\n",
    "    c.card_type,\n",
    "    u.user_name,\n",
    "    u.city\n",
    "FROM transactions t\n",
    "INNER JOIN cards c ON t.card_id = c.card_id\n",
    "INNER JOIN users u ON c.user_id = u.user_id\n",
    "WHERE t.transaction_status = 'approved'\n",
    "ORDER BY t.transaction_date DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9787c138",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 3.2 LEFT JOIN\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "LEFT JOIN ä¿ç•™å·¦è¡¨æ‰€æœ‰è³‡æ–™ï¼Œå³è¡¨æ²’æœ‰å°æ‡‰å‰‡é¡¯ç¤º NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb9f307",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 3.3: æŸ¥è©¢æ‰€æœ‰ä½¿ç”¨è€…åŠå…¶ä¿¡ç”¨å¡æ•¸é‡ï¼ˆåŒ…å«æ²’æœ‰ä¿¡ç”¨å¡çš„ä½¿ç”¨è€…ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04202388",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    u.user_id,\n",
    "    u.user_name,\n",
    "    u.city,\n",
    "    COUNT(c.card_id) as card_count\n",
    "FROM users u\n",
    "LEFT JOIN cards c ON u.user_id = c.user_id\n",
    "GROUP BY u.user_id, u.user_name, u.city\n",
    "ORDER BY card_count DESC, u.user_id\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e69c5b4",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## ğŸ“š å–®å…ƒå››ï¼šé€²éš SQL æŸ¥è©¢\n",
    "\n",
    "### 4.1 å­æŸ¥è©¢ (Subquery)\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "å­æŸ¥è©¢æ˜¯åœ¨æŸ¥è©¢å…§éƒ¨å†é€²è¡Œå¦ä¸€å€‹æŸ¥è©¢ï¼Œå¯ç”¨æ–¼è¤‡é›œçš„æ¢ä»¶åˆ¤æ–·"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6d2b0",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 4.1: æŸ¥è©¢äº¤æ˜“é‡‘é¡é«˜æ–¼å¹³å‡å€¼çš„äº¤æ˜“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199d43e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    t.transaction_id,\n",
    "    t.transaction_date,\n",
    "    t.amount,\n",
    "    t.merchant_category,\n",
    "    u.user_name\n",
    "FROM transactions t\n",
    "INNER JOIN cards c ON t.card_id = c.card_id\n",
    "INNER JOIN users u ON c.user_id = u.user_id\n",
    "WHERE t.amount > (SELECT AVG(amount) FROM transactions)\n",
    "    AND t.transaction_status = 'approved'\n",
    "ORDER BY t.amount DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ac1052",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 4.2: æ‰¾å‡ºæ“æœ‰æœ€å¤šä¿¡ç”¨å¡çš„å‰10åä½¿ç”¨è€…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef64f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    u.user_id,\n",
    "    u.user_name,\n",
    "    u.occupation,\n",
    "    u.annual_income,\n",
    "    card_stats.card_count\n",
    "FROM users u\n",
    "INNER JOIN (\n",
    "    SELECT user_id, COUNT(*) as card_count\n",
    "    FROM cards\n",
    "    WHERE card_status = 'active'\n",
    "    GROUP BY user_id\n",
    ") card_stats ON u.user_id = card_stats.user_id\n",
    "ORDER BY card_stats.card_count DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d3482b",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 4.2 Window Functions (çª—å£å‡½æ•¸)\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "Window Functions å¯ä»¥åœ¨ä¸æ”¹è®Šçµæœé›†è¡Œæ•¸çš„æƒ…æ³ä¸‹é€²è¡Œèšåˆè¨ˆç®—"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd26459",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 4.3: è¨ˆç®—æ¯ä½ä½¿ç”¨è€…çš„ç´¯ç©æ¶ˆè²»é‡‘é¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e75625",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    u.user_name,\n",
    "    t.transaction_date,\n",
    "    t.amount,\n",
    "    t.merchant_category,\n",
    "    SUM(t.amount) OVER (\n",
    "        PARTITION BY u.user_id \n",
    "        ORDER BY t.transaction_date\n",
    "    ) as cumulative_amount\n",
    "FROM transactions t\n",
    "INNER JOIN cards c ON t.card_id = c.card_id\n",
    "INNER JOIN users u ON c.user_id = u.user_id\n",
    "WHERE t.transaction_status = 'approved'\n",
    "    AND u.user_id IN (SELECT user_id FROM users LIMIT 3)\n",
    "ORDER BY u.user_name, t.transaction_date\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0ad379",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 4.4: ç‚ºæ¯ä½ä½¿ç”¨è€…çš„äº¤æ˜“æ’åï¼ˆä¾é‡‘é¡ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87658f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    user_name,\n",
    "    transaction_date,\n",
    "    amount,\n",
    "    merchant_category,\n",
    "    rank\n",
    "FROM (\n",
    "    SELECT \n",
    "        u.user_name,\n",
    "        t.transaction_date,\n",
    "        t.amount,\n",
    "        t.merchant_category,\n",
    "        RANK() OVER (\n",
    "            PARTITION BY u.user_id \n",
    "            ORDER BY t.amount DESC\n",
    "        ) as rank\n",
    "    FROM transactions t\n",
    "    INNER JOIN cards c ON t.card_id = c.card_id\n",
    "    INNER JOIN users u ON c.user_id = u.user_id\n",
    "    WHERE t.transaction_status = 'approved'\n",
    ") ranked_transactions\n",
    "WHERE rank <= 5\n",
    "ORDER BY user_name, rank\n",
    "LIMIT 30\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b097dc0a",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## ğŸ“š å–®å…ƒäº”ï¼šè³‡æ–™åˆ†æå¯¦æˆ°\n",
    "\n",
    "### 5.1 æœˆåº¦æ¶ˆè²»åˆ†æ\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "ä½¿ç”¨æ—¥æœŸå‡½æ•¸é€²è¡Œæ™‚é–“åºåˆ—åˆ†æ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb24d7d",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 5.1: æ¯æœˆäº¤æ˜“çµ±è¨ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24b4364",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    YEAR(transaction_date) as year,\n",
    "    MONTH(transaction_date) as month,\n",
    "    COUNT(*) as transaction_count,\n",
    "    SUM(amount) as total_amount,\n",
    "    AVG(amount) as avg_amount\n",
    "FROM transactions\n",
    "WHERE transaction_status = 'approved'\n",
    "GROUP BY YEAR(transaction_date), MONTH(transaction_date)\n",
    "ORDER BY year, month\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53510b11",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 5.2: å„æœˆä»½ç†±é–€æ¶ˆè²»é¡åˆ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae641b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    YEAR(transaction_date) as year,\n",
    "    MONTH(transaction_date) as month,\n",
    "    merchant_category,\n",
    "    COUNT(*) as transaction_count,\n",
    "    SUM(amount) as total_amount\n",
    "FROM transactions\n",
    "WHERE transaction_status = 'approved'\n",
    "GROUP BY YEAR(transaction_date), MONTH(transaction_date), merchant_category\n",
    "ORDER BY year, month, total_amount DESC\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9cc291",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 5.2 ä½¿ç”¨è€…æ¶ˆè²»è¡Œç‚ºåˆ†æ\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "åˆ†æä½¿ç”¨è€…çš„æ¶ˆè²»æ¨¡å¼èˆ‡åå¥½"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7cba06",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 5.3: é«˜æ¶ˆè²»ä½¿ç”¨è€…åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2c7aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    u.user_id,\n",
    "    u.user_name,\n",
    "    u.age,\n",
    "    u.occupation,\n",
    "    u.city,\n",
    "    COUNT(DISTINCT t.transaction_id) as transaction_count,\n",
    "    SUM(t.amount) as total_spending,\n",
    "    AVG(t.amount) as avg_transaction_amount,\n",
    "    COUNT(DISTINCT t.merchant_category) as category_diversity\n",
    "FROM users u\n",
    "INNER JOIN cards c ON u.user_id = c.user_id\n",
    "INNER JOIN transactions t ON c.card_id = t.card_id\n",
    "WHERE t.transaction_status = 'approved'\n",
    "GROUP BY u.user_id, u.user_name, u.age, u.occupation, u.city\n",
    "HAVING SUM(t.amount) > 100000\n",
    "ORDER BY total_spending DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7279b8df",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 5.4: ä½¿ç”¨è€…æœ€å¸¸æ¶ˆè²»çš„é¡åˆ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199665dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH user_category_spending AS (\n",
    "    SELECT \n",
    "        u.user_id,\n",
    "        u.user_name,\n",
    "        t.merchant_category,\n",
    "        COUNT(*) as transaction_count,\n",
    "        SUM(t.amount) as category_spending,\n",
    "        RANK() OVER (\n",
    "            PARTITION BY u.user_id \n",
    "            ORDER BY COUNT(*) DESC\n",
    "        ) as category_rank\n",
    "    FROM users u\n",
    "    INNER JOIN cards c ON u.user_id = c.user_id\n",
    "    INNER JOIN transactions t ON c.card_id = t.card_id\n",
    "    WHERE t.transaction_status = 'approved'\n",
    "    GROUP BY u.user_id, u.user_name, t.merchant_category\n",
    ")\n",
    "SELECT \n",
    "    user_id,\n",
    "    user_name,\n",
    "    merchant_category as favorite_category,\n",
    "    transaction_count,\n",
    "    category_spending\n",
    "FROM user_category_spending\n",
    "WHERE category_rank = 1\n",
    "ORDER BY transaction_count DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52135ded",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 5.3 ç•°å¸¸äº¤æ˜“åµæ¸¬\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "ä½¿ç”¨çµ±è¨ˆæ–¹æ³•æ‰¾å‡ºå¯èƒ½çš„ç•°å¸¸äº¤æ˜“"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ea497",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 5.5: æ‰¾å‡ºå–®ç­†é‡‘é¡ç•°å¸¸é«˜çš„äº¤æ˜“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd74255",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH transaction_stats AS (\n",
    "    SELECT \n",
    "        merchant_category,\n",
    "        AVG(amount) as avg_amount,\n",
    "        STDDEV(amount) as stddev_amount\n",
    "    FROM transactions\n",
    "    WHERE transaction_status = 'approved'\n",
    "    GROUP BY merchant_category\n",
    ")\n",
    "SELECT \n",
    "    t.transaction_id,\n",
    "    t.transaction_date,\n",
    "    t.merchant_category,\n",
    "    t.amount,\n",
    "    u.user_name,\n",
    "    ROUND(ts.avg_amount, 2) as category_avg,\n",
    "    ROUND((t.amount - ts.avg_amount) / ts.stddev_amount, 2) as z_score\n",
    "FROM transactions t\n",
    "INNER JOIN cards c ON t.card_id = c.card_id\n",
    "INNER JOIN users u ON c.user_id = u.user_id\n",
    "INNER JOIN transaction_stats ts ON t.merchant_category = ts.merchant_category\n",
    "WHERE t.transaction_status = 'approved'\n",
    "    AND ABS((t.amount - ts.avg_amount) / ts.stddev_amount) > 3\n",
    "ORDER BY z_score DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7de4b8",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 5.6: åµæ¸¬çŸ­æ™‚é–“å…§çš„å¤šæ¬¡äº¤æ˜“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc9e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH transaction_intervals AS (\n",
    "    SELECT \n",
    "        t1.transaction_id,\n",
    "        t1.card_id,\n",
    "        t1.transaction_date,\n",
    "        t1.amount,\n",
    "        t1.merchant_category,\n",
    "        COUNT(t2.transaction_id) as transactions_within_hour\n",
    "    FROM transactions t1\n",
    "    LEFT JOIN transactions t2 \n",
    "        ON t1.card_id = t2.card_id\n",
    "        AND t2.transaction_date BETWEEN \n",
    "            (t1.transaction_date - INTERVAL 3 HOURS) \n",
    "            AND t1.transaction_date\n",
    "        AND t2.transaction_id != t1.transaction_id\n",
    "    WHERE t1.transaction_status = 'approved'\n",
    "    GROUP BY t1.transaction_id, t1.card_id, t1.transaction_date, t1.amount, t1.merchant_category\n",
    ")\n",
    "SELECT \n",
    "    ti.transaction_id,\n",
    "    ti.transaction_date,\n",
    "    ti.amount,\n",
    "    ti.merchant_category,\n",
    "    ti.transactions_within_hour,\n",
    "    u.user_name,\n",
    "    c.card_type\n",
    "FROM transaction_intervals ti\n",
    "INNER JOIN cards c ON ti.card_id = c.card_id\n",
    "INNER JOIN users u ON c.user_id = u.user_id\n",
    "WHERE ti.transactions_within_hour >= 3\n",
    "ORDER BY ti.transactions_within_hour DESC, ti.transaction_date DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ed4708",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## ğŸ“š å–®å…ƒå…­ï¼šPySpark DataFrame API æ“ä½œ\n",
    "\n",
    "### 6.1 DataFrame åŸºæœ¬æ“ä½œ\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "ä½¿ç”¨ PySpark DataFrame API é€²è¡Œè³‡æ–™æ“ä½œï¼Œèˆ‡ SQL æ•ˆæœç›¸åŒä½†èªæ³•ä¸åŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbc756a",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 6.1: ä½¿ç”¨ DataFrame API é€²è¡Œç¯©é¸èˆ‡èšåˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47954771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# çµ±è¨ˆå„åŸå¸‚çš„å¹³å‡å¹´æ”¶å…¥\n",
    "result = users_spark \\\n",
    "    .groupBy('city') \\\n",
    "    .agg(\n",
    "        count('*').alias('user_count'),\n",
    "        avg('annual_income').alias('avg_income'),\n",
    "        avg('age').alias('avg_age')\n",
    "    ) \\\n",
    "    .orderBy(desc('avg_income'))\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a875760c",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 6.2: ä½¿ç”¨ DataFrame API é€²è¡Œ JOIN æ“ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d07e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥è©¢äº¤æ˜“èˆ‡ä½¿ç”¨è€…è³‡è¨Š\n",
    "result = transactions_spark \\\n",
    "    .filter(col('transaction_status') == 'approved') \\\n",
    "    .join(cards_spark, 'card_id') \\\n",
    "    .join(users_spark, 'user_id') \\\n",
    "    .select(\n",
    "        'transaction_id',\n",
    "        'transaction_date',\n",
    "        'amount',\n",
    "        'merchant_category',\n",
    "        'user_name',\n",
    "        'city'\n",
    "    ) \\\n",
    "    .orderBy(desc('transaction_date')) \\\n",
    "    .limit(20)\n",
    "\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097bf870",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 6.2 Window Functions with DataFrame API\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "ä½¿ç”¨ DataFrame API å¯¦ç¾çª—å£å‡½æ•¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afc55a7",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 6.3: è¨ˆç®—æ¯ä½ä½¿ç”¨è€…çš„æ¶ˆè²»æ’å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d020da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šç¾©çª—å£\n",
    "windowSpec = Window.partitionBy('user_id').orderBy(desc('amount'))\n",
    "\n",
    "# è¨ˆç®—æ’å\n",
    "result = transactions_spark \\\n",
    "    .filter(col('transaction_status') == 'approved') \\\n",
    "    .join(cards_spark, 'card_id') \\\n",
    "    .join(users_spark, 'user_id') \\\n",
    "    .select(\n",
    "        'user_name',\n",
    "        'transaction_date',\n",
    "        'amount',\n",
    "        'merchant_category',\n",
    "        rank().over(windowSpec).alias('rank')\n",
    "    ) \\\n",
    "    .filter(col('rank') <= 5) \\\n",
    "    .orderBy('user_name', 'rank')\n",
    "\n",
    "result.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df63e77",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## ğŸ“š å–®å…ƒä¸ƒï¼šè³‡æ–™åŒ¯å‡ºèˆ‡å„²å­˜\n",
    "\n",
    "### 7.1 å„²å­˜ç‚º Parquet æ ¼å¼\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "Parquet æ˜¯ä¸€ç¨®åˆ—å¼å­˜å„²æ ¼å¼ï¼Œé©åˆå¤§æ•¸æ“šåˆ†æ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0969a9",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 7.1: å°‡è™•ç†å¾Œçš„è³‡æ–™å„²å­˜ç‚º Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8805e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹å½™æ•´å ±è¡¨\n",
    "summary_report = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        u.user_id,\n",
    "        u.user_name,\n",
    "        u.city,\n",
    "        u.age,\n",
    "        u.occupation,\n",
    "        COUNT(DISTINCT c.card_id) as card_count,\n",
    "        COUNT(DISTINCT t.transaction_id) as transaction_count,\n",
    "        COALESCE(SUM(t.amount), 0) as total_spending,\n",
    "        COALESCE(AVG(t.amount), 0) as avg_transaction_amount\n",
    "    FROM users u\n",
    "    LEFT JOIN cards c ON u.user_id = c.user_id AND c.card_status = 'active'\n",
    "    LEFT JOIN transactions t ON c.card_id = t.card_id AND t.transaction_status = 'approved'\n",
    "    GROUP BY u.user_id, u.user_name, u.city, u.age, u.occupation\n",
    "    ORDER BY total_spending DESC\n",
    "\"\"\")\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "summary_report.show(20)\n",
    "\n",
    "# å„²å­˜ç‚º Parquetï¼ˆè¨»è§£æ‰ä»¥é¿å…å¯¦éš›å¯«å…¥ï¼‰\n",
    "# summary_report.write.mode('overwrite').parquet('./output/user_summary_report.parquet')\n",
    "print(\"âœ… è³‡æ–™è™•ç†å®Œæˆï¼ˆå„²å­˜å·²è¨»è§£ï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13b8c39",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 7.2 å»ºç«‹è³‡æ–™è¦–åœ–\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "å»ºç«‹å¯é‡è¤‡ä½¿ç”¨çš„è³‡æ–™è¦–åœ–ï¼ˆViewï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e8c47",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 7.2: å»ºç«‹äº¤æ˜“å½™ç¸½è¦–åœ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df68cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹è¦–åœ–\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW transaction_summary AS\n",
    "    SELECT \n",
    "        DATE(t.transaction_date) as transaction_day,\n",
    "        t.merchant_category,\n",
    "        COUNT(*) as transaction_count,\n",
    "        SUM(t.amount) as daily_total,\n",
    "        AVG(t.amount) as daily_avg\n",
    "    FROM transactions t\n",
    "    WHERE t.transaction_status = 'approved'\n",
    "    GROUP BY DATE(t.transaction_date), t.merchant_category\n",
    "\"\"\")\n",
    "\n",
    "# ä½¿ç”¨è¦–åœ–æŸ¥è©¢\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM transaction_summary\n",
    "    WHERE daily_total > 10000\n",
    "    ORDER BY daily_total DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae3a45",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## ğŸ“š å–®å…ƒå…«ï¼šé€²éšåˆ†ææ¡ˆä¾‹\n",
    "\n",
    "### 8.1 RFM åˆ†æï¼ˆRecency, Frequency, Monetaryï¼‰\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "RFM æ˜¯è¡¡é‡å®¢æˆ¶åƒ¹å€¼çš„ç¶“å…¸æ¨¡å‹ï¼Œåˆ†ææœ€è¿‘æ¶ˆè²»ã€æ¶ˆè²»é »ç‡ã€æ¶ˆè²»é‡‘é¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799bbba",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 8.1: è¨ˆç®—ä½¿ç”¨è€… RFM æŒ‡æ¨™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64a40fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH rfm_data AS (\n",
    "    SELECT \n",
    "        u.user_id,\n",
    "        u.user_name,\n",
    "        u.city,\n",
    "        DATEDIFF(CURRENT_DATE(), MAX(t.transaction_date)) as recency,\n",
    "        COUNT(DISTINCT t.transaction_id) as frequency,\n",
    "        SUM(t.amount) as monetary\n",
    "    FROM users u\n",
    "    INNER JOIN cards c ON u.user_id = c.user_id\n",
    "    INNER JOIN transactions t ON c.card_id = t.card_id\n",
    "    WHERE t.transaction_status = 'approved'\n",
    "    GROUP BY u.user_id, u.user_name, u.city\n",
    "),\n",
    "rfm_scores AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        NTILE(5) OVER (ORDER BY recency ASC) as r_score,\n",
    "        NTILE(5) OVER (ORDER BY frequency DESC) as f_score,\n",
    "        NTILE(5) OVER (ORDER BY monetary DESC) as m_score\n",
    "    FROM rfm_data\n",
    ")\n",
    "SELECT \n",
    "    user_id,\n",
    "    user_name,\n",
    "    city,\n",
    "    recency,\n",
    "    frequency,\n",
    "    monetary,\n",
    "    r_score,\n",
    "    f_score,\n",
    "    m_score,\n",
    "    (r_score + f_score + m_score) as rfm_total_score,\n",
    "    CASE \n",
    "        WHEN r_score >= 4 AND f_score >= 4 AND m_score >= 4 THEN 'é‡è¦åƒ¹å€¼å®¢æˆ¶'\n",
    "        WHEN r_score >= 4 AND f_score >= 3 THEN 'é‡è¦ç™¼å±•å®¢æˆ¶'\n",
    "        WHEN r_score >= 3 AND f_score >= 3 AND m_score >= 3 THEN 'é‡è¦ä¿æŒå®¢æˆ¶'\n",
    "        WHEN r_score >= 4 AND m_score >= 4 THEN 'é‡è¦æŒ½ç•™å®¢æˆ¶'\n",
    "        WHEN r_score <= 2 AND f_score <= 2 THEN 'æµå¤±å®¢æˆ¶'\n",
    "        ELSE 'ä¸€èˆ¬å®¢æˆ¶'\n",
    "    END as customer_segment\n",
    "FROM rfm_scores\n",
    "ORDER BY rfm_total_score DESC\n",
    "LIMIT 30\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af877b6a",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### ğŸ“Œ ç¯„ä¾‹ 8.2: å®¢æˆ¶åˆ†ç¾¤çµ±è¨ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca21b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH rfm_data AS (\n",
    "    SELECT \n",
    "        u.user_id,\n",
    "        DATEDIFF(CURRENT_DATE(), MAX(t.transaction_date)) as recency,\n",
    "        COUNT(DISTINCT t.transaction_id) as frequency,\n",
    "        SUM(t.amount) as monetary\n",
    "    FROM users u\n",
    "    INNER JOIN cards c ON u.user_id = c.user_id\n",
    "    INNER JOIN transactions t ON c.card_id = t.card_id\n",
    "    WHERE t.transaction_status = 'approved'\n",
    "    GROUP BY u.user_id\n",
    "),\n",
    "rfm_scores AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        NTILE(5) OVER (ORDER BY recency ASC) as r_score,\n",
    "        NTILE(5) OVER (ORDER BY frequency DESC) as f_score,\n",
    "        NTILE(5) OVER (ORDER BY monetary DESC) as m_score\n",
    "    FROM rfm_data\n",
    "),\n",
    "customer_segments AS (\n",
    "    SELECT \n",
    "        user_id,\n",
    "        CASE \n",
    "            WHEN r_score >= 4 AND f_score >= 4 AND m_score >= 4 THEN 'é‡è¦åƒ¹å€¼å®¢æˆ¶'\n",
    "            WHEN r_score >= 4 AND f_score >= 3 THEN 'é‡è¦ç™¼å±•å®¢æˆ¶'\n",
    "            WHEN r_score >= 3 AND f_score >= 3 AND m_score >= 3 THEN 'é‡è¦ä¿æŒå®¢æˆ¶'\n",
    "            WHEN r_score >= 4 AND m_score >= 4 THEN 'é‡è¦æŒ½ç•™å®¢æˆ¶'\n",
    "            WHEN r_score <= 2 AND f_score <= 2 THEN 'æµå¤±å®¢æˆ¶'\n",
    "            ELSE 'ä¸€èˆ¬å®¢æˆ¶'\n",
    "        END as customer_segment\n",
    "    FROM rfm_scores\n",
    ")\n",
    "SELECT \n",
    "    customer_segment,\n",
    "    COUNT(*) as customer_count,\n",
    "    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentage\n",
    "FROM customer_segments\n",
    "GROUP BY customer_segment\n",
    "ORDER BY customer_count DESC\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49ac7a3",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## ğŸ“š ç¸½çµèˆ‡æ¸…ç†\n",
    "\n",
    "### èª²ç¨‹é‡é»å›é¡§\n",
    "\n",
    "ğŸˆ æœ¬èª²ç¨‹æ¶µè“‹å…§å®¹ï¼š\n",
    "\n",
    "1. **è³‡æ–™å»ºç«‹**ï¼šä½¿ç”¨ Numpyã€Pandas å»ºç«‹æ¨¡æ“¬çš„ä¿¡ç”¨å¡äº¤æ˜“è³‡æ–™\n",
    "2. **SQL åŸºç¤**ï¼šSELECTã€WHEREã€GROUP BYã€èšåˆå‡½æ•¸\n",
    "3. **JOIN æ“ä½œ**ï¼šINNER JOINã€LEFT JOIN å¤šè¡¨é—œè¯æŸ¥è©¢\n",
    "4. **é€²éšæŸ¥è©¢**ï¼šå­æŸ¥è©¢ã€Window Functionsã€CTE (Common Table Expression)\n",
    "5. **è³‡æ–™åˆ†æ**ï¼šæ™‚é–“åºåˆ—åˆ†æã€ä½¿ç”¨è€…è¡Œç‚ºåˆ†æã€ç•°å¸¸åµæ¸¬\n",
    "6. **DataFrame API**ï¼šPySpark DataFrame æ“ä½œæ–¹æ³•\n",
    "7. **å¯¦æˆ°æ¡ˆä¾‹**ï¼šRFM å®¢æˆ¶åƒ¹å€¼åˆ†æã€å®¢æˆ¶åˆ†ç¾¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e39075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸…ç†è³‡æº\n",
    "print(\"ğŸ“Š è³‡æ–™çµ±è¨ˆç¸½è¦½ï¼š\")\n",
    "print(f\"ä½¿ç”¨è€…ç¸½æ•¸ï¼š{users_spark.count()}\")\n",
    "print(f\"ä¿¡ç”¨å¡ç¸½æ•¸ï¼š{cards_spark.count()}\")\n",
    "print(f\"äº¤æ˜“ç¸½ç­†æ•¸ï¼š{transactions_spark.count()}\")\n",
    "print(f\"æ ¸å‡†äº¤æ˜“ç­†æ•¸ï¼š{transactions_spark.filter(col('transaction_status') == 'approved').count()}\")\n",
    "\n",
    "# é¡¯ç¤ºå·²å»ºç«‹çš„è‡¨æ™‚è¡¨æ ¼\n",
    "print(\"\\nå·²å»ºç«‹çš„è‡¨æ™‚è¡¨æ ¼ï¼š\")\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cb8e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœæ­¢ Spark Sessionï¼ˆå¯é¸ï¼‰\n",
    "# spark.stop()\n",
    "print(\"\\nâœ… èª²ç¨‹çµæŸï¼\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "region_name,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "fju",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
