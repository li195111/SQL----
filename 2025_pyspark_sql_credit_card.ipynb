{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5219a437",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "# PySpark SQL 信用卡交易資料分析實戰\n",
    "\n",
    "作者：QChoice AI 教學團隊\n",
    "日期：2025-01-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境設定與套件載入\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# 建立 Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CreditCardAnalysis\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"./spark-warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"✅ Spark版本: {spark.version}\")\n",
    "print(f\"✅ 環境設定完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7056407e",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 📚 單元一：建立信用卡交易假資料\n",
    "\n",
    "### 1.1 使用者資料建立\n",
    "\n",
    "🎈 概念解釋：\n",
    "使用 Numpy 和 Pandas 建立使用者基本資料，包含使用者ID、姓名、年齡、職業等資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1657b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定隨機種子以確保結果可重現\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# 建立使用者資料\n",
    "n_users = 1000\n",
    "\n",
    "# 姓氏和名字列表（台灣常見姓名）\n",
    "last_names = ['陳', '林', '黃', '張', '李', '王', '吳', '劉', '蔡', '楊', '許', '鄭', '謝', '郭', '洪']\n",
    "first_names = ['怡君', '志明', '雅婷', '建國', '淑芬', '俊傑', '美玲', '家豪', '詩涵', '冠宇', '佳穎', '宗翰', '筱涵', '承恩', '雅筑']\n",
    "\n",
    "users_data = {\n",
    "    'user_id': [f'U{str(i).zfill(6)}' for i in range(1, n_users + 1)],\n",
    "    'user_name': [random.choice(last_names) + random.choice(first_names) for _ in range(n_users)],\n",
    "    'age': np.random.randint(20, 70, n_users),\n",
    "    'gender': np.random.choice(['M', 'F'], n_users),\n",
    "    'occupation': np.random.choice(['上班族', '學生', '自由業', '退休', '家管', '公務員', '醫療', '教育'], n_users),\n",
    "    'city': np.random.choice(['台北', '新北', '桃園', '台中', '台南', '高雄', '新竹', '基隆'], n_users),\n",
    "    'annual_income': np.random.randint(300000, 2000000, n_users)\n",
    "}\n",
    "\n",
    "users_df = pd.DataFrame(users_data)\n",
    "print(\"✅ 使用者資料建立完成\")\n",
    "users_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd1b09f",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 1.1: 將 Pandas DataFrame 轉換為 PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5cd670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 轉換為 PySpark DataFrame\n",
    "users_spark = spark.createDataFrame(users_df)\n",
    "\n",
    "# 註冊為臨時表格\n",
    "users_spark.createOrReplaceTempView(\"users\")\n",
    "\n",
    "print(\"📊 使用者資料表結構：\")\n",
    "users_spark.printSchema()\n",
    "print(f\"\\n總使用者數：{users_spark.count()}\")\n",
    "users_spark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9231a1",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 1.2 信用卡持卡人資料建立\n",
    "\n",
    "🎈 概念解釋：\n",
    "每位使用者可能持有多張信用卡，建立信用卡基本資訊，包含卡號、卡別、額度等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca45c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立信用卡資料\n",
    "n_cards = 1500\n",
    "\n",
    "card_types = ['金卡', '白金卡', '鈦金卡', '普卡', '商務卡']\n",
    "card_brands = ['VISA', 'MasterCard', 'JCB', 'American Express']\n",
    "banks = ['台新銀行', '國泰世華', '中信銀行', '玉山銀行', '富邦銀行', '第一銀行']\n",
    "\n",
    "# 隨機分配信用卡給使用者（有些使用者有多張卡）\n",
    "card_owners = np.random.choice(users_df['user_id'].values, n_cards)\n",
    "\n",
    "cards_data = {\n",
    "    'card_id': [f'C{str(i).zfill(8)}' for i in range(1, n_cards + 1)],\n",
    "    'user_id': card_owners,\n",
    "    'card_number': [f'{random.randint(1000, 9999)}-{random.randint(1000, 9999)}-{random.randint(1000, 9999)}-{random.randint(1000, 9999)}' for _ in range(n_cards)],\n",
    "    'card_type': np.random.choice(card_types, n_cards, p=[0.15, 0.25, 0.10, 0.40, 0.10]),\n",
    "    'card_brand': np.random.choice(card_brands, n_cards, p=[0.40, 0.35, 0.15, 0.10]),\n",
    "    'bank': np.random.choice(banks, n_cards),\n",
    "    'credit_limit': np.random.choice([50000, 100000, 200000, 300000, 500000, 800000], n_cards),\n",
    "    'issue_date': pd.to_datetime([datetime(2020, 1, 1) + timedelta(days=random.randint(0, 1825)) for _ in range(n_cards)]),\n",
    "    'card_status': np.random.choice(['active', 'suspended', 'closed'], n_cards, p=[0.85, 0.10, 0.05])\n",
    "}\n",
    "\n",
    "cards_df = pd.DataFrame(cards_data)\n",
    "print(\"✅ 信用卡資料建立完成\")\n",
    "cards_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b4e689",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 1.2: 轉換信用卡資料為 Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 轉換為 PySpark DataFrame\n",
    "cards_spark = spark.createDataFrame(cards_df)\n",
    "\n",
    "# 註冊為臨時表格\n",
    "cards_spark.createOrReplaceTempView(\"cards\")\n",
    "\n",
    "print(\"📊 信用卡資料表結構：\")\n",
    "cards_spark.printSchema()\n",
    "print(f\"\\n總信用卡數：{cards_spark.count()}\")\n",
    "cards_spark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b79e1d",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 1.3 信用卡刷卡交易紀錄建立\n",
    "\n",
    "🎈 概念解釋：\n",
    "建立信用卡交易明細，包含交易時間、金額、商店類別、交易狀態等資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a202c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立交易資料\n",
    "n_transactions = 50000\n",
    "\n",
    "# 只使用狀態為 active 的信用卡\n",
    "active_cards = cards_df[cards_df['card_status'] == 'active']['card_id'].values\n",
    "\n",
    "# 商店類別\n",
    "merchant_categories = [\n",
    "    '超市', '餐廳', '加油站', '百貨公司', '網購', \n",
    "    '電影院', '書店', '藥局', '咖啡廳', '服飾店',\n",
    "    '3C賣場', '便利商店', '旅遊', '飯店', '醫療'\n",
    "]\n",
    "\n",
    "# 交易地點\n",
    "locations = ['台北', '新北', '桃園', '台中', '台南', '高雄', '新竹', '基隆', '國外']\n",
    "\n",
    "# 生成交易紀錄\n",
    "transactions_data = {\n",
    "    'transaction_id': [f'T{str(i).zfill(10)}' for i in range(1, n_transactions + 1)],\n",
    "    'card_id': np.random.choice(active_cards, n_transactions),\n",
    "    'transaction_date': pd.to_datetime([\n",
    "        datetime(2024, 1, 1) + timedelta(\n",
    "            days=random.randint(0, 365),\n",
    "            hours=random.randint(0, 23),\n",
    "            minutes=random.randint(0, 59)\n",
    "        ) for _ in range(n_transactions)\n",
    "    ]),\n",
    "    'merchant_name': [f\"{random.choice(merchant_categories)}{random.randint(1, 100)}號店\" for _ in range(n_transactions)],\n",
    "    'merchant_category': np.random.choice(merchant_categories, n_transactions),\n",
    "    'amount': np.random.gamma(2, 500, n_transactions).astype(int),  # 使用 Gamma 分布模擬真實交易金額\n",
    "    'location': np.random.choice(locations, n_transactions, p=[0.25, 0.20, 0.15, 0.15, 0.10, 0.08, 0.05, 0.01, 0.01]),\n",
    "    'transaction_status': np.random.choice(['approved', 'declined', 'pending'], n_transactions, p=[0.90, 0.08, 0.02]),\n",
    "    'is_online': np.random.choice([True, False], n_transactions, p=[0.35, 0.65])\n",
    "}\n",
    "\n",
    "transactions_df = pd.DataFrame(transactions_data)\n",
    "\n",
    "# 確保金額為正數且合理\n",
    "transactions_df['amount'] = transactions_df['amount'].clip(lower=10, upper=100000)\n",
    "\n",
    "print(\"✅ 交易資料建立完成\")\n",
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e97f2",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 1.3: 轉換交易資料為 Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cb5776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 轉換為 PySpark DataFrame\n",
    "transactions_spark = spark.createDataFrame(transactions_df)\n",
    "\n",
    "# 註冊為臨時表格\n",
    "transactions_spark.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "print(\"📊 交易資料表結構：\")\n",
    "transactions_spark.printSchema()\n",
    "print(f\"\\n總交易筆數：{transactions_spark.count()}\")\n",
    "transactions_spark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522c9866",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 📚 單元二：SQL 基礎查詢練習\n",
    "\n",
    "### 2.1 SELECT 與 WHERE 子句\n",
    "\n",
    "🎈 概念解釋：\n",
    "使用 SELECT 選擇欄位，WHERE 進行條件篩選"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c869b79",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 2.1: 查詢年齡大於40歲的使用者"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6489d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT user_id, user_name, age, city, occupation\n",
    "FROM users\n",
    "WHERE age > 40\n",
    "ORDER BY age DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f48bd0",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 2.2: 查詢台北地區的高收入使用者（年收入超過100萬）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8af99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT user_id, user_name, age, city, annual_income\n",
    "FROM users\n",
    "WHERE city = '台北' AND annual_income > 1000000\n",
    "ORDER BY annual_income DESC\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a660471",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 2.2 聚合函數與 GROUP BY\n",
    "\n",
    "🎈 概念解釋：\n",
    "使用聚合函數（COUNT, SUM, AVG, MAX, MIN）搭配 GROUP BY 進行資料統計"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26604b52",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 2.3: 統計各城市的使用者數量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ae8edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    city,\n",
    "    COUNT(*) as user_count,\n",
    "    AVG(age) as avg_age,\n",
    "    AVG(annual_income) as avg_income\n",
    "FROM users\n",
    "GROUP BY city\n",
    "ORDER BY user_count DESC\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c3b64b",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 2.4: 統計各商店類別的交易總額"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e72c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    merchant_category,\n",
    "    COUNT(*) as transaction_count,\n",
    "    SUM(amount) as total_amount,\n",
    "    AVG(amount) as avg_amount,\n",
    "    MAX(amount) as max_amount\n",
    "FROM transactions\n",
    "WHERE transaction_status = 'approved'\n",
    "GROUP BY merchant_category\n",
    "ORDER BY total_amount DESC\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3170ec",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 📚 單元三：JOIN 連接查詢\n",
    "\n",
    "### 3.1 INNER JOIN\n",
    "\n",
    "🎈 概念解釋：\n",
    "INNER JOIN 取得兩個表格的交集資料"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27996607",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 3.1: 查詢使用者的信用卡資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c789a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    u.user_id,\n",
    "    u.user_name,\n",
    "    u.city,\n",
    "    c.card_id,\n",
    "    c.card_type,\n",
    "    c.card_brand,\n",
    "    c.credit_limit\n",
    "FROM users u\n",
    "INNER JOIN cards c ON u.user_id = c.user_id\n",
    "WHERE c.card_status = 'active'\n",
    "ORDER BY u.user_id\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957aba2f",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 3.2: 查詢交易紀錄與持卡人資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb8355",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    t.transaction_id,\n",
    "    t.transaction_date,\n",
    "    t.amount,\n",
    "    t.merchant_category,\n",
    "    c.card_type,\n",
    "    u.user_name,\n",
    "    u.city\n",
    "FROM transactions t\n",
    "INNER JOIN cards c ON t.card_id = c.card_id\n",
    "INNER JOIN users u ON c.user_id = u.user_id\n",
    "WHERE t.transaction_status = 'approved'\n",
    "ORDER BY t.transaction_date DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9787c138",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 3.2 LEFT JOIN\n",
    "\n",
    "🎈 概念解釋：\n",
    "LEFT JOIN 保留左表所有資料，右表沒有對應則顯示 NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb9f307",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 3.3: 查詢所有使用者及其信用卡數量（包含沒有信用卡的使用者）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04202388",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    u.user_id,\n",
    "    u.user_name,\n",
    "    u.city,\n",
    "    COUNT(c.card_id) as card_count\n",
    "FROM users u\n",
    "LEFT JOIN cards c ON u.user_id = c.user_id\n",
    "GROUP BY u.user_id, u.user_name, u.city\n",
    "ORDER BY card_count DESC, u.user_id\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e69c5b4",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 📚 單元四：進階 SQL 查詢\n",
    "\n",
    "### 4.1 子查詢 (Subquery)\n",
    "\n",
    "🎈 概念解釋：\n",
    "子查詢是在查詢內部再進行另一個查詢，可用於複雜的條件判斷"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6d2b0",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 4.1: 查詢交易金額高於平均值的交易"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199d43e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    t.transaction_id,\n",
    "    t.transaction_date,\n",
    "    t.amount,\n",
    "    t.merchant_category,\n",
    "    u.user_name\n",
    "FROM transactions t\n",
    "INNER JOIN cards c ON t.card_id = c.card_id\n",
    "INNER JOIN users u ON c.user_id = u.user_id\n",
    "WHERE t.amount > (SELECT AVG(amount) FROM transactions)\n",
    "    AND t.transaction_status = 'approved'\n",
    "ORDER BY t.amount DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ac1052",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 4.2: 找出擁有最多信用卡的前10名使用者"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef64f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    u.user_id,\n",
    "    u.user_name,\n",
    "    u.occupation,\n",
    "    u.annual_income,\n",
    "    card_stats.card_count\n",
    "FROM users u\n",
    "INNER JOIN (\n",
    "    SELECT user_id, COUNT(*) as card_count\n",
    "    FROM cards\n",
    "    WHERE card_status = 'active'\n",
    "    GROUP BY user_id\n",
    ") card_stats ON u.user_id = card_stats.user_id\n",
    "ORDER BY card_stats.card_count DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d3482b",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 4.2 Window Functions (窗口函數)\n",
    "\n",
    "🎈 概念解釋：\n",
    "Window Functions 可以在不改變結果集行數的情況下進行聚合計算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd26459",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 4.3: 計算每位使用者的累積消費金額"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e75625",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    u.user_name,\n",
    "    t.transaction_date,\n",
    "    t.amount,\n",
    "    t.merchant_category,\n",
    "    SUM(t.amount) OVER (\n",
    "        PARTITION BY u.user_id \n",
    "        ORDER BY t.transaction_date\n",
    "    ) as cumulative_amount\n",
    "FROM transactions t\n",
    "INNER JOIN cards c ON t.card_id = c.card_id\n",
    "INNER JOIN users u ON c.user_id = u.user_id\n",
    "WHERE t.transaction_status = 'approved'\n",
    "    AND u.user_id IN (SELECT user_id FROM users LIMIT 3)\n",
    "ORDER BY u.user_name, t.transaction_date\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0ad379",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 4.4: 為每位使用者的交易排名（依金額）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87658f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    user_name,\n",
    "    transaction_date,\n",
    "    amount,\n",
    "    merchant_category,\n",
    "    rank\n",
    "FROM (\n",
    "    SELECT \n",
    "        u.user_name,\n",
    "        t.transaction_date,\n",
    "        t.amount,\n",
    "        t.merchant_category,\n",
    "        RANK() OVER (\n",
    "            PARTITION BY u.user_id \n",
    "            ORDER BY t.amount DESC\n",
    "        ) as rank\n",
    "    FROM transactions t\n",
    "    INNER JOIN cards c ON t.card_id = c.card_id\n",
    "    INNER JOIN users u ON c.user_id = u.user_id\n",
    "    WHERE t.transaction_status = 'approved'\n",
    ") ranked_transactions\n",
    "WHERE rank <= 5\n",
    "ORDER BY user_name, rank\n",
    "LIMIT 30\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b097dc0a",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 📚 單元五：資料分析實戰\n",
    "\n",
    "### 5.1 月度消費分析\n",
    "\n",
    "🎈 概念解釋：\n",
    "使用日期函數進行時間序列分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb24d7d",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 5.1: 每月交易統計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24b4364",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    YEAR(transaction_date) as year,\n",
    "    MONTH(transaction_date) as month,\n",
    "    COUNT(*) as transaction_count,\n",
    "    SUM(amount) as total_amount,\n",
    "    AVG(amount) as avg_amount\n",
    "FROM transactions\n",
    "WHERE transaction_status = 'approved'\n",
    "GROUP BY YEAR(transaction_date), MONTH(transaction_date)\n",
    "ORDER BY year, month\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53510b11",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 5.2: 各月份熱門消費類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae641b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    YEAR(transaction_date) as year,\n",
    "    MONTH(transaction_date) as month,\n",
    "    merchant_category,\n",
    "    COUNT(*) as transaction_count,\n",
    "    SUM(amount) as total_amount\n",
    "FROM transactions\n",
    "WHERE transaction_status = 'approved'\n",
    "GROUP BY YEAR(transaction_date), MONTH(transaction_date), merchant_category\n",
    "ORDER BY year, month, total_amount DESC\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9cc291",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 5.2 使用者消費行為分析\n",
    "\n",
    "🎈 概念解釋：\n",
    "分析使用者的消費模式與偏好"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7cba06",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 5.3: 高消費使用者分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2c7aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    u.user_id,\n",
    "    u.user_name,\n",
    "    u.age,\n",
    "    u.occupation,\n",
    "    u.city,\n",
    "    COUNT(DISTINCT t.transaction_id) as transaction_count,\n",
    "    SUM(t.amount) as total_spending,\n",
    "    AVG(t.amount) as avg_transaction_amount,\n",
    "    COUNT(DISTINCT t.merchant_category) as category_diversity\n",
    "FROM users u\n",
    "INNER JOIN cards c ON u.user_id = c.user_id\n",
    "INNER JOIN transactions t ON c.card_id = t.card_id\n",
    "WHERE t.transaction_status = 'approved'\n",
    "GROUP BY u.user_id, u.user_name, u.age, u.occupation, u.city\n",
    "HAVING SUM(t.amount) > 100000\n",
    "ORDER BY total_spending DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7279b8df",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 5.4: 使用者最常消費的類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199665dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH user_category_spending AS (\n",
    "    SELECT \n",
    "        u.user_id,\n",
    "        u.user_name,\n",
    "        t.merchant_category,\n",
    "        COUNT(*) as transaction_count,\n",
    "        SUM(t.amount) as category_spending,\n",
    "        RANK() OVER (\n",
    "            PARTITION BY u.user_id \n",
    "            ORDER BY COUNT(*) DESC\n",
    "        ) as category_rank\n",
    "    FROM users u\n",
    "    INNER JOIN cards c ON u.user_id = c.user_id\n",
    "    INNER JOIN transactions t ON c.card_id = t.card_id\n",
    "    WHERE t.transaction_status = 'approved'\n",
    "    GROUP BY u.user_id, u.user_name, t.merchant_category\n",
    ")\n",
    "SELECT \n",
    "    user_id,\n",
    "    user_name,\n",
    "    merchant_category as favorite_category,\n",
    "    transaction_count,\n",
    "    category_spending\n",
    "FROM user_category_spending\n",
    "WHERE category_rank = 1\n",
    "ORDER BY transaction_count DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52135ded",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 5.3 異常交易偵測\n",
    "\n",
    "🎈 概念解釋：\n",
    "使用統計方法找出可能的異常交易"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ea497",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 5.5: 找出單筆金額異常高的交易"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd74255",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH transaction_stats AS (\n",
    "    SELECT \n",
    "        merchant_category,\n",
    "        AVG(amount) as avg_amount,\n",
    "        STDDEV(amount) as stddev_amount\n",
    "    FROM transactions\n",
    "    WHERE transaction_status = 'approved'\n",
    "    GROUP BY merchant_category\n",
    ")\n",
    "SELECT \n",
    "    t.transaction_id,\n",
    "    t.transaction_date,\n",
    "    t.merchant_category,\n",
    "    t.amount,\n",
    "    u.user_name,\n",
    "    ROUND(ts.avg_amount, 2) as category_avg,\n",
    "    ROUND((t.amount - ts.avg_amount) / ts.stddev_amount, 2) as z_score\n",
    "FROM transactions t\n",
    "INNER JOIN cards c ON t.card_id = c.card_id\n",
    "INNER JOIN users u ON c.user_id = u.user_id\n",
    "INNER JOIN transaction_stats ts ON t.merchant_category = ts.merchant_category\n",
    "WHERE t.transaction_status = 'approved'\n",
    "    AND ABS((t.amount - ts.avg_amount) / ts.stddev_amount) > 3\n",
    "ORDER BY z_score DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7de4b8",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 5.6: 偵測短時間內的多次交易"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc9e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH transaction_intervals AS (\n",
    "    SELECT \n",
    "        t1.transaction_id,\n",
    "        t1.card_id,\n",
    "        t1.transaction_date,\n",
    "        t1.amount,\n",
    "        t1.merchant_category,\n",
    "        COUNT(t2.transaction_id) as transactions_within_hour\n",
    "    FROM transactions t1\n",
    "    LEFT JOIN transactions t2 \n",
    "        ON t1.card_id = t2.card_id\n",
    "        AND t2.transaction_date BETWEEN \n",
    "            (t1.transaction_date - INTERVAL 3 HOURS) \n",
    "            AND t1.transaction_date\n",
    "        AND t2.transaction_id != t1.transaction_id\n",
    "    WHERE t1.transaction_status = 'approved'\n",
    "    GROUP BY t1.transaction_id, t1.card_id, t1.transaction_date, t1.amount, t1.merchant_category\n",
    ")\n",
    "SELECT \n",
    "    ti.transaction_id,\n",
    "    ti.transaction_date,\n",
    "    ti.amount,\n",
    "    ti.merchant_category,\n",
    "    ti.transactions_within_hour,\n",
    "    u.user_name,\n",
    "    c.card_type\n",
    "FROM transaction_intervals ti\n",
    "INNER JOIN cards c ON ti.card_id = c.card_id\n",
    "INNER JOIN users u ON c.user_id = u.user_id\n",
    "WHERE ti.transactions_within_hour >= 3\n",
    "ORDER BY ti.transactions_within_hour DESC, ti.transaction_date DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ed4708",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 📚 單元六：PySpark DataFrame API 操作\n",
    "\n",
    "### 6.1 DataFrame 基本操作\n",
    "\n",
    "🎈 概念解釋：\n",
    "使用 PySpark DataFrame API 進行資料操作，與 SQL 效果相同但語法不同"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbc756a",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 6.1: 使用 DataFrame API 進行篩選與聚合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47954771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 統計各城市的平均年收入\n",
    "result = users_spark \\\n",
    "    .groupBy('city') \\\n",
    "    .agg(\n",
    "        count('*').alias('user_count'),\n",
    "        avg('annual_income').alias('avg_income'),\n",
    "        avg('age').alias('avg_age')\n",
    "    ) \\\n",
    "    .orderBy(desc('avg_income'))\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a875760c",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 6.2: 使用 DataFrame API 進行 JOIN 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d07e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查詢交易與使用者資訊\n",
    "result = transactions_spark \\\n",
    "    .filter(col('transaction_status') == 'approved') \\\n",
    "    .join(cards_spark, 'card_id') \\\n",
    "    .join(users_spark, 'user_id') \\\n",
    "    .select(\n",
    "        'transaction_id',\n",
    "        'transaction_date',\n",
    "        'amount',\n",
    "        'merchant_category',\n",
    "        'user_name',\n",
    "        'city'\n",
    "    ) \\\n",
    "    .orderBy(desc('transaction_date')) \\\n",
    "    .limit(20)\n",
    "\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097bf870",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 6.2 Window Functions with DataFrame API\n",
    "\n",
    "🎈 概念解釋：\n",
    "使用 DataFrame API 實現窗口函數"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afc55a7",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 6.3: 計算每位使用者的消費排名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d020da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義窗口\n",
    "windowSpec = Window.partitionBy('user_id').orderBy(desc('amount'))\n",
    "\n",
    "# 計算排名\n",
    "result = transactions_spark \\\n",
    "    .filter(col('transaction_status') == 'approved') \\\n",
    "    .join(cards_spark, 'card_id') \\\n",
    "    .join(users_spark, 'user_id') \\\n",
    "    .select(\n",
    "        'user_name',\n",
    "        'transaction_date',\n",
    "        'amount',\n",
    "        'merchant_category',\n",
    "        rank().over(windowSpec).alias('rank')\n",
    "    ) \\\n",
    "    .filter(col('rank') <= 5) \\\n",
    "    .orderBy('user_name', 'rank')\n",
    "\n",
    "result.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df63e77",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 📚 單元七：資料匯出與儲存\n",
    "\n",
    "### 7.1 儲存為 Parquet 格式\n",
    "\n",
    "🎈 概念解釋：\n",
    "Parquet 是一種列式存儲格式，適合大數據分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0969a9",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 7.1: 將處理後的資料儲存為 Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8805e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立彙整報表\n",
    "summary_report = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        u.user_id,\n",
    "        u.user_name,\n",
    "        u.city,\n",
    "        u.age,\n",
    "        u.occupation,\n",
    "        COUNT(DISTINCT c.card_id) as card_count,\n",
    "        COUNT(DISTINCT t.transaction_id) as transaction_count,\n",
    "        COALESCE(SUM(t.amount), 0) as total_spending,\n",
    "        COALESCE(AVG(t.amount), 0) as avg_transaction_amount\n",
    "    FROM users u\n",
    "    LEFT JOIN cards c ON u.user_id = c.user_id AND c.card_status = 'active'\n",
    "    LEFT JOIN transactions t ON c.card_id = t.card_id AND t.transaction_status = 'approved'\n",
    "    GROUP BY u.user_id, u.user_name, u.city, u.age, u.occupation\n",
    "    ORDER BY total_spending DESC\n",
    "\"\"\")\n",
    "\n",
    "# 顯示結果\n",
    "summary_report.show(20)\n",
    "\n",
    "# 儲存為 Parquet（註解掉以避免實際寫入）\n",
    "# summary_report.write.mode('overwrite').parquet('./output/user_summary_report.parquet')\n",
    "print(\"✅ 資料處理完成（儲存已註解）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13b8c39",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 7.2 建立資料視圖\n",
    "\n",
    "🎈 概念解釋：\n",
    "建立可重複使用的資料視圖（View）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e8c47",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 7.2: 建立交易彙總視圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df68cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立視圖\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW transaction_summary AS\n",
    "    SELECT \n",
    "        DATE(t.transaction_date) as transaction_day,\n",
    "        t.merchant_category,\n",
    "        COUNT(*) as transaction_count,\n",
    "        SUM(t.amount) as daily_total,\n",
    "        AVG(t.amount) as daily_avg\n",
    "    FROM transactions t\n",
    "    WHERE t.transaction_status = 'approved'\n",
    "    GROUP BY DATE(t.transaction_date), t.merchant_category\n",
    "\"\"\")\n",
    "\n",
    "# 使用視圖查詢\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM transaction_summary\n",
    "    WHERE daily_total > 10000\n",
    "    ORDER BY daily_total DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae3a45",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 📚 單元八：進階分析案例\n",
    "\n",
    "### 8.1 RFM 分析（Recency, Frequency, Monetary）\n",
    "\n",
    "🎈 概念解釋：\n",
    "RFM 是衡量客戶價值的經典模型，分析最近消費、消費頻率、消費金額"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799bbba",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 8.1: 計算使用者 RFM 指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64a40fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH rfm_data AS (\n",
    "    SELECT \n",
    "        u.user_id,\n",
    "        u.user_name,\n",
    "        u.city,\n",
    "        DATEDIFF(CURRENT_DATE(), MAX(t.transaction_date)) as recency,\n",
    "        COUNT(DISTINCT t.transaction_id) as frequency,\n",
    "        SUM(t.amount) as monetary\n",
    "    FROM users u\n",
    "    INNER JOIN cards c ON u.user_id = c.user_id\n",
    "    INNER JOIN transactions t ON c.card_id = t.card_id\n",
    "    WHERE t.transaction_status = 'approved'\n",
    "    GROUP BY u.user_id, u.user_name, u.city\n",
    "),\n",
    "rfm_scores AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        NTILE(5) OVER (ORDER BY recency ASC) as r_score,\n",
    "        NTILE(5) OVER (ORDER BY frequency DESC) as f_score,\n",
    "        NTILE(5) OVER (ORDER BY monetary DESC) as m_score\n",
    "    FROM rfm_data\n",
    ")\n",
    "SELECT \n",
    "    user_id,\n",
    "    user_name,\n",
    "    city,\n",
    "    recency,\n",
    "    frequency,\n",
    "    monetary,\n",
    "    r_score,\n",
    "    f_score,\n",
    "    m_score,\n",
    "    (r_score + f_score + m_score) as rfm_total_score,\n",
    "    CASE \n",
    "        WHEN r_score >= 4 AND f_score >= 4 AND m_score >= 4 THEN '重要價值客戶'\n",
    "        WHEN r_score >= 4 AND f_score >= 3 THEN '重要發展客戶'\n",
    "        WHEN r_score >= 3 AND f_score >= 3 AND m_score >= 3 THEN '重要保持客戶'\n",
    "        WHEN r_score >= 4 AND m_score >= 4 THEN '重要挽留客戶'\n",
    "        WHEN r_score <= 2 AND f_score <= 2 THEN '流失客戶'\n",
    "        ELSE '一般客戶'\n",
    "    END as customer_segment\n",
    "FROM rfm_scores\n",
    "ORDER BY rfm_total_score DESC\n",
    "LIMIT 30\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af877b6a",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 8.2: 客戶分群統計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca21b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH rfm_data AS (\n",
    "    SELECT \n",
    "        u.user_id,\n",
    "        DATEDIFF(CURRENT_DATE(), MAX(t.transaction_date)) as recency,\n",
    "        COUNT(DISTINCT t.transaction_id) as frequency,\n",
    "        SUM(t.amount) as monetary\n",
    "    FROM users u\n",
    "    INNER JOIN cards c ON u.user_id = c.user_id\n",
    "    INNER JOIN transactions t ON c.card_id = t.card_id\n",
    "    WHERE t.transaction_status = 'approved'\n",
    "    GROUP BY u.user_id\n",
    "),\n",
    "rfm_scores AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        NTILE(5) OVER (ORDER BY recency ASC) as r_score,\n",
    "        NTILE(5) OVER (ORDER BY frequency DESC) as f_score,\n",
    "        NTILE(5) OVER (ORDER BY monetary DESC) as m_score\n",
    "    FROM rfm_data\n",
    "),\n",
    "customer_segments AS (\n",
    "    SELECT \n",
    "        user_id,\n",
    "        CASE \n",
    "            WHEN r_score >= 4 AND f_score >= 4 AND m_score >= 4 THEN '重要價值客戶'\n",
    "            WHEN r_score >= 4 AND f_score >= 3 THEN '重要發展客戶'\n",
    "            WHEN r_score >= 3 AND f_score >= 3 AND m_score >= 3 THEN '重要保持客戶'\n",
    "            WHEN r_score >= 4 AND m_score >= 4 THEN '重要挽留客戶'\n",
    "            WHEN r_score <= 2 AND f_score <= 2 THEN '流失客戶'\n",
    "            ELSE '一般客戶'\n",
    "        END as customer_segment\n",
    "    FROM rfm_scores\n",
    ")\n",
    "SELECT \n",
    "    customer_segment,\n",
    "    COUNT(*) as customer_count,\n",
    "    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentage\n",
    "FROM customer_segments\n",
    "GROUP BY customer_segment\n",
    "ORDER BY customer_count DESC\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49ac7a3",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 📚 總結與清理\n",
    "\n",
    "### 課程重點回顧\n",
    "\n",
    "🎈 本課程涵蓋內容：\n",
    "\n",
    "1. **資料建立**：使用 Numpy、Pandas 建立模擬的信用卡交易資料\n",
    "2. **SQL 基礎**：SELECT、WHERE、GROUP BY、聚合函數\n",
    "3. **JOIN 操作**：INNER JOIN、LEFT JOIN 多表關聯查詢\n",
    "4. **進階查詢**：子查詢、Window Functions、CTE (Common Table Expression)\n",
    "5. **資料分析**：時間序列分析、使用者行為分析、異常偵測\n",
    "6. **DataFrame API**：PySpark DataFrame 操作方法\n",
    "7. **實戰案例**：RFM 客戶價值分析、客戶分群"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e39075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理資源\n",
    "print(\"📊 資料統計總覽：\")\n",
    "print(f\"使用者總數：{users_spark.count()}\")\n",
    "print(f\"信用卡總數：{cards_spark.count()}\")\n",
    "print(f\"交易總筆數：{transactions_spark.count()}\")\n",
    "print(f\"核准交易筆數：{transactions_spark.filter(col('transaction_status') == 'approved').count()}\")\n",
    "\n",
    "# 顯示已建立的臨時表格\n",
    "print(\"\\n已建立的臨時表格：\")\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cb8e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停止 Spark Session（可選）\n",
    "# spark.stop()\n",
    "print(\"\\n✅ 課程結束！\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "region_name,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "fju",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
