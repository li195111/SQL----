# %% [md]
## PySpark SQL ä¿¡ç”¨å¡äº¤æ˜“è³‡æ–™åˆ†æå¯¦æˆ°
# 
# ä½œè€…ï¼šQChoice AI æ•™å­¸åœ˜éšŠ
# æ—¥æœŸï¼š2025-01-16

# %%
# ç’°å¢ƒè¨­å®šèˆ‡å¥—ä»¶è¼‰å…¥
import numpy as np
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window
from datetime import datetime, timedelta
import random

# å»ºç«‹ Spark Session
spark = SparkSession.builder \
    .appName("CreditCardAnalysis") \
    .config("spark.sql.warehouse.dir", "./spark-warehouse") \
    .getOrCreate()

print(f"âœ… Sparkç‰ˆæœ¬: {spark.version}")
print(f"âœ… ç’°å¢ƒè¨­å®šå®Œæˆ")

# %% [md]
### ğŸ“š å–®å…ƒä¸€ï¼šå»ºç«‹ä¿¡ç”¨å¡äº¤æ˜“å‡è³‡æ–™

#### 1.1 ä½¿ç”¨è€…è³‡æ–™å»ºç«‹

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# ä½¿ç”¨ Numpy å’Œ Pandas å»ºç«‹ä½¿ç”¨è€…åŸºæœ¬è³‡æ–™ï¼ŒåŒ…å«ä½¿ç”¨è€…IDã€å§“åã€å¹´é½¡ã€è·æ¥­ç­‰è³‡è¨Š

# %%
# è¨­å®šéš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿çµæœå¯é‡ç¾
np.random.seed(42)
random.seed(42)

# å»ºç«‹ä½¿ç”¨è€…è³‡æ–™
n_users = 1000

# å§“æ°å’Œåå­—åˆ—è¡¨ï¼ˆå°ç£å¸¸è¦‹å§“åï¼‰
last_names = ['é™³', 'æ—', 'é»ƒ', 'å¼µ', 'æ', 'ç‹', 'å³', 'åŠ‰', 'è”¡', 'æ¥Š', 'è¨±', 'é„­', 'è¬', 'éƒ­', 'æ´ª']
first_names = ['æ€¡å›', 'å¿—æ˜', 'é›…å©·', 'å»ºåœ‹', 'æ·‘èŠ¬', 'ä¿Šå‚‘', 'ç¾ç²', 'å®¶è±ª', 'è©©æ¶µ', 'å† å®‡', 'ä½³ç©', 'å®—ç¿°', 'ç­±æ¶µ', 'æ‰¿æ©', 'é›…ç­‘']

users_data = {
    'user_id': [f'U{str(i).zfill(6)}' for i in range(1, n_users + 1)],
    'user_name': [random.choice(last_names) + random.choice(first_names) for _ in range(n_users)],
    'age': np.random.randint(20, 70, n_users),
    'gender': np.random.choice(['M', 'F'], n_users),
    'occupation': np.random.choice(['ä¸Šç­æ—', 'å­¸ç”Ÿ', 'è‡ªç”±æ¥­', 'é€€ä¼‘', 'å®¶ç®¡', 'å…¬å‹™å“¡', 'é†«ç™‚', 'æ•™è‚²'], n_users),
    'city': np.random.choice(['å°åŒ—', 'æ–°åŒ—', 'æ¡ƒåœ’', 'å°ä¸­', 'å°å—', 'é«˜é›„', 'æ–°ç«¹', 'åŸºéš†'], n_users),
    'annual_income': np.random.randint(300000, 2000000, n_users)
}

users_df = pd.DataFrame(users_data)
print("âœ… ä½¿ç”¨è€…è³‡æ–™å»ºç«‹å®Œæˆ")
users_df.head()

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 1.1: å°‡ Pandas DataFrame è½‰æ›ç‚º PySpark DataFrame

# %%
# è½‰æ›ç‚º PySpark DataFrame
users_spark = spark.createDataFrame(users_df)

# è¨»å†Šç‚ºè‡¨æ™‚è¡¨æ ¼
users_spark.createOrReplaceTempView("users")

print("ğŸ“Š ä½¿ç”¨è€…è³‡æ–™è¡¨çµæ§‹ï¼š")
users_spark.printSchema()
print(f"\nç¸½ä½¿ç”¨è€…æ•¸ï¼š{users_spark.count()}")
users_spark.show(5)

# %% [md]
#### 1.2 ä¿¡ç”¨å¡æŒå¡äººè³‡æ–™å»ºç«‹

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# æ¯ä½ä½¿ç”¨è€…å¯èƒ½æŒæœ‰å¤šå¼µä¿¡ç”¨å¡ï¼Œå»ºç«‹ä¿¡ç”¨å¡åŸºæœ¬è³‡è¨Šï¼ŒåŒ…å«å¡è™Ÿã€å¡åˆ¥ã€é¡åº¦ç­‰

# %%
# å»ºç«‹ä¿¡ç”¨å¡è³‡æ–™
n_cards = 1500

card_types = ['é‡‘å¡', 'ç™½é‡‘å¡', 'éˆ¦é‡‘å¡', 'æ™®å¡', 'å•†å‹™å¡']
card_brands = ['VISA', 'MasterCard', 'JCB', 'American Express']
banks = ['å°æ–°éŠ€è¡Œ', 'åœ‹æ³°ä¸–è¯', 'ä¸­ä¿¡éŠ€è¡Œ', 'ç‰å±±éŠ€è¡Œ', 'å¯Œé‚¦éŠ€è¡Œ', 'ç¬¬ä¸€éŠ€è¡Œ']

# éš¨æ©Ÿåˆ†é…ä¿¡ç”¨å¡çµ¦ä½¿ç”¨è€…ï¼ˆæœ‰äº›ä½¿ç”¨è€…æœ‰å¤šå¼µå¡ï¼‰
card_owners = np.random.choice(users_df['user_id'].values, n_cards)

cards_data = {
    'card_id': [f'C{str(i).zfill(8)}' for i in range(1, n_cards + 1)],
    'user_id': card_owners,
    'card_number': [f'{random.randint(1000, 9999)}-{random.randint(1000, 9999)}-{random.randint(1000, 9999)}-{random.randint(1000, 9999)}' for _ in range(n_cards)],
    'card_type': np.random.choice(card_types, n_cards, p=[0.15, 0.25, 0.10, 0.40, 0.10]),
    'card_brand': np.random.choice(card_brands, n_cards, p=[0.40, 0.35, 0.15, 0.10]),
    'bank': np.random.choice(banks, n_cards),
    'credit_limit': np.random.choice([50000, 100000, 200000, 300000, 500000, 800000], n_cards),
    'issue_date': pd.to_datetime([datetime(2020, 1, 1) + timedelta(days=random.randint(0, 1825)) for _ in range(n_cards)]),
    'card_status': np.random.choice(['active', 'suspended', 'closed'], n_cards, p=[0.85, 0.10, 0.05])
}

cards_df = pd.DataFrame(cards_data)
print("âœ… ä¿¡ç”¨å¡è³‡æ–™å»ºç«‹å®Œæˆ")
cards_df.head()

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 1.2: è½‰æ›ä¿¡ç”¨å¡è³‡æ–™ç‚º Spark DataFrame

# %%
# è½‰æ›ç‚º PySpark DataFrame
cards_spark = spark.createDataFrame(cards_df)

# è¨»å†Šç‚ºè‡¨æ™‚è¡¨æ ¼
cards_spark.createOrReplaceTempView("cards")

print("ğŸ“Š ä¿¡ç”¨å¡è³‡æ–™è¡¨çµæ§‹ï¼š")
cards_spark.printSchema()
print(f"\nç¸½ä¿¡ç”¨å¡æ•¸ï¼š{cards_spark.count()}")
cards_spark.show(5)

# %% [md]
#### 1.3 ä¿¡ç”¨å¡åˆ·å¡äº¤æ˜“ç´€éŒ„å»ºç«‹

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# å»ºç«‹ä¿¡ç”¨å¡äº¤æ˜“æ˜ç´°ï¼ŒåŒ…å«äº¤æ˜“æ™‚é–“ã€é‡‘é¡ã€å•†åº—é¡åˆ¥ã€äº¤æ˜“ç‹€æ…‹ç­‰è³‡è¨Š

# %%
# å»ºç«‹äº¤æ˜“è³‡æ–™
n_transactions = 50000

# åªä½¿ç”¨ç‹€æ…‹ç‚º active çš„ä¿¡ç”¨å¡
active_cards = cards_df[cards_df['card_status'] == 'active']['card_id'].values

# å•†åº—é¡åˆ¥
merchant_categories = [
    'è¶…å¸‚', 'é¤å»³', 'åŠ æ²¹ç«™', 'ç™¾è²¨å…¬å¸', 'ç¶²è³¼', 
    'é›»å½±é™¢', 'æ›¸åº—', 'è—¥å±€', 'å’–å•¡å»³', 'æœé£¾åº—',
    '3Cè³£å ´', 'ä¾¿åˆ©å•†åº—', 'æ—…éŠ', 'é£¯åº—', 'é†«ç™‚'
]

# äº¤æ˜“åœ°é»
locations = ['å°åŒ—', 'æ–°åŒ—', 'æ¡ƒåœ’', 'å°ä¸­', 'å°å—', 'é«˜é›„', 'æ–°ç«¹', 'åŸºéš†', 'åœ‹å¤–']

# ç”Ÿæˆäº¤æ˜“ç´€éŒ„
transactions_data = {
    'transaction_id': [f'T{str(i).zfill(10)}' for i in range(1, n_transactions + 1)],
    'card_id': np.random.choice(active_cards, n_transactions),
    'transaction_date': pd.to_datetime([
        datetime(2024, 1, 1) + timedelta(
            days=random.randint(0, 365),
            hours=random.randint(0, 23),
            minutes=random.randint(0, 59)
        ) for _ in range(n_transactions)
    ]),
    'merchant_name': [f"{random.choice(merchant_categories)}{random.randint(1, 100)}è™Ÿåº—" for _ in range(n_transactions)],
    'merchant_category': np.random.choice(merchant_categories, n_transactions),
    'amount': np.random.gamma(2, 500, n_transactions).astype(int),  # ä½¿ç”¨ Gamma åˆ†å¸ƒæ¨¡æ“¬çœŸå¯¦äº¤æ˜“é‡‘é¡
    'location': np.random.choice(locations, n_transactions, p=[0.25, 0.20, 0.15, 0.15, 0.10, 0.08, 0.05, 0.01, 0.01]),
    'transaction_status': np.random.choice(['approved', 'declined', 'pending'], n_transactions, p=[0.90, 0.08, 0.02]),
    'is_online': np.random.choice([True, False], n_transactions, p=[0.35, 0.65])
}

transactions_df = pd.DataFrame(transactions_data)

# ç¢ºä¿é‡‘é¡ç‚ºæ­£æ•¸ä¸”åˆç†
transactions_df['amount'] = transactions_df['amount'].clip(lower=10, upper=100000)

print("âœ… äº¤æ˜“è³‡æ–™å»ºç«‹å®Œæˆ")
transactions_df.head()

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 1.3: è½‰æ›äº¤æ˜“è³‡æ–™ç‚º Spark DataFrame

# %%
# è½‰æ›ç‚º PySpark DataFrame
transactions_spark = spark.createDataFrame(transactions_df)

# è¨»å†Šç‚ºè‡¨æ™‚è¡¨æ ¼
transactions_spark.createOrReplaceTempView("transactions")

print("ğŸ“Š äº¤æ˜“è³‡æ–™è¡¨çµæ§‹ï¼š")
transactions_spark.printSchema()
print(f"\nç¸½äº¤æ˜“ç­†æ•¸ï¼š{transactions_spark.count()}")
transactions_spark.show(5)

# %% [md]
### ğŸ“š å–®å…ƒäºŒï¼šSQL åŸºç¤æŸ¥è©¢ç·´ç¿’

#### 2.1 SELECT èˆ‡ WHERE å­å¥

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# ä½¿ç”¨ SELECT é¸æ“‡æ¬„ä½ï¼ŒWHERE é€²è¡Œæ¢ä»¶ç¯©é¸

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 2.1: æŸ¥è©¢å¹´é½¡å¤§æ–¼40æ­²çš„ä½¿ç”¨è€…

# %%
query = """
SELECT user_id, user_name, age, city, occupation
FROM users
WHERE age > 40
ORDER BY age DESC
LIMIT 10
"""

result = spark.sql(query)
result.show()

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 2.2: æŸ¥è©¢å°åŒ—åœ°å€çš„é«˜æ”¶å…¥ä½¿ç”¨è€…ï¼ˆå¹´æ”¶å…¥è¶…é100è¬ï¼‰

# %%
query = """
SELECT user_id, user_name, age, city, annual_income
FROM users
WHERE city = 'å°åŒ—' AND annual_income > 1000000
ORDER BY annual_income DESC
"""

result = spark.sql(query)
result.show()

# %% [md]
#### 2.2 èšåˆå‡½æ•¸èˆ‡ GROUP BY

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# ä½¿ç”¨èšåˆå‡½æ•¸ï¼ˆCOUNT, SUM, AVG, MAX, MINï¼‰æ­é… GROUP BY é€²è¡Œè³‡æ–™çµ±è¨ˆ

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 2.3: çµ±è¨ˆå„åŸå¸‚çš„ä½¿ç”¨è€…æ•¸é‡

# %%
query = """
SELECT 
    city,
    COUNT(*) as user_count,
    AVG(age) as avg_age,
    AVG(annual_income) as avg_income
FROM users
GROUP BY city
ORDER BY user_count DESC
"""

result = spark.sql(query)
result.show()

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 2.4: çµ±è¨ˆå„å•†åº—é¡åˆ¥çš„äº¤æ˜“ç¸½é¡

# %%
query = """
SELECT 
    merchant_category,
    COUNT(*) as transaction_count,
    SUM(amount) as total_amount,
    AVG(amount) as avg_amount,
    MAX(amount) as max_amount
FROM transactions
WHERE transaction_status = 'approved'
GROUP BY merchant_category
ORDER BY total_amount DESC
"""

result = spark.sql(query)
result.show()

# %% [md]
### ğŸ“š å–®å…ƒä¸‰ï¼šJOIN é€£æ¥æŸ¥è©¢

#### 3.1 INNER JOIN

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# INNER JOIN å–å¾—å…©å€‹è¡¨æ ¼çš„äº¤é›†è³‡æ–™

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 3.1: æŸ¥è©¢ä½¿ç”¨è€…çš„ä¿¡ç”¨å¡è³‡è¨Š

# %%
query = """
SELECT 
    u.user_id,
    u.user_name,
    u.city,
    c.card_id,
    c.card_type,
    c.card_brand,
    c.credit_limit
FROM users u
INNER JOIN cards c ON u.user_id = c.user_id
WHERE c.card_status = 'active'
ORDER BY u.user_id
LIMIT 20
"""

result = spark.sql(query)
result.show()

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 3.2: æŸ¥è©¢äº¤æ˜“ç´€éŒ„èˆ‡æŒå¡äººè³‡è¨Š

# %%
query = """
SELECT 
    t.transaction_id,
    t.transaction_date,
    t.amount,
    t.merchant_category,
    c.card_type,
    u.user_name,
    u.city
FROM transactions t
INNER JOIN cards c ON t.card_id = c.card_id
INNER JOIN users u ON c.user_id = u.user_id
WHERE t.transaction_status = 'approved'
ORDER BY t.transaction_date DESC
LIMIT 20
"""

result = spark.sql(query)
result.show(truncate=False)

# %% [md]
#### 3.2 LEFT JOIN

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# LEFT JOIN ä¿ç•™å·¦è¡¨æ‰€æœ‰è³‡æ–™ï¼Œå³è¡¨æ²’æœ‰å°æ‡‰å‰‡é¡¯ç¤º NULL

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 3.3: æŸ¥è©¢æ‰€æœ‰ä½¿ç”¨è€…åŠå…¶ä¿¡ç”¨å¡æ•¸é‡ï¼ˆåŒ…å«æ²’æœ‰ä¿¡ç”¨å¡çš„ä½¿ç”¨è€…ï¼‰

# %%
query = """
SELECT 
    u.user_id,
    u.user_name,
    u.city,
    COUNT(c.card_id) as card_count
FROM users u
LEFT JOIN cards c ON u.user_id = c.user_id
GROUP BY u.user_id, u.user_name, u.city
ORDER BY card_count DESC, u.user_id
LIMIT 20
"""

result = spark.sql(query)
result.show()

# %% [md]
### ğŸ“š å–®å…ƒå››ï¼šé€²éš SQL æŸ¥è©¢

#### 4.1 å­æŸ¥è©¢ (Subquery)

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# å­æŸ¥è©¢æ˜¯åœ¨æŸ¥è©¢å…§éƒ¨å†é€²è¡Œå¦ä¸€å€‹æŸ¥è©¢ï¼Œå¯ç”¨æ–¼è¤‡é›œçš„æ¢ä»¶åˆ¤æ–·

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 4.1: æŸ¥è©¢äº¤æ˜“é‡‘é¡é«˜æ–¼å¹³å‡å€¼çš„äº¤æ˜“

# %%
query = """
SELECT 
    t.transaction_id,
    t.transaction_date,
    t.amount,
    t.merchant_category,
    u.user_name
FROM transactions t
INNER JOIN cards c ON t.card_id = c.card_id
INNER JOIN users u ON c.user_id = u.user_id
WHERE t.amount > (SELECT AVG(amount) FROM transactions)
    AND t.transaction_status = 'approved'
ORDER BY t.amount DESC
LIMIT 20
"""

result = spark.sql(query)
result.show()

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 4.2: æ‰¾å‡ºæ“æœ‰æœ€å¤šä¿¡ç”¨å¡çš„å‰10åä½¿ç”¨è€…

# %%
query = """
SELECT 
    u.user_id,
    u.user_name,
    u.occupation,
    u.annual_income,
    card_stats.card_count
FROM users u
INNER JOIN (
    SELECT user_id, COUNT(*) as card_count
    FROM cards
    WHERE card_status = 'active'
    GROUP BY user_id
) card_stats ON u.user_id = card_stats.user_id
ORDER BY card_stats.card_count DESC
LIMIT 10
"""

result = spark.sql(query)
result.show()

# %% [md]
#### 4.2 Window Functions (çª—å£å‡½æ•¸)

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# Window Functions å¯ä»¥åœ¨ä¸æ”¹è®Šçµæœé›†è¡Œæ•¸çš„æƒ…æ³ä¸‹é€²è¡Œèšåˆè¨ˆç®—

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 4.3: è¨ˆç®—æ¯ä½ä½¿ç”¨è€…çš„ç´¯ç©æ¶ˆè²»é‡‘é¡

# %%
query = """
SELECT 
    u.user_name,
    t.transaction_date,
    t.amount,
    t.merchant_category,
    SUM(t.amount) OVER (
        PARTITION BY u.user_id 
        ORDER BY t.transaction_date
    ) as cumulative_amount
FROM transactions t
INNER JOIN cards c ON t.card_id = c.card_id
INNER JOIN users u ON c.user_id = u.user_id
WHERE t.transaction_status = 'approved'
    AND u.user_id IN (SELECT user_id FROM users LIMIT 3)
ORDER BY u.user_name, t.transaction_date
"""

result = spark.sql(query)
result.show(30)

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 4.4: ç‚ºæ¯ä½ä½¿ç”¨è€…çš„äº¤æ˜“æ’åï¼ˆä¾é‡‘é¡ï¼‰

# %%
query = """
SELECT 
    user_name,
    transaction_date,
    amount,
    merchant_category,
    rank
FROM (
    SELECT 
        u.user_name,
        t.transaction_date,
        t.amount,
        t.merchant_category,
        RANK() OVER (
            PARTITION BY u.user_id 
            ORDER BY t.amount DESC
        ) as rank
    FROM transactions t
    INNER JOIN cards c ON t.card_id = c.card_id
    INNER JOIN users u ON c.user_id = u.user_id
    WHERE t.transaction_status = 'approved'
) ranked_transactions
WHERE rank <= 5
ORDER BY user_name, rank
LIMIT 30
"""

result = spark.sql(query)
result.show(30, truncate=False)

# %% [md]
### ğŸ“š å–®å…ƒäº”ï¼šè³‡æ–™åˆ†æå¯¦æˆ°

#### 5.1 æœˆåº¦æ¶ˆè²»åˆ†æ

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# ä½¿ç”¨æ—¥æœŸå‡½æ•¸é€²è¡Œæ™‚é–“åºåˆ—åˆ†æ

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 5.1: æ¯æœˆäº¤æ˜“çµ±è¨ˆ

# %%
query = """
SELECT 
    YEAR(transaction_date) as year,
    MONTH(transaction_date) as month,
    COUNT(*) as transaction_count,
    SUM(amount) as total_amount,
    AVG(amount) as avg_amount
FROM transactions
WHERE transaction_status = 'approved'
GROUP BY YEAR(transaction_date), MONTH(transaction_date)
ORDER BY year, month
"""

result = spark.sql(query)
result.show(12)

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 5.2: å„æœˆä»½ç†±é–€æ¶ˆè²»é¡åˆ¥

# %%
query = """
SELECT 
    YEAR(transaction_date) as year,
    MONTH(transaction_date) as month,
    merchant_category,
    COUNT(*) as transaction_count,
    SUM(amount) as total_amount
FROM transactions
WHERE transaction_status = 'approved'
GROUP BY YEAR(transaction_date), MONTH(transaction_date), merchant_category
ORDER BY year, month, total_amount DESC
"""

result = spark.sql(query)
result.show(30)

# %% [md]
#### 5.2 ä½¿ç”¨è€…æ¶ˆè²»è¡Œç‚ºåˆ†æ

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# åˆ†æä½¿ç”¨è€…çš„æ¶ˆè²»æ¨¡å¼èˆ‡åå¥½

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 5.3: é«˜æ¶ˆè²»ä½¿ç”¨è€…åˆ†æ

# %%
query = """
SELECT 
    u.user_id,
    u.user_name,
    u.age,
    u.occupation,
    u.city,
    COUNT(DISTINCT t.transaction_id) as transaction_count,
    SUM(t.amount) as total_spending,
    AVG(t.amount) as avg_transaction_amount,
    COUNT(DISTINCT t.merchant_category) as category_diversity
FROM users u
INNER JOIN cards c ON u.user_id = c.user_id
INNER JOIN transactions t ON c.card_id = t.card_id
WHERE t.transaction_status = 'approved'
GROUP BY u.user_id, u.user_name, u.age, u.occupation, u.city
HAVING SUM(t.amount) > 100000
ORDER BY total_spending DESC
LIMIT 20
"""

result = spark.sql(query)
result.show()

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 5.4: ä½¿ç”¨è€…æœ€å¸¸æ¶ˆè²»çš„é¡åˆ¥

# %%
query = """
WITH user_category_spending AS (
    SELECT 
        u.user_id,
        u.user_name,
        t.merchant_category,
        COUNT(*) as transaction_count,
        SUM(t.amount) as category_spending,
        RANK() OVER (
            PARTITION BY u.user_id 
            ORDER BY COUNT(*) DESC
        ) as category_rank
    FROM users u
    INNER JOIN cards c ON u.user_id = c.user_id
    INNER JOIN transactions t ON c.card_id = t.card_id
    WHERE t.transaction_status = 'approved'
    GROUP BY u.user_id, u.user_name, t.merchant_category
)
SELECT 
    user_id,
    user_name,
    merchant_category as favorite_category,
    transaction_count,
    category_spending
FROM user_category_spending
WHERE category_rank = 1
ORDER BY transaction_count DESC
LIMIT 20
"""

result = spark.sql(query)
result.show()

# %% [md]
#### 5.3 ç•°å¸¸äº¤æ˜“åµæ¸¬

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# ä½¿ç”¨çµ±è¨ˆæ–¹æ³•æ‰¾å‡ºå¯èƒ½çš„ç•°å¸¸äº¤æ˜“

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 5.5: æ‰¾å‡ºå–®ç­†é‡‘é¡ç•°å¸¸é«˜çš„äº¤æ˜“

# %%
query = """
WITH transaction_stats AS (
    SELECT 
        merchant_category,
        AVG(amount) as avg_amount,
        STDDEV(amount) as stddev_amount
    FROM transactions
    WHERE transaction_status = 'approved'
    GROUP BY merchant_category
)
SELECT 
    t.transaction_id,
    t.transaction_date,
    t.merchant_category,
    t.amount,
    u.user_name,
    ROUND(ts.avg_amount, 2) as category_avg,
    ROUND((t.amount - ts.avg_amount) / ts.stddev_amount, 2) as z_score
FROM transactions t
INNER JOIN cards c ON t.card_id = c.card_id
INNER JOIN users u ON c.user_id = u.user_id
INNER JOIN transaction_stats ts ON t.merchant_category = ts.merchant_category
WHERE t.transaction_status = 'approved'
    AND ABS((t.amount - ts.avg_amount) / ts.stddev_amount) > 3
ORDER BY z_score DESC
LIMIT 20
"""

result = spark.sql(query)
result.show()

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 5.6: åµæ¸¬çŸ­æ™‚é–“å…§çš„å¤šæ¬¡äº¤æ˜“

# %%
query = """
WITH transaction_intervals AS (
    SELECT 
        t1.transaction_id,
        t1.card_id,
        t1.transaction_date,
        t1.amount,
        t1.merchant_category,
        COUNT(t2.transaction_id) as transactions_within_hour
    FROM transactions t1
    LEFT JOIN transactions t2 
        ON t1.card_id = t2.card_id
        AND t2.transaction_date BETWEEN 
            (t1.transaction_date - INTERVAL 3 HOURS) 
            AND t1.transaction_date
        AND t2.transaction_id != t1.transaction_id
    WHERE t1.transaction_status = 'approved'
    GROUP BY t1.transaction_id, t1.card_id, t1.transaction_date, t1.amount, t1.merchant_category
)
SELECT 
    ti.transaction_id,
    ti.transaction_date,
    ti.amount,
    ti.merchant_category,
    ti.transactions_within_hour,
    u.user_name,
    c.card_type
FROM transaction_intervals ti
INNER JOIN cards c ON ti.card_id = c.card_id
INNER JOIN users u ON c.user_id = u.user_id
WHERE ti.transactions_within_hour >= 3
ORDER BY ti.transactions_within_hour DESC, ti.transaction_date DESC
LIMIT 20
"""

result = spark.sql(query)
result.show(truncate=False)

# %% [md]
### ğŸ“š å–®å…ƒå…­ï¼šPySpark DataFrame API æ“ä½œ

#### 6.1 DataFrame åŸºæœ¬æ“ä½œ

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# ä½¿ç”¨ PySpark DataFrame API é€²è¡Œè³‡æ–™æ“ä½œï¼Œèˆ‡ SQL æ•ˆæœç›¸åŒä½†èªæ³•ä¸åŒ

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 6.1: ä½¿ç”¨ DataFrame API é€²è¡Œç¯©é¸èˆ‡èšåˆ

# %%
# çµ±è¨ˆå„åŸå¸‚çš„å¹³å‡å¹´æ”¶å…¥
result = users_spark \
    .groupBy('city') \
    .agg(
        count('*').alias('user_count'),
        avg('annual_income').alias('avg_income'),
        avg('age').alias('avg_age')
    ) \
    .orderBy(desc('avg_income'))

result.show()

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 6.2: ä½¿ç”¨ DataFrame API é€²è¡Œ JOIN æ“ä½œ

# %%
# æŸ¥è©¢äº¤æ˜“èˆ‡ä½¿ç”¨è€…è³‡è¨Š
result = transactions_spark \
    .filter(col('transaction_status') == 'approved') \
    .join(cards_spark, 'card_id') \
    .join(users_spark, 'user_id') \
    .select(
        'transaction_id',
        'transaction_date',
        'amount',
        'merchant_category',
        'user_name',
        'city'
    ) \
    .orderBy(desc('transaction_date')) \
    .limit(20)

result.show(truncate=False)

# %% [md]
#### 6.2 Window Functions with DataFrame API

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# ä½¿ç”¨ DataFrame API å¯¦ç¾çª—å£å‡½æ•¸

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 6.3: è¨ˆç®—æ¯ä½ä½¿ç”¨è€…çš„æ¶ˆè²»æ’å

# %%
# å®šç¾©çª—å£
windowSpec = Window.partitionBy('user_id').orderBy(desc('amount'))

# è¨ˆç®—æ’å
result = transactions_spark \
    .filter(col('transaction_status') == 'approved') \
    .join(cards_spark, 'card_id') \
    .join(users_spark, 'user_id') \
    .select(
        'user_name',
        'transaction_date',
        'amount',
        'merchant_category',
        rank().over(windowSpec).alias('rank')
    ) \
    .filter(col('rank') <= 5) \
    .orderBy('user_name', 'rank')

result.show(30)

# %% [md]
### ğŸ“š å–®å…ƒä¸ƒï¼šè³‡æ–™åŒ¯å‡ºèˆ‡å„²å­˜

#### 7.1 å„²å­˜ç‚º Parquet æ ¼å¼

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# Parquet æ˜¯ä¸€ç¨®åˆ—å¼å­˜å„²æ ¼å¼ï¼Œé©åˆå¤§æ•¸æ“šåˆ†æ

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 7.1: å°‡è™•ç†å¾Œçš„è³‡æ–™å„²å­˜ç‚º Parquet

# %%
# å»ºç«‹å½™æ•´å ±è¡¨
summary_report = spark.sql("""
    SELECT 
        u.user_id,
        u.user_name,
        u.city,
        u.age,
        u.occupation,
        COUNT(DISTINCT c.card_id) as card_count,
        COUNT(DISTINCT t.transaction_id) as transaction_count,
        COALESCE(SUM(t.amount), 0) as total_spending,
        COALESCE(AVG(t.amount), 0) as avg_transaction_amount
    FROM users u
    LEFT JOIN cards c ON u.user_id = c.user_id AND c.card_status = 'active'
    LEFT JOIN transactions t ON c.card_id = t.card_id AND t.transaction_status = 'approved'
    GROUP BY u.user_id, u.user_name, u.city, u.age, u.occupation
    ORDER BY total_spending DESC
""")

# é¡¯ç¤ºçµæœ
summary_report.show(20)

# å„²å­˜ç‚º Parquetï¼ˆè¨»è§£æ‰ä»¥é¿å…å¯¦éš›å¯«å…¥ï¼‰
# summary_report.write.mode('overwrite').parquet('./output/user_summary_report.parquet')
print("âœ… è³‡æ–™è™•ç†å®Œæˆï¼ˆå„²å­˜å·²è¨»è§£ï¼‰")

# %% [md]
#### 7.2 å»ºç«‹è³‡æ–™è¦–åœ–

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# å»ºç«‹å¯é‡è¤‡ä½¿ç”¨çš„è³‡æ–™è¦–åœ–ï¼ˆViewï¼‰

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 7.2: å»ºç«‹äº¤æ˜“å½™ç¸½è¦–åœ–

# %%
# å»ºç«‹è¦–åœ–
spark.sql("""
    CREATE OR REPLACE TEMP VIEW transaction_summary AS
    SELECT 
        DATE(t.transaction_date) as transaction_day,
        t.merchant_category,
        COUNT(*) as transaction_count,
        SUM(t.amount) as daily_total,
        AVG(t.amount) as daily_avg
    FROM transactions t
    WHERE t.transaction_status = 'approved'
    GROUP BY DATE(t.transaction_date), t.merchant_category
""")

# ä½¿ç”¨è¦–åœ–æŸ¥è©¢
result = spark.sql("""
    SELECT *
    FROM transaction_summary
    WHERE daily_total > 10000
    ORDER BY daily_total DESC
    LIMIT 20
""")

result.show()

# %% [md]
### ğŸ“š å–®å…ƒå…«ï¼šé€²éšåˆ†ææ¡ˆä¾‹

#### 8.1 RFM åˆ†æï¼ˆRecency, Frequency, Monetaryï¼‰

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# RFM æ˜¯è¡¡é‡å®¢æˆ¶åƒ¹å€¼çš„ç¶“å…¸æ¨¡å‹ï¼Œåˆ†ææœ€è¿‘æ¶ˆè²»ã€æ¶ˆè²»é »ç‡ã€æ¶ˆè²»é‡‘é¡

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 8.1: è¨ˆç®—ä½¿ç”¨è€… RFM æŒ‡æ¨™

# %%
query = """
WITH rfm_data AS (
    SELECT 
        u.user_id,
        u.user_name,
        u.city,
        DATEDIFF(CURRENT_DATE(), MAX(t.transaction_date)) as recency,
        COUNT(DISTINCT t.transaction_id) as frequency,
        SUM(t.amount) as monetary
    FROM users u
    INNER JOIN cards c ON u.user_id = c.user_id
    INNER JOIN transactions t ON c.card_id = t.card_id
    WHERE t.transaction_status = 'approved'
    GROUP BY u.user_id, u.user_name, u.city
),
rfm_scores AS (
    SELECT 
        *,
        NTILE(5) OVER (ORDER BY recency ASC) as r_score,
        NTILE(5) OVER (ORDER BY frequency DESC) as f_score,
        NTILE(5) OVER (ORDER BY monetary DESC) as m_score
    FROM rfm_data
)
SELECT 
    user_id,
    user_name,
    city,
    recency,
    frequency,
    monetary,
    r_score,
    f_score,
    m_score,
    (r_score + f_score + m_score) as rfm_total_score,
    CASE 
        WHEN r_score >= 4 AND f_score >= 4 AND m_score >= 4 THEN 'é‡è¦åƒ¹å€¼å®¢æˆ¶'
        WHEN r_score >= 4 AND f_score >= 3 THEN 'é‡è¦ç™¼å±•å®¢æˆ¶'
        WHEN r_score >= 3 AND f_score >= 3 AND m_score >= 3 THEN 'é‡è¦ä¿æŒå®¢æˆ¶'
        WHEN r_score >= 4 AND m_score >= 4 THEN 'é‡è¦æŒ½ç•™å®¢æˆ¶'
        WHEN r_score <= 2 AND f_score <= 2 THEN 'æµå¤±å®¢æˆ¶'
        ELSE 'ä¸€èˆ¬å®¢æˆ¶'
    END as customer_segment
FROM rfm_scores
ORDER BY rfm_total_score DESC
LIMIT 30
"""

result = spark.sql(query)
result.show(30, truncate=False)

# %% [md]
##### ğŸ“Œ ç¯„ä¾‹ 8.2: å®¢æˆ¶åˆ†ç¾¤çµ±è¨ˆ

# %%
query = """
WITH rfm_data AS (
    SELECT 
        u.user_id,
        DATEDIFF(CURRENT_DATE(), MAX(t.transaction_date)) as recency,
        COUNT(DISTINCT t.transaction_id) as frequency,
        SUM(t.amount) as monetary
    FROM users u
    INNER JOIN cards c ON u.user_id = c.user_id
    INNER JOIN transactions t ON c.card_id = t.card_id
    WHERE t.transaction_status = 'approved'
    GROUP BY u.user_id
),
rfm_scores AS (
    SELECT 
        *,
        NTILE(5) OVER (ORDER BY recency ASC) as r_score,
        NTILE(5) OVER (ORDER BY frequency DESC) as f_score,
        NTILE(5) OVER (ORDER BY monetary DESC) as m_score
    FROM rfm_data
),
customer_segments AS (
    SELECT 
        user_id,
        CASE 
            WHEN r_score >= 4 AND f_score >= 4 AND m_score >= 4 THEN 'é‡è¦åƒ¹å€¼å®¢æˆ¶'
            WHEN r_score >= 4 AND f_score >= 3 THEN 'é‡è¦ç™¼å±•å®¢æˆ¶'
            WHEN r_score >= 3 AND f_score >= 3 AND m_score >= 3 THEN 'é‡è¦ä¿æŒå®¢æˆ¶'
            WHEN r_score >= 4 AND m_score >= 4 THEN 'é‡è¦æŒ½ç•™å®¢æˆ¶'
            WHEN r_score <= 2 AND f_score <= 2 THEN 'æµå¤±å®¢æˆ¶'
            ELSE 'ä¸€èˆ¬å®¢æˆ¶'
        END as customer_segment
    FROM rfm_scores
)
SELECT 
    customer_segment,
    COUNT(*) as customer_count,
    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentage
FROM customer_segments
GROUP BY customer_segment
ORDER BY customer_count DESC
"""

result = spark.sql(query)
result.show()

# %% [md]
### ğŸ“š ç¸½çµèˆ‡æ¸…ç†

#### èª²ç¨‹é‡é»å›é¡§

# ğŸˆ æœ¬èª²ç¨‹æ¶µè“‹å…§å®¹ï¼š
# 
# 1. **è³‡æ–™å»ºç«‹**ï¼šä½¿ç”¨ Numpyã€Pandas å»ºç«‹æ¨¡æ“¬çš„ä¿¡ç”¨å¡äº¤æ˜“è³‡æ–™
# 2. **SQL åŸºç¤**ï¼šSELECTã€WHEREã€GROUP BYã€èšåˆå‡½æ•¸
# 3. **JOIN æ“ä½œ**ï¼šINNER JOINã€LEFT JOIN å¤šè¡¨é—œè¯æŸ¥è©¢
# 4. **é€²éšæŸ¥è©¢**ï¼šå­æŸ¥è©¢ã€Window Functionsã€CTE (Common Table Expression)
# 5. **è³‡æ–™åˆ†æ**ï¼šæ™‚é–“åºåˆ—åˆ†æã€ä½¿ç”¨è€…è¡Œç‚ºåˆ†æã€ç•°å¸¸åµæ¸¬
# 6. **DataFrame API**ï¼šPySpark DataFrame æ“ä½œæ–¹æ³•
# 7. **å¯¦æˆ°æ¡ˆä¾‹**ï¼šRFM å®¢æˆ¶åƒ¹å€¼åˆ†æã€å®¢æˆ¶åˆ†ç¾¤

# %%
# æ¸…ç†è³‡æº
print("ğŸ“Š è³‡æ–™çµ±è¨ˆç¸½è¦½ï¼š")
print(f"ä½¿ç”¨è€…ç¸½æ•¸ï¼š{users_spark.count()}")
print(f"ä¿¡ç”¨å¡ç¸½æ•¸ï¼š{cards_spark.count()}")
print(f"äº¤æ˜“ç¸½ç­†æ•¸ï¼š{transactions_spark.count()}")
print(f"æ ¸å‡†äº¤æ˜“ç­†æ•¸ï¼š{transactions_spark.filter(col('transaction_status') == 'approved').count()}")

# é¡¯ç¤ºå·²å»ºç«‹çš„è‡¨æ™‚è¡¨æ ¼
print("\nå·²å»ºç«‹çš„è‡¨æ™‚è¡¨æ ¼ï¼š")
spark.sql("SHOW TABLES").show()

# %%
# åœæ­¢ Spark Sessionï¼ˆå¯é¸ï¼‰
# spark.stop()
print("\nâœ… èª²ç¨‹çµæŸï¼")
