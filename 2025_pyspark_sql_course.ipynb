{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16897f87",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "# PySpark SQL 零基礎入門教學 🎈\n",
    "\n",
    "\n",
    "這份教學專為完全沒有 SQL 經驗的你設計！\n",
    "SQL 就像是跟資料庫溝通的語言，學會了就能輕鬆管理各種資料。\n",
    "我們會用生活化的例子，讓你快速上手！✨\n",
    "\n",
    "作者：QChoice AI 教學團隊\n",
    "日期：2025-01-15\n",
    "\n",
    "\n",
    "🎉 歡迎學習 SQL！讓我們開始吧！\n",
    "\n",
    "--------------------------------------------------\n",
    "\n",
    "🎯 環境設定 - 準備開發環境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f058402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# 創建 Spark Session - 啟動資料處理引擎\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SQL入門教學\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a8579f",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 📚 第一章：基礎查詢操作\n",
    "\n",
    "### 1️⃣ SELECT - 查詢資料（從資料表中取出你要的資料）\n",
    "\n",
    "🎈 概念解釋：\n",
    "SELECT 是最基本的查詢指令，用來從資料表中取出資料。\n",
    "就像在圖書館找書一樣，你可以選擇看全部的書，或只看特定類型的書。\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，從玩具資料表中選取所有玩具的名稱和顏色，\n",
    "並顯示前5筆資料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313bb875",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# 範例資料：建立一個玩具清單\n",
    "toys_data = [\n",
    "    (1, \"小熊\", \"棕色\", 299),\n",
    "    (2, \"機器人\", \"藍色\", 599),\n",
    "    (3, \"芭比娃娃\", \"粉色\", 399),\n",
    "    (4, \"樂高\", \"彩色\", 899),\n",
    "    (5, \"小汽車\", \"紅色\", 199)\n",
    "]\n",
    "toys_df = spark.createDataFrame(toys_data, [\"id\", \"名稱\", \"顏色\", \"價格\"])\n",
    "toys_df.createOrReplaceTempView(\"toys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50257d59",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 📌 範例 1: SELECT - 選取玩具名稱和顏色\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156608e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📌 玩具資料表內容：\")\n",
    "toys_df.show()\n",
    "\n",
    "# SQL 方式 1：使用 SQL 語法\n",
    "result = spark.sql(\"SELECT `名稱`, `顏色` FROM toys\")\n",
    "print(\"📌 使用 SQL 語法的結果：\")\n",
    "result.show()\n",
    "\n",
    "# PySpark DataFrame 方式（另一種寫法）\n",
    "result2 = toys_df.select(\"名稱\", \"價格\")\n",
    "print(\"📌 使用 DataFrame API 的結果：\")\n",
    "result2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1885f0",
   "metadata": {
    "lines_to_next_cell": 0,
    "region_name": "md"
   },
   "source": [
    "### 2️⃣ INSERT - 新增資料（在資料表中加入新的記錄）\n",
    "\n",
    "🎈 概念解釋：\n",
    "INSERT 用來在資料表中新增資料。\n",
    "就像在筆記本上寫下新的一筆記錄，資料庫會永久保存這些資料。\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，將一個新玩具「泰迪熊」新增到玩具資料表中，\n",
    "顏色是「白色」，價格是 450 元。\n",
    "\n",
    "#### 📌 範例 2: INSERT - 新增新玩具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c8abbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📌 玩具資料表內容：\")\n",
    "toys_df.show()\n",
    "\n",
    "# 新玩具資料\n",
    "new_toy = [(6, \"泰迪熊\", \"白色\", 450)]\n",
    "new_toy_df = spark.createDataFrame(new_toy, [\"id\", \"名稱\", \"顏色\", \"價格\"])\n",
    "print(\"📌 新增的玩具資料：\")\n",
    "new_toy_df.show()\n",
    "\n",
    "# 合併資料（模擬 INSERT）\n",
    "union_toys_df = toys_df.union(new_toy_df)\n",
    "union_toys_df.createOrReplaceTempView(\"toys\")\n",
    "print(\"📌 新玩具已新增到資料表中！\")\n",
    "union_toys_df.show()\n",
    "\n",
    "spark.sql(\"SELECT * FROM toys\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe0cc0f",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "#### 3️⃣ UPDATE - 更新資料（修改已存在的資料內容）\n",
    "\n",
    "🎈 概念解釋：\n",
    "UPDATE 用來修改資料表中已存在的資料。\n",
    "就像修改文件內容一樣，把舊的資料改成新的資料。\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，將玩具「小熊」的價格從 299 元更新為 350 元。\n",
    "\n",
    "PySpark 沒有直接的 UPDATE，我們用 withColumn 和 when\n",
    "\n",
    "#### 📌 範例 3: UPDATE - 更新小熊的價格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac41bb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "toys_df = toys_df.withColumn(\n",
    "    \"價格\",\n",
    "    when(col(\"名稱\") == \"小熊\", 350).otherwise(col(\"價格\"))\n",
    ")\n",
    "toys_df.createOrReplaceTempView(\"toys\")\n",
    "spark.sql(\"SELECT * FROM toys WHERE `名稱` = '小熊'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b69e51",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 4️⃣ DELETE - 刪除資料（移除不需要的記錄）\n",
    "\n",
    "🎈 概念解釋：\n",
    "DELETE 用來從資料表中移除資料。\n",
    "就像刪除檔案一樣，被刪除的資料就不會再出現了。\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，刪除價格低於 200 元的所有玩具。\n",
    "\n",
    "#### 📌 範例 4: DELETE - 刪除便宜的玩具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477df715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 filter 保留價格 >= 200 的玩具\n",
    "toys_df = toys_df.filter(col(\"價格\") >= 200)\n",
    "toys_df.createOrReplaceTempView(\"toys\")\n",
    "spark.sql(\"SELECT * FROM toys\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de388de9",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 📚 第二章：資料庫和資料表管理\n",
    "\n",
    "### 5️⃣ CREATE DATABASE - 建立資料庫（建立資料的容器）\n",
    "🎈 概念解釋：\n",
    "CREATE DATABASE 用來建立一個新的資料庫。\n",
    "資料庫就像一個資料夾，裡面可以存放很多個資料表。\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，建立一個名為「toy_warehouse」的資料庫。\n",
    "\n",
    "⚠️ 注意：PySpark 預設不支援 CREATE DATABASE（需要 Hive）\n",
    "在 PySpark 中，我們使用臨時視圖（Temp Views）來組織資料\n",
    "\n",
    "### 📌 範例 5: CREATE DATABASE - 建立資料庫（概念說明）\n",
    "\n",
    "在 PySpark 中，我們不需要建立資料庫\n",
    "直接使用臨時視圖即可\n",
    "在標準 SQL 中：\n",
    "  CREATE DATABASE toy_warehouse\n",
    "\n",
    "在 PySpark 中：\n",
    "  使用臨時視圖來組織資料，不需要明確建立資料庫\n",
    "  臨時視圖會自動在 'default' 命名空間中\n",
    "  \n",
    "如果需要資料庫功能，可以：\n",
    "  1. 使用 Hive：設定 spark.sql.catalogImplementation=hive\n",
    "  2. 使用 Delta Lake：提供更強大的資料管理功能\n",
    "  3. 使用檔案系統：直接儲存為 Parquet/CSV 等格式\n",
    "\n",
    "### 6️⃣ CREATE TABLE - 建立資料表（定義資料的結構）\n",
    "\n",
    "🎈 概念解釋：\n",
    "CREATE TABLE 用來建立一個新的資料表，並定義欄位名稱和資料型態。\n",
    "就像建立一個 Excel 工作表，先定義好有哪些欄位。\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，建立一個「學生成績」資料表，\n",
    "包含學生姓名、科目、分數三個欄位。\n",
    "\n",
    "⚠️ 注意：PySpark 預設不支援 CREATE TABLE DDL 語法（需要 Hive）\n",
    "我們使用 DataFrame 來模擬建立資料表的效果\n",
    "\n",
    "#### 📌 範例 6: CREATE TABLE - 建立資料表（使用 DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d94aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法 1：使用 DataFrame 定義結構並建立臨時視圖\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "students_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"subject\", StringType(), True),\n",
    "    StructField(\"score\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# 建立空的 DataFrame 作為「資料表」\n",
    "students_df = spark.createDataFrame([], students_schema)\n",
    "students_df.createOrReplaceTempView(\"students\")\n",
    "\n",
    "print(\"✅ 已建立 students 臨時視圖（類似資料表）\")\n",
    "print(\"資料表結構：\")\n",
    "\n",
    "students_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687643b8",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 7️⃣ DROP DATABASE - 刪除資料庫（整個倉庫都拆掉）\n",
    "\n",
    "🎈 概念解釋：\n",
    "DROP DATABASE 會刪除整個資料庫及其所有內容\n",
    "裡面的所有資料表都會被刪除，使用時需特別小心\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，刪除名為「old_warehouse」的資料庫。\n",
    "（注意：會刪除所有裡面的資料表）\n",
    "\n",
    "⚠️ 注意：PySpark 預設不支援 CREATE/DROP DATABASE（需要 Hive）\n",
    "這裡僅作為概念說明\n",
    "\n",
    "#### 📌 範例 7: DROP DATABASE - 刪除資料庫（概念說明\n",
    "\n",
    "在 PySpark 中，我們使用臨時視圖而非資料庫\n",
    "概念說明：\n",
    "\n",
    "在標準 SQL 中：\n",
    "  DROP DATABASE IF EXISTS old_warehouse\n",
    "\n",
    "在 PySpark 中：\n",
    "  使用臨時視圖（Temp Views）來組織資料\n",
    "  可以透過命名規則來模擬資料庫分組，例如：\n",
    "  - db1_table1\n",
    "  - db1_table2\n",
    "  - db2_table1\n",
    "\n",
    "### 8️⃣ DROP TABLE - 刪除資料表（丟掉一個盒子）\n",
    "🎈 概念解釋：\n",
    "DROP TABLE 會刪除整個資料表\n",
    "資料庫仍然存在，只是移除了這個資料表\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，刪除「舊玩具」資料表。\n",
    "\n",
    "⚠️ 注意：在 PySpark 中，我們使用 dropTempView 來刪除臨時視圖\n",
    "\n",
    "#### 📌 範例 8: DROP TABLE - 刪除資料表（使用 dropTempView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c421b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先建立一個測試用的臨時視圖\n",
    "test_data = [(1, \"舊玩具\")]\n",
    "test_df = spark.createDataFrame(test_data, [\"id\", \"name\"])\n",
    "test_df.createOrReplaceTempView(\"old_toys\")\n",
    "print(\"✅ 已建立 old_toys 臨時視圖\")\n",
    "\n",
    "# 刪除臨時視圖\n",
    "spark.catalog.dropTempView(\"old_toys\")\n",
    "print(\"✅ 已刪除 old_toys 臨時視圖\")\n",
    "\n",
    "# 驗證是否已刪除\n",
    "try:\n",
    "    spark.sql(\"SELECT * FROM old_toys\").show()\n",
    "except Exception as e:\n",
    "    print(f\"✅ 確認已刪除：{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5dd744",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 9️⃣ ALTER TABLE - 修改資料表（改造盒子）\n",
    "\n",
    "🎈 概念解釋：\n",
    "ALTER TABLE 可以新增欄位、修改欄位名稱或改變欄位型態\n",
    "常用於調整資料表結構\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，在玩具資料表中新增一個「製造商」欄位。\n",
    "\n",
    "#### 📌 範例 9: ALTER TABLE - 修改資料表結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f68a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新增欄位\n",
    "toys_df = toys_df.withColumn(\"製造商\", lit(\"快樂玩具公司\"))\n",
    "toys_df.createOrReplaceTempView(\"toys\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM toys LIMIT 3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c25f3c",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 🔟 TRUNCATE TABLE - 清空資料表（把盒子裡的東西全倒掉）\n",
    "\n",
    "🎈 概念解釋：\n",
    "TRUNCATE TABLE 會清空資料表中的所有資料\n",
    "但資料表結構保留，只是沒有任何記錄\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，清空「暫存資料」資料表的所有內容。\n",
    "\n",
    "#### 📌 範例 10: TRUNCATE TABLE - 清空資料表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae64c7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立空的 DataFrame（模擬 TRUNCATE）\n",
    "empty_df = spark.createDataFrame([], toys_df.schema)\n",
    "print(\"✅ 資料表已清空（保留結構）\")\n",
    "\n",
    "empty_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060b9aed",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 📚 第三章：索引管理\n",
    "\n",
    "### 1️⃣1️⃣ CREATE INDEX - 建立索引（做一本目錄）\n",
    "🎈 概念解釋：\n",
    "CREATE INDEX 建立索引可以加快查詢速度\n",
    "想找東西的時候可以更快找到\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，在玩具資料表的「名稱」欄位上建立索引，\n",
    "以加快查詢速度。（說明：PySpark 使用 cache 來優化）\n",
    "\n",
    "#### 📌 範例 11: CREATE INDEX - 優化查詢速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa3b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark 使用 cache() 來優化常用查詢\n",
    "toys_df.cache()\n",
    "print(\"✅ 資料已快取，查詢會更快！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4511f76",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 1️⃣2️⃣ DROP INDEX - 刪除索引（把目錄撕掉）\n",
    "\n",
    "🎈 概念解釋：\n",
    "DROP INDEX 刪除不需要的索引\n",
    "不會影響內容，只是找東西會慢一點\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，移除玩具資料表上的快取索引。\n",
    "\n",
    "#### 📌 範例 12: DROP INDEX - 移除快取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f315edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "toys_df.unpersist()\n",
    "print(\"✅ 快取已移除\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0829f7dd",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 📚 第四章：資料表連接（JOIN）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9febd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 準備範例資料：學生和成績\n",
    "students_data = [\n",
    "    (1, \"小明\", 10),\n",
    "    (2, \"小華\", 10),\n",
    "    (3, \"小美\", 11)\n",
    "]\n",
    "\n",
    "scores_data = [\n",
    "    (1, \"數學\", 85),\n",
    "    (2, \"數學\", 90),\n",
    "    (3, \"數學\", 88),\n",
    "    (4, \"數學\", 92)  # 這個學生不在學生表中\n",
    "]\n",
    "\n",
    "students_df = spark.createDataFrame(students_data, [\"id\", \"姓名\", \"年齡\"])\n",
    "\n",
    "scores_df = spark.createDataFrame(scores_data, [\"student_id\", \"科目\", \"分數\"])\n",
    "\n",
    "students_df.createOrReplaceTempView(\"students\")\n",
    "scores_df.createOrReplaceTempView(\"scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e285d602",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 1️⃣3️⃣ INNER JOIN - 內部連接（只拿兩邊都有的）\n",
    "\n",
    "🎈 概念解釋：\n",
    "INNER JOIN 只保留兩個資料表都有匹配的記錄\n",
    "只有兩邊都有的才會被選出來\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，將學生資料表和成績資料表做 INNER JOIN，\n",
    "只顯示有成績記錄的學生資訊。\n",
    "\n",
    "#### 📌 範例 13: INNER JOIN - 兩邊都有的才留下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317b607",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "    SELECT s.`姓名`, sc.`科目`, sc.`分數`\n",
    "    FROM students s\n",
    "    INNER JOIN scores sc ON s.id = sc.student_id\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ab8bcb",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 1️⃣4️⃣ LEFT JOIN - 左連接（保留左邊全部）\n",
    "\n",
    "🎈 概念解釋：\n",
    "LEFT JOIN 保留左表的所有記錄\n",
    "左邊的全部保留，右邊有配對的就加上，沒有就留空\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，顯示所有學生，即使他們沒有成績記錄也要顯示。\n",
    "\n",
    "#### 📌 範例 14: LEFT JOIN - 保留所有學生"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4ad349",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "    SELECT s.`姓名`, sc.`科目`, sc.`分數`\n",
    "    FROM students s\n",
    "    LEFT JOIN scores sc ON s.id = sc.student_id\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1f43b6",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 1️⃣5️⃣ RIGHT JOIN - 右連接（保留右邊全部）\n",
    "\n",
    "🎈 概念解釋：\n",
    "RIGHT JOIN 保留右表的所有記錄\n",
    "右邊的全部保留，左邊有配對的就加上\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，顯示所有成績記錄，\n",
    "即使某些成績找不到對應的學生也要顯示。\n",
    "\n",
    "#### 📌 範例 15: RIGHT JOIN - 保留所有成績"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc075e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "    SELECT s.`姓名`, sc.`科目`, sc.`分數`\n",
    "    FROM students s\n",
    "    RIGHT JOIN scores sc ON s.id = sc.student_id\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ccba16",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 1️⃣6️⃣ FULL OUTER JOIN - 全外部連接（兩邊都保留）\n",
    "\n",
    "🎈 概念解釋：\n",
    "FULL OUTER JOIN 保留兩個資料表的所有記錄\n",
    "不管有沒有配對，全部都要！\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，顯示所有學生和所有成績記錄，\n",
    "不管是否有配對都要顯示。\n",
    "\n",
    "#### 📌 範例 16: FULL OUTER JOIN - 全部都要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65412d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "    SELECT s.`姓名`, sc.`科目`, sc.`分數`\n",
    "    FROM students s\n",
    "    FULL OUTER JOIN scores sc ON s.id = sc.student_id\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n",
    "## 📚 第五章：集合操作\n",
    "# 準備範例資料\n",
    "class_a = [(1, \"小明\"), (2, \"小華\"), (3, \"小美\")]\n",
    "class_b = [(3, \"小美\"), (4, \"小強\"), (5, \"小芳\")]\n",
    "class_a_df = spark.createDataFrame(class_a, [\"id\", \"姓名\"])\n",
    "class_b_df = spark.createDataFrame(class_b, [\"id\", \"姓名\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4575e74d",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 1️⃣7️⃣ UNION - 聯集（把兩堆玩具合在一起，重複的只留一個）\n",
    "\n",
    "🎈 概念解釋：\n",
    "UNION 合併兩個查詢結果\n",
    "如果有重複的人，只會記錄一次\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，合併 A 班和 B 班的學生名單，\n",
    "重複的學生只顯示一次。\n",
    "#### 📌 範例 17: UNION - 合併並去除重複"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97f9b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_a_df.createOrReplaceTempView(\"class_a\")\n",
    "class_b_df.createOrReplaceTempView(\"class_b\")\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT * FROM class_a\n",
    "    UNION\n",
    "    SELECT * FROM class_b\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fea25e3",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 1️⃣8️⃣ UNION ALL - 全部聯集（重複的也保留）\n",
    "\n",
    "🎈 概念解釋：\n",
    "UNION ALL 合併所有查詢結果\n",
    "就算有一樣的也不管，全部都要\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，合併兩個班級的點名記錄，\n",
    "包含所有重複的記錄。\n",
    "#### 📌 範例 18: UNION ALL - 全部合併（含重複）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ae902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "    SELECT * FROM class_a\n",
    "    UNION ALL\n",
    "    SELECT * FROM class_b\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa295c",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "# 📚 第六章：資料篩選與排序\n",
    "\n",
    "1️⃣9️⃣ DISTINCT - 去除重複（只留不一樣的）\n",
    "\n",
    "🎈 概念解釋：\n",
    "DISTINCT 去除重複的記錄\n",
    "重複的口味只拿一顆\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，找出玩具資料表中所有不同的顏色。\n",
    "#### 📌 範例 19: DISTINCT - 找出所有不同的顏色"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf72d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"SELECT DISTINCT `顏色` FROM toys\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468c3863",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 2️⃣0️⃣ WHERE - 條件篩選（只要符合條件的）\n",
    "\n",
    "🎈 概念解釋：\n",
    "WHERE 用來篩選符合條件的資料\n",
    "設定條件，只有符合的才能被選出來\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，找出價格大於 400 元的所有玩具。\n",
    "\n",
    "#### 📌 範例 20: WHERE - 篩選貴的玩具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"SELECT * FROM toys WHERE `價格` > 400\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc69f68",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 2️⃣1️⃣ ORDER BY - 排序（把東西排排站）\n",
    "\n",
    "🎈 概念解釋：\n",
    "ORDER BY 用來排序查詢結果\n",
    "讓資料有順序\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，將所有玩具按照價格從高到低排序。\n",
    "#### 📌 範例 21: ORDER BY - 價格排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875286f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"SELECT * FROM toys ORDER BY `價格` DESC\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2013ef4e",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 2️⃣2️⃣ GROUP BY - 分組（把同類的放一起）\n",
    "\n",
    "🎈 概念解釋：\n",
    "GROUP BY 將資料依指定欄位分組\n",
    "紅色的放一堆，藍色的放一堆\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，計算每種顏色有幾個玩具。\n",
    "#### 📌 範例 22: GROUP BY - 依顏色分組計數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f342ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "    SELECT `顏色`, COUNT(*) as `數量`\n",
    "    FROM toys\n",
    "    GROUP BY `顏色`\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192ff04d",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 2️⃣3️⃣ HAVING - 分組後篩選（分好組後再挑）\n",
    "\n",
    "🎈 概念解釋：\n",
    "HAVING 用來篩選分組後的結果\n",
    "先分組，再選出符合條件的組\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，找出玩具數量大於 1 個的顏色。\n",
    "#### 📌 範例 23: HAVING - 篩選玩具數量多的顏色"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7510f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "    SELECT `顏色`, COUNT(*) as `數量`\n",
    "    FROM toys\n",
    "    GROUP BY `顏色`\n",
    "    HAVING COUNT(*) > 1\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcaf233",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "# 📚 第七章：聚合函數（統計運算）\n",
    "\n",
    "### 2️⃣4️⃣ COUNT - 計數（數數看有幾個）\n",
    "\n",
    "🎈 概念解釋：\n",
    "COUNT 計算記錄的數量\n",
    "告訴你總共有幾個\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，計算玩具資料表中總共有幾個玩具。\n",
    "#### 📌 範例 24: COUNT - 數玩具總數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e246dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"SELECT COUNT(*) as `玩具總數` FROM toys\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14feddd",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 2️⃣5️⃣ SUM - 總和（對指定欄位進行加總）\n",
    "\n",
    "🎈 概念解釋：\n",
    "SUM 計算數值欄位的總和\n",
    "算算看總共要花多少錢\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，計算所有玩具的總價值。\n",
    "#### 📌 範例 25: SUM - 計算總價值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07e4b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"SELECT SUM(`價格`) as `總價值` FROM toys\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7e3339",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 2️⃣6️⃣ AVG - 平均（算平均值）\n",
    "\n",
    "🎈 概念解釋：\n",
    "AVG 計算數值欄位的平均值\n",
    "所有價格加起來除以數量\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，計算玩具的平均價格。\n",
    "#### 📌 範例 26: AVG - 計算平均價格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9260bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"SELECT AVG(`價格`) as `平均價格` FROM toys\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b6cd28",
   "metadata": {
    "lines_to_next_cell": 0,
    "region_name": "md"
   },
   "source": [
    "### 2️⃣7️⃣ MIN - 最小值（找最小的）\n",
    "\n",
    "🎈 概念解釋：\n",
    "MIN 找出最小值\n",
    "看看哪個價格最低\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，找出價格最低的玩具價格。\n",
    "#### 📌 範例 27: MIN - 找最低價格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d456025",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = spark.sql(\"SELECT MIN(`價格`) as `最低價格` FROM toys\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e007b6d",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 2️⃣8️⃣ MAX - 最大值（找最大的）\n",
    "\n",
    "🎈 概念解釋：\n",
    "MAX 找出最大值\n",
    "看看哪個價格最高\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，找出價格最高的玩具及其價格。\n",
    "#### 📌 範例 28: MAX - 找最高價格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb877fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"SELECT MAX(`價格`) as `最高價格` FROM toys\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa0377f",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "# 📚 第八章：條件與範圍查詢\n",
    "\n",
    "### 2️⃣9️⃣ BETWEEN - 範圍查詢（在某個範圍內）\n",
    "\n",
    "🎈 概念解釋：\n",
    "BETWEEN 用來查詢某個範圍內的資料\n",
    "找出在某個範圍內的東西\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，找出價格在 300 到 500 元之間的玩具。\n",
    "#### 📌 範例 29: BETWEEN - 找特定價格範圍的玩具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"SELECT * FROM toys WHERE `價格` BETWEEN 300 AND 500\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef7fef5",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 3️⃣0️⃣ LIKE - 模糊搜尋（名字像什麼的）\n",
    "\n",
    "🎈 概念解釋：\n",
    "LIKE 用來進行模糊搜尋\n",
    "不用完全一樣，有包含就可以\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，找出名稱中包含「熊」字的所有玩具。\n",
    "#### 📌 範例 30: LIKE - 模糊搜尋玩具名稱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996a999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"SELECT * FROM toys WHERE `名稱` LIKE '%熊%'\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b70ddc",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 3️⃣1️⃣ IN - 清單比對（在這些裡面的）\n",
    "\n",
    "🎈 概念解釋：\n",
    "IN 檢查值是否在指定清單中\n",
    "只要在清單裡的都可以\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，找出顏色是「紅色」、「藍色」或「粉色」的玩具。\n",
    "#### 📌 範例 31: IN - 找特定顏色的玩具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7436fec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"SELECT * FROM toys WHERE `顏色` IN ('紅色', '藍色', '粉色')\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb6e0b2",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 3️⃣2️⃣ NOT - 否定（不要這個）\n",
    "\n",
    "🎈 概念解釋：\n",
    "NOT 用來反轉條件\n",
    "把條件相反\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，找出顏色不是「棕色」的所有玩具。\n",
    "#### 📌 範例 32: NOT - 排除特定條件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e4d3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"SELECT * FROM toys WHERE NOT `顏色` = '棕色'\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef08710",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 3️⃣3️⃣ IS NULL - 檢查空值（有沒有遺失）\n",
    "\n",
    "🎈 概念解釋：\n",
    "IS NULL 檢查欄位是否為空值\n",
    "找出資料遺失的地方\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，找出沒有填寫顏色的玩具。\n",
    "#### 📌 範例 33: IS NULL - 找遺失資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bc062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先加一個沒有顏色的玩具\n",
    "toys_with_null = toys_df.union(\n",
    "    spark.createDataFrame([(7, \"神秘玩具\", None, 999, \"快樂玩具公司\")], \n",
    "                         toys_df.schema)\n",
    ")\n",
    "toys_with_null.createOrReplaceTempView(\"toys_with_null\")\n",
    "\n",
    "result = spark.sql(\"SELECT * FROM toys_with_null WHERE `顏色` IS NULL\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d9abca",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 3️⃣4️⃣ IS NOT NULL - 檢查非空值（有資料的）\n",
    "\n",
    "🎈 概念解釋：\n",
    "IS NOT NULL 檢查欄位是否有值\n",
    "找出有完整資訊的\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，找出所有有填寫顏色的玩具。\n",
    "#### 📌 範例 34: IS NOT NULL - 找完整資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5828bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"SELECT * FROM toys_with_null WHERE `顏色` IS NOT NULL\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6627ca",
   "metadata": {
    "lines_to_next_cell": 0,
    "region_name": "md"
   },
   "source": [
    "# 📚 第九章：進階條件邏輯\n",
    "\n",
    "### 3️⃣5️⃣ CASE - 條件判斷（如果...那麼...）\n",
    "\n",
    "🎈 概念解釋：\n",
    "CASE 根據不同條件回傳不同的值\n",
    "根據不同情況給不同答案\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，依據價格將玩具分為「昂貴」（>500）、\n",
    "「中等」（300-500）、「便宜」（<300）三個等級。\n",
    "#### 📌 範例 35: CASE - 價格等級分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55c18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM toys\").show()\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT `名稱`, `價格`,\n",
    "        CASE \n",
    "            WHEN `價格` > 500 THEN '昂貴'\n",
    "            WHEN `價格` >= 300 THEN '中等'\n",
    "            ELSE '便宜'\n",
    "        END AS `價格等級`\n",
    "    FROM toys\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9debbc4",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 3️⃣6️⃣ COALESCE - 取第一個非空值（找替代方案）\n",
    "\n",
    "🎈 概念解釋：\n",
    "COALESCE 回傳第一個非 NULL 的值\n",
    "找第一個不是空的值\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，如果玩具沒有顏色資訊，就顯示「未知」。\n",
    "#### 📌 範例 36: COALESCE - 處理空值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9607e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "    SELECT `名稱`, COALESCE(`顏色`, '未知') as `顏色`\n",
    "    FROM toys_with_null\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df8b201",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "# 📚 第十章：子查詢\n",
    "\n",
    "### 3️⃣7️⃣ EXISTS - 存在檢查（有沒有這個東西）\n",
    "\n",
    "🎈 概念解釋：\n",
    "EXISTS 檢查子查詢是否有結果\n",
    "檢查是否存在符合條件的資料\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，找出有昂貴玩具（>500元）的製造商。\n",
    "#### 📌 範例 37: EXISTS - 檢查是否存在"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b6d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT `製造商`\n",
    "    FROM toys t1\n",
    "    WHERE EXISTS (\n",
    "        SELECT 1 FROM toys t2 \n",
    "        WHERE t2.`製造商` = t1.`製造商` AND t2.`價格` > 500\n",
    "    )\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21828fc",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 3️⃣8️⃣ ANY/SOME - 任一比對（只要有一個符合）NOTE: Spark SQL 沒有支援 ANY/SOME\n",
    "\n",
    "🎈 概念解釋：\n",
    "ANY/SOME 與子查詢的任一值比較\n",
    "和一堆值比較，有一個符合就可以\n",
    "註:\n",
    "你可以用 MAX()（或 MIN()） 來達到相同邏輯。\n",
    "例如你原本意思是：「找出價格 大於任一藍色玩具價格 的玩具」，\n",
    "這其實等價於「價格大於藍色玩具中最低價格」。\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，找出價格比任一藍色玩具還貴的玩具。\n",
    "#### 📌 範例 38: ANY/SOME - 任一比對"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c0aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "    SELECT `名稱`, `價格`\n",
    "    FROM toys\n",
    "    WHERE `價格` > (\n",
    "        SELECT MIN(`價格`)\n",
    "        FROM toys\n",
    "        WHERE `顏色` = '藍色'\n",
    "    )\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb0239",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 3️⃣9️⃣ ALL - 全部比對（要全部都符合）NOTE: Spark SQL 沒有支援 ALL\n",
    "\n",
    "🎈 概念解釋：\n",
    "ALL 與子查詢的所有值比較\n",
    "和一堆值比較，全部都要符合\n",
    "註:\n",
    "用聚合函數 MAX() 取代 ALL 的語意：\n",
    "\n",
    "「大於藍色玩具所有價格」 ≡ 「大於藍色玩具的最高價」\n",
    "\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，找出價格比所有藍色玩具都貴的玩具。\n",
    "#### 📌 範例 39: ALL - 全部比對"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "    SELECT `名稱`, `價格`\n",
    "    FROM toys\n",
    "    WHERE `價格` > (\n",
    "        SELECT MAX(`價格`)\n",
    "        FROM toys\n",
    "        WHERE `顏色` = '藍色'\n",
    "    )\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd4f92c",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 4️⃣0️⃣ JOIN - 一般連接（把兩個表格接起來）\n",
    "\n",
    "🎈 概念解釋：\n",
    "JOIN 將兩個資料表連接起來\n",
    "找到相同的地方連接起來\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，將玩具表和製造商詳細資料表連接，\n",
    "顯示玩具名稱和製造商地址。\n",
    "#### 📌 範例 40: JOIN - 連接兩個表格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c2f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立製造商資料\n",
    "manufacturers = [(1, \"快樂玩具公司\", \"台北市\")]\n",
    "manufacturers_df = spark.createDataFrame(\n",
    "    manufacturers, [\"id\", \"公司名稱\", \"地址\"]\n",
    ")\n",
    "manufacturers_df.createOrReplaceTempView(\"manufacturers\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT t.`名稱`, m.`公司名稱`, m.`地址`\n",
    "    FROM toys t\n",
    "    JOIN manufacturers m ON t.`製造商` = m.`公司名稱`\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769a2218",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "# 📚 第十一章：資料表約束\n",
    "\n",
    "### 4️⃣1️⃣ PRIMARY KEY - 主鍵（每個玩具的身份證）\n",
    "\n",
    "🎈 概念解釋：\n",
    "PRIMARY KEY 唯一識別每一筆記錄的欄位\n",
    "確保每個資料都有獨一無二的識別碼\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，建立一個玩具資料表，\n",
    "以 id 作為主鍵（唯一識別碼）。\n",
    "\n",
    "⚠️ 注意：PySpark DataFrame 沒有內建 PRIMARY KEY 約束\n",
    "但可以透過程式邏輯來確保唯一性\n",
    "#### 📌 範例 41: PRIMARY KEY - 主鍵概念（透過唯一性驗證）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff1650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在 PySpark 中，我們透過程式邏輯來確保主鍵唯一性\n",
    "toys_with_id = [\n",
    "    (1, \"小熊\"),\n",
    "    (2, \"機器人\"),\n",
    "    (3, \"芭比娃娃\"),\n",
    "    # (1, \"重複ID\")  # 這會違反主鍵唯一性\n",
    "]\n",
    "toys_pk_df = spark.createDataFrame(toys_with_id, [\"id\", \"name\"])\n",
    "\n",
    "# 驗證 id 的唯一性（模擬主鍵檢查）\n",
    "total_count = toys_pk_df.count()\n",
    "unique_count = toys_pk_df.select(\"id\").distinct().count()\n",
    "print(f\"總記錄數: {total_count}\")\n",
    "print(f\"唯一 ID 數: {unique_count}\")\n",
    "\n",
    "if total_count == unique_count:\n",
    "    print(\"✅ ID 唯一性驗證通過（符合主鍵要求）\")\n",
    "    toys_pk_df.show()\n",
    "else:\n",
    "    print(\"❌ 發現重複的 ID（違反主鍵約束）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d6ffcc",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 4️⃣2️⃣ FOREIGN KEY - 外鍵（連結到另一個表格）\n",
    "\n",
    "🎈 概念解釋：\n",
    "FOREIGN KEY 確保參照的資料存在於另一個資料表中\n",
    "確保資料之間有正確的關聯\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，建立訂單資料表，\n",
    "其中客戶 ID 是外鍵，必須參照客戶資料表。\n",
    "#### 📌 範例 42: FOREIGN KEY - 外鍵關聯（透過 JOIN 驗證）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688b445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在 PySpark 中，我們透過 JOIN 來確保外鍵關聯的正確性\n",
    "customers_data = [\n",
    "    (1, \"王小明\"),\n",
    "    (2, \"李小華\"),\n",
    "    (3, \"張小美\")\n",
    "]\n",
    "customers_df = spark.createDataFrame(customers_data, [\"id\", \"name\"])\n",
    "\n",
    "orders_data = [\n",
    "    (101, 1, \"訂單A\"),  # customer_id = 1 存在\n",
    "    (102, 2, \"訂單B\"),  # customer_id = 2 存在\n",
    "    (103, 1, \"訂單C\"),  # customer_id = 1 存在\n",
    "    # (104, 99, \"訂單D\")  # customer_id = 99 不存在，違反外鍵約束\n",
    "]\n",
    "orders_df = spark.createDataFrame(orders_data, [\"order_id\", \"customer_id\", \"description\"])\n",
    "\n",
    "# 驗證外鍵完整性（使用 LEFT JOIN 檢查）\n",
    "validation_df = orders_df.join(\n",
    "    customers_df, \n",
    "    orders_df.customer_id == customers_df.id, \n",
    "    \"left\"\n",
    ").filter(customers_df.id.isNull())\n",
    "\n",
    "print(\"檢查外鍵約束...\")\n",
    "invalid_count = validation_df.count()\n",
    "\n",
    "if invalid_count == 0:\n",
    "    print(\"✅ 外鍵完整性驗證通過\")\n",
    "    print(\"\\n訂單與客戶關聯：\")\n",
    "    orders_df.join(customers_df, orders_df.customer_id == customers_df.id) \\\n",
    "        .select(orders_df.order_id, customers_df.name, orders_df.description) \\\n",
    "        .show()\n",
    "else:\n",
    "    print(f\"❌ 發現 {invalid_count} 筆無效的外鍵參照\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b623ef9a",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 4️⃣3️⃣ CONSTRAINT - 約束條件（設定規則）\n",
    "\n",
    "🎈 概念解釋：\n",
    "CONSTRAINT 定義資料表的限制條件\n",
    "給資料設定必須遵守的規則\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，建立玩具資料表，\n",
    "並設定價格必須大於 0 的約束條件。\n",
    "#### 📌 範例 43: CONSTRAINT - 設定約束條件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c984734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 DataFrame API 驗證\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 驗證價格都大於 0\n",
    "valid_toys = toys_df.filter(col(\"價格\") > 0)\n",
    "print(f\"✅ 驗證通過，所有玩具價格都大於 0\")\n",
    "valid_toys.select(\"名稱\", \"價格\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2213fe10",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 4️⃣4️⃣ INDEX / PARTITION - 索引優化（加快搜尋速度）\n",
    "\n",
    "🎈 概念解釋：\n",
    "INDEX / PARTITION 加快資料查詢的速度\n",
    "可以快速找到想要的資料\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，對常查詢的欄位建立索引/分區以提升效能。\n",
    "#### 📌 範例 44: INDEX / PARTITION - 效能優化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a4870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark 使用分區和快取來優化\n",
    "toys_df.repartition(\"顏色\").cache()\n",
    "print(\"✅ 已按顏色分區並快取資料\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce61d1",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "# 📚 第十二章：交易控制\n",
    "\n",
    "### 4️⃣5️⃣ TRANSACTION - 交易（一起做完或一起取消）\n",
    "\n",
    "🎈 概念解釋：\n",
    "TRANSACTION 確保一系列操作要嘛全部成功，要嘛全部失敗\n",
    "確保多個操作要一起成功或一起失敗\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式說明交易的概念，\n",
    "將多個操作包在一個交易中執行。\n",
    "#### 📌 範例 45: TRANSACTION - 交易控制\n",
    "\n",
    "交易概念說明：\n",
    "想像你要買 3 個玩具，但錢只夠買 3 個\n",
    "如果其中一個賣完了，那就全部都不買\n",
    "這樣才不會只買到一部分玩具\n",
    "\n",
    "PySpark 中交易通常在寫入資料庫時使用：\n",
    "- 開始交易 (BEGIN)\n",
    "- 執行多個操作\n",
    "- 全部成功就提交 (COMMIT)\n",
    "- 有錯誤就回滾 (ROLLBACK)\n",
    "\n",
    "### 4️⃣6️⃣ COMMIT - 提交（確定要存檔）\n",
    "\n",
    "🎈 概念解釋：\n",
    "COMMIT 永久保存所有變更\n",
    "確定要把改變永久保存\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，示範如何在完成所有操作後提交交易。\n",
    "#### 📌 範例 46: COMMIT - 提交變更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d9f75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模擬交易提交\n",
    "try:\n",
    "    # 執行操作\n",
    "    new_data = [(8, \"新玩具\", \"黃色\", 350, \"快樂玩具公司\")]\n",
    "    new_df = spark.createDataFrame(new_data, toys_df.schema)\n",
    "    \n",
    "    # 寫入資料（模擬 COMMIT）\n",
    "    updated_toys = toys_df.union(new_df)\n",
    "    print(\"✅ 交易已提交，新玩具已加入\")\n",
    "    updated_toys.select(\"名稱\", \"價格\").tail(3)\n",
    "except Exception as e:\n",
    "    print(f\"❌ 發生錯誤，交易回滾: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cfa7f9",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 4️⃣7️⃣ ROLLBACK - 回滾（取消不要存檔）\n",
    "\n",
    "🎈 概念解釋：\n",
    "ROLLBACK 取消尚未提交的變更\n",
    "回復到交易開始前的狀態\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，示範當發生錯誤時如何回滾交易。\n",
    "#### 📌 範例 47: ROLLBACK - 回滾交易\n",
    "\n",
    "回滾範例：\n",
    "假設你不小心刪掉了重要資料\n",
    "ROLLBACK 可以讓你回到刪除之前的狀態\n",
    "還原所有未提交的操作\n",
    "\n",
    "在 PySpark 中：\n",
    "- 可以使用 checkpoint 保存檢查點\n",
    "- 發生錯誤時可以讀取檢查點資料\n",
    "- 恢復到之前的狀態\n",
    "\n",
    "### 4️⃣8️⃣ SAVEPOINT - 儲存點（設定中途存檔點）\n",
    "\n",
    "🎈 概念解釋：\n",
    "SAVEPOINT 在交易中設定中途檢查點\n",
    "可以選擇回滾到特定的儲存點\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，示範如何在交易中設定多個儲存點。\n",
    "\n",
    "⚠️ 注意：PySpark 不支援傳統資料庫的 SAVEPOINT 概念\n",
    "在 PySpark 中，我們使用版本控制或快照來達到類似效果\n",
    "#### 📌 範例 48: SAVEPOINT - 儲存點概念（使用快照替代）\n",
    "\n",
    "在 PySpark 中，我們使用變數保存不同階段的 DataFrame\n",
    "這類似於設定多個「儲存點」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac82c8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 儲存點 1：原始資料\n",
    "savepoint_1 = toys_df\n",
    "print(\"✅ 儲存點 1：原始玩具資料\")\n",
    "print(f\"   記錄數：{savepoint_1.count()}\")\n",
    "savepoint_1.show()\n",
    "\n",
    "# 執行一些操作\n",
    "filtered_df = toys_df.filter(col(\"價格\") > 500)\n",
    "\n",
    "# 儲存點 2：過濾後的資料\n",
    "savepoint_2 = filtered_df\n",
    "print(\"✅ 儲存點 2：過濾後的資料\")\n",
    "print(f\"   記錄數：{savepoint_2.count()}\")\n",
    "savepoint_2.show()\n",
    "\n",
    "# 如果需要回到某個儲存點，直接使用該變數\n",
    "print(\"💡 回到儲存點 1：\")\n",
    "savepoint_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9391ae2f",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "在傳統 SQL 中：\n",
    "  SAVEPOINT sp1;\n",
    "  -- 執行操作\n",
    "  SAVEPOINT sp2;\n",
    "  -- 可以 ROLLBACK TO sp1;\n",
    "\n",
    "在 PySpark 中：\n",
    "  使用變數保存不同階段的 DataFrame\n",
    "  或使用 Delta Lake 的時間旅行功能\n",
    "\n",
    "# 📚 第十三章：權限管理\n",
    "\n",
    "### 4️⃣9️⃣ GRANT - 授予權限（給別人鑰匙）\n",
    "\n",
    "🎈 概念解釋：\n",
    "GRANT 授予使用者特定的操作權限\n",
    "控制誰可以存取資料\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，授予使用者查詢玩具資料表的權限。\n",
    "#### 📌 範例 49: GRANT - 授予權限\n",
    "\n",
    "權限授予範例：\n",
    "\n",
    "-- 讓 user1 可以查詢玩具表\n",
    "GRANT SELECT ON toys TO user1;\n",
    "\n",
    "-- 讓 user2 可以新增玩具\n",
    "GRANT INSERT ON toys TO user2;\n",
    "\n",
    "-- 讓 admin 擁有所有權限\n",
    "GRANT ALL ON toys TO admin;\n",
    "### 5️⃣0️⃣ REVOKE - 撤銷權限（收回鑰匙）\n",
    "\n",
    "🎈 概念解釋：\n",
    "REVOKE 撤銷使用者的權限\n",
    "移除已授予的存取權限\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，撤銷使用者對玩具資料表的刪除權限。\n",
    "#### 📌 範例 50: REVOKE - 撤銷權限\n",
    "\n",
    "權限撤銷範例：\n",
    "\n",
    "-- 不讓 user1 刪除玩具\n",
    "REVOKE DELETE ON toys FROM user1;\n",
    "\n",
    "-- 收回 user2 的所有權限\n",
    "REVOKE ALL ON toys FROM user2;\n",
    "\n",
    "# 🎓 課程總結\n",
    "## 🎊 恭喜！你已經完成了 SQL 的 50 個重要觀念！\")\n",
    "📝 學習總結：\n",
    "\n",
    "1️⃣ 基礎操作：SELECT, INSERT, UPDATE, DELETE\n",
    "   - 這些是最常用的基本操作\n",
    "\n",
    "2️⃣ 資料表管理：CREATE, DROP, ALTER, TRUNCATE\n",
    "   - 管理資料庫結構\n",
    "\n",
    "3️⃣ 連接操作：INNER JOIN, LEFT JOIN, RIGHT JOIN, FULL OUTER JOIN\n",
    "   - 連接多個資料表\n",
    "\n",
    "4️⃣ 資料篩選：WHERE, HAVING, DISTINCT, ORDER BY, GROUP BY\n",
    "   - 篩選和排序資料\n",
    "\n",
    "5️⃣ 聚合函數：COUNT, SUM, AVG, MIN, MAX\n",
    "   - 執行統計運算\n",
    "\n",
    "6️⃣ 進階查詢：CASE, EXISTS, ANY, ALL, BETWEEN, LIKE, IN\n",
    "   - 進階的查詢技巧\n",
    "\n",
    "7️⃣ 交易控制：TRANSACTION, COMMIT, ROLLBACK, SAVEPOINT\n",
    "   - 維護資料一致性\n",
    "\n",
    "8️⃣ 權限管理：GRANT, REVOKE\n",
    "   - 管理使用者權限\n",
    "\n",
    "💡 學習建議：\n",
    "- 每個概念都要實際練習\n",
    "- 嘗試用真實的資料練習\n",
    "- 遇到問題可以尋求協助\n",
    "- 循序漸進地學習\n",
    "\n",
    "🚀 下一步：\n",
    "- 練習組合多個 SQL 語句\n",
    "- 應用於實際案例\n",
    "- 深入學習進階功能\n",
    "\n",
    "記住：熟能生巧，多練習就能掌握 SQL！\n",
    "# 📚 教學文件結束 - 祝你學習愉快！🎈\n",
    "\n",
    "關閉 Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb6435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "region_name,-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
