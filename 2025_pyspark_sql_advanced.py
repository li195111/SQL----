# -*- coding: utf-8 -*-
# %% [md]
## PySpark SQL é€²éšæ•™å­¸ ğŸš€
# 
# é€™ä»½æ•™å­¸å°ˆç‚ºå·²ç¶“æŒæ¡ SQL åŸºç¤çš„ä½ è¨­è¨ˆï¼
# æˆ‘å€‘å°‡æ·±å…¥æ¢è¨é€²éšçš„ SQL æŠ€å·§ï¼ŒåŒ…æ‹¬è¦–çª—å‡½æ•¸ã€CTEã€æ•ˆèƒ½å„ªåŒ–ç­‰ä¸»é¡Œã€‚
# è®“ä½ çš„ SQL æŠ€èƒ½æ›´ä¸Šä¸€å±¤æ¨“ï¼âœ¨
# 
# ä½œè€…ï¼šQChoice AI æ•™å­¸åœ˜éšŠ
# æ—¥æœŸï¼š2025-01-15

# 
# ğŸš€ æ­¡è¿ä¾†åˆ° SQL é€²éšèª²ç¨‹ï¼
#
# --------------------------------------------------
#
# ğŸ¯ ç’°å¢ƒè¨­å®š - æº–å‚™é€²éšé–‹ç™¼ç’°å¢ƒ

# %%
from pyspark.sql import SparkSession, Window
from pyspark.sql.functions import *
from pyspark.sql.types import *
import time

# å‰µå»º Spark Session - é…ç½®é€²éšæ•ˆèƒ½åƒæ•¸
spark = SparkSession.builder \
    .appName("SQLé€²éšæ•™å­¸") \
    .config("spark.sql.shuffle.partitions", "4") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .getOrCreate()

# %% [md]
### ğŸ“š ç¬¬ä¸€ç« ï¼šè¦–çª—å‡½æ•¸ (Window Functions)

#### 1ï¸âƒ£ ROW_NUMBER - ç‚ºæ¯ä¸€åˆ—åˆ†é…å”¯ä¸€çš„åºè™Ÿ

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# ROW_NUMBER() ç‚ºåˆ†å€å…§çš„æ¯ä¸€åˆ—åˆ†é…ä¸€å€‹å”¯ä¸€çš„åºè™Ÿã€‚
# å¸¸ç”¨æ–¼æ’åã€å»é‡ã€åˆ†é ç­‰å ´æ™¯ã€‚
# 
# ğŸ¯ AI Prompt ç¯„ä¾‹ï¼š
# è«‹å¹«æˆ‘å¯«ä¸€å€‹ PySpark ç¨‹å¼ï¼Œç‚ºæ¯å€‹éƒ¨é–€çš„å“¡å·¥æŒ‰è–ªè³‡æ’åºä¸¦åˆ†é…åºè™Ÿã€‚

##### ğŸ“Œ ç¯„ä¾‹ 1: ROW_NUMBER - æ’åèˆ‡ç·¨è™Ÿ

# %%

# å»ºç«‹å“¡å·¥è³‡æ–™
employees_data = [
    (1, "Alice", "å·¥ç¨‹éƒ¨", 75000),
    (2, "Bob", "å·¥ç¨‹éƒ¨", 82000),
    (3, "Charlie", "å·¥ç¨‹éƒ¨", 68000),
    (4, "David", "è¡ŒéŠ·éƒ¨", 65000),
    (5, "Eve", "è¡ŒéŠ·éƒ¨", 72000),
    (6, "Frank", "è¡ŒéŠ·éƒ¨", 70000),
    (7, "Grace", "äººè³‡éƒ¨", 60000),
    (8, "Henry", "äººè³‡éƒ¨", 63000)
]

employees_df = spark.createDataFrame(
    employees_data, 
    ["å“¡å·¥ç·¨è™Ÿ", "å§“å", "éƒ¨é–€", "è–ªè³‡"]
)

# å®šç¾©è¦–çª—è¦æ ¼ï¼šæŒ‰éƒ¨é–€åˆ†çµ„ï¼ŒæŒ‰è–ªè³‡é™åºæ’åˆ—
windowSpec = Window.partitionBy("éƒ¨é–€").orderBy(desc("è–ªè³‡"))

# ä½¿ç”¨ ROW_NUMBER
result = employees_df.withColumn("éƒ¨é–€æ’å", row_number().over(windowSpec))

result.orderBy("éƒ¨é–€", "éƒ¨é–€æ’å").show()

# SQL å¯«æ³•
employees_df.createOrReplaceTempView("employees")

# %%
sql_result = spark.sql("""
    SELECT 
        `å“¡å·¥ç·¨è™Ÿ`,
        `å§“å`,
        `éƒ¨é–€`,
        `è–ªè³‡`,
        ROW_NUMBER() OVER (PARTITION BY `éƒ¨é–€` ORDER BY `è–ªè³‡` DESC) as `éƒ¨é–€æ’å`
    FROM employees
    ORDER BY `éƒ¨é–€`, `éƒ¨é–€æ’å`
""")
sql_result.show()

# %% [md]
#### 2ï¸âƒ£ RANK & DENSE_RANK - è™•ç†ä¸¦åˆ—æ’å

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# RANK() å’Œ DENSE_RANK() éƒ½ç”¨æ–¼æ’åï¼Œä½†è™•ç†ç›¸åŒå€¼çš„æ–¹å¼ä¸åŒï¼š
# - RANK(): ç›¸åŒå€¼å¾—åˆ°ç›¸åŒæ’åï¼Œä¸‹ä¸€å€‹æ’åæœƒè·³è™Ÿ
# - DENSE_RANK(): ç›¸åŒå€¼å¾—åˆ°ç›¸åŒæ’åï¼Œä¸‹ä¸€å€‹æ’åé€£çºŒ
# 
# ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š
# æˆç¸¾æ’åã€éŠ·å”®æ¥­ç¸¾æ’åç­‰éœ€è¦è™•ç†ä¸¦åˆ—æƒ…æ³çš„å ´æ™¯

##### ğŸ“Œ ç¯„ä¾‹ 2: RANK vs DENSE_RANK - ä¸¦åˆ—æ’åè™•ç†

# %%

# å»ºç«‹è€ƒè©¦æˆç¸¾è³‡æ–™
scores_data = [
    (1, "Alice", 95),
    (2, "Bob", 95),
    (3, "Charlie", 90),
    (4, "David", 90),
    (5, "Eve", 85),
    (6, "Frank", 80),
]

scores_df = spark.createDataFrame(scores_data, ["å­¸è™Ÿ", "å§“å", "åˆ†æ•¸"])

# å®šç¾©è¦–çª—è¦æ ¼
windowSpec = Window.orderBy(desc("åˆ†æ•¸"))

result = scores_df.withColumn("RANK", rank().over(windowSpec)) \
                  .withColumn("DENSE_RANK", dense_rank().over(windowSpec)) \
                  .withColumn("ROW_NUMBER", row_number().over(windowSpec))

result.show()

# %% [md]
# ğŸ’¡ èªªæ˜ï¼š
# - Alice å’Œ Bob éƒ½æ˜¯ 95 åˆ†ï¼ŒRANK å’Œ DENSE_RANK éƒ½æ˜¯ 1
# - Charlie å’Œ David éƒ½æ˜¯ 90 åˆ†ï¼š
#   * RANK æ˜¯ 3ï¼ˆå› ç‚ºå‰é¢æœ‰ 2 å€‹äººï¼‰
#   * DENSE_RANK æ˜¯ 2ï¼ˆé€£çºŒç·¨è™Ÿï¼‰
# - ROW_NUMBER ç‚ºæ¯å€‹äººåˆ†é…å”¯ä¸€ç·¨è™Ÿ

#### 3ï¸âƒ£ LEAD & LAG - è¨ªå•å‰å¾Œåˆ—çš„è³‡æ–™
# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# LEAD() å’Œ LAG() å…è¨±ä½ è¨ªå•ç•¶å‰åˆ—ä¹‹å‰æˆ–ä¹‹å¾Œçš„è³‡æ–™ï¼š
# - LAG(): è¨ªå•å‰é¢çš„åˆ—
# - LEAD(): è¨ªå•å¾Œé¢çš„åˆ—
# 
# ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š
# è¨ˆç®—å¢é•·ç‡ã€åŒæ¯”ç’°æ¯”åˆ†æã€æ™‚é–“åºåˆ—åˆ†æ

##### ğŸ“Œ ç¯„ä¾‹ 3: LEAD & LAG - æ™‚é–“åºåˆ—åˆ†æ

# %%

# å»ºç«‹æœˆåº¦éŠ·å”®è³‡æ–™
sales_data = [
    ("2024-01", 100000),
    ("2024-02", 120000),
    ("2024-03", 115000),
    ("2024-04", 130000),
    ("2024-05", 135000),
    ("2024-06", 140000),
]

sales_df = spark.createDataFrame(sales_data, ["æœˆä»½", "éŠ·å”®é¡"])

# å®šç¾©è¦–çª—è¦æ ¼
windowSpec = Window.orderBy("æœˆä»½")

result = sales_df.withColumn("ä¸ŠæœˆéŠ·å”®é¡", lag("éŠ·å”®é¡", 1).over(windowSpec)) \
                 .withColumn("ä¸‹æœˆéŠ·å”®é¡", lead("éŠ·å”®é¡", 1).over(windowSpec)) \
                 .withColumn("æœˆå¢é•·é¡", col("éŠ·å”®é¡") - lag("éŠ·å”®é¡", 1).over(windowSpec)) \
                 .withColumn("æœˆå¢é•·ç‡%", 
                            round((col("éŠ·å”®é¡") - lag("éŠ·å”®é¡", 1).over(windowSpec)) / 
                                  lag("éŠ·å”®é¡", 1).over(windowSpec) * 100, 2))

result.show()

# %% \[md]
#### 4ï¸âƒ£ ç´¯è¨ˆè¨ˆç®— - SUM/AVG OVER
# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# è¦–çª—å‡½æ•¸é…åˆèšåˆå‡½æ•¸å¯ä»¥é€²è¡Œç´¯è¨ˆè¨ˆç®—ï¼Œè€Œä¸æœƒæŠ˜ç–Šè³‡æ–™åˆ—ã€‚
# 
# ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š
# ç´¯è¨ˆéŠ·å”®é¡ã€ç§»å‹•å¹³å‡ã€æ»¾å‹•çµ±è¨ˆ



##### ğŸ“Œ ç¯„ä¾‹ 4: ç´¯è¨ˆè¨ˆç®—èˆ‡ç§»å‹•å¹³å‡

# %%

# ç¹¼çºŒä½¿ç”¨éŠ·å”®è³‡æ–™
windowSpec = Window.orderBy("æœˆä»½").rowsBetween(Window.unboundedPreceding, Window.currentRow)
movingAvgSpec = Window.orderBy("æœˆä»½").rowsBetween(-2, 0)  # 3å€‹æœˆç§»å‹•å¹³å‡

result = sales_df.withColumn("ç´¯è¨ˆéŠ·å”®é¡", sum("éŠ·å”®é¡").over(windowSpec)) \
                 .withColumn("3æœˆç§»å‹•å¹³å‡", round(avg("éŠ·å”®é¡").over(movingAvgSpec), 2))

result.show()


# %% [md]
### ğŸ“š ç¬¬äºŒç« ï¼šé€šç”¨è¡¨è¡¨é”å¼ (Common Table Expressions - CTE)

# %% [md]
#### 5ï¸âƒ£ WITH å­å¥ - å»ºç«‹è‡¨æ™‚çµæœé›†
# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# CTE (WITH å­å¥) è®“ä½ å»ºç«‹è‡¨æ™‚çš„å‘½åçµæœé›†ï¼Œæé«˜æŸ¥è©¢çš„å¯è®€æ€§å’Œå¯ç¶­è­·æ€§ã€‚
# å¯ä»¥æŠŠè¤‡é›œæŸ¥è©¢åˆ†è§£ç‚ºå¤šå€‹æ­¥é©Ÿã€‚
# 
# ğŸ¯ å„ªå‹¢ï¼š
# 1. æé«˜å¯è®€æ€§
# 2. å¯é‡è¤‡ä½¿ç”¨
# 3. ä¾¿æ–¼é™¤éŒ¯
# 4. æ”¯æ´éè¿´æŸ¥è©¢



##### ğŸ“Œ ç¯„ä¾‹ 5: CTE - ç°¡åŒ–è¤‡é›œæŸ¥è©¢

# %%

# å»ºç«‹è¨‚å–®è³‡æ–™
orders_data = [
    (1, "Alice", "2024-01-15", 1500),
    (2, "Bob", "2024-01-20", 2300),
    (3, "Alice", "2024-02-10", 1800),
    (4, "Charlie", "2024-02-15", 3200),
    (5, "Bob", "2024-03-05", 2100),
    (6, "Alice", "2024-03-20", 2500),
]

orders_df = spark.createDataFrame(
    orders_data,
    ["è¨‚å–®ç·¨è™Ÿ", "å®¢æˆ¶", "è¨‚å–®æ—¥æœŸ", "é‡‘é¡"]
)
orders_df.createOrReplaceTempView("orders")

# ä½¿ç”¨ CTE è¨ˆç®—å®¢æˆ¶ç¸½æ¶ˆè²»å’Œå¹³å‡æ¶ˆè²»
sql_with_cte = spark.sql("""
    WITH customer_stats AS (
        SELECT 
            `å®¢æˆ¶`,
            COUNT(*) as `è¨‚å–®æ•¸`,
            SUM(`é‡‘é¡`) as `ç¸½æ¶ˆè²»`,
            AVG(`é‡‘é¡`) as `å¹³å‡æ¶ˆè²»`
        FROM orders
        GROUP BY `å®¢æˆ¶`
    ),
    high_value_customers AS (
        SELECT 
            `å®¢æˆ¶`,
            `è¨‚å–®æ•¸`,
            `ç¸½æ¶ˆè²»`,
            ROUND(`å¹³å‡æ¶ˆè²»`, 2) as `å¹³å‡æ¶ˆè²»`
        FROM customer_stats
        WHERE `ç¸½æ¶ˆè²»` > 5000
    )
    SELECT * FROM high_value_customers
    ORDER BY `ç¸½æ¶ˆè²»` DESC
""")

sql_with_cte.show()

# %% \[md]
#### 6ï¸âƒ£ å¤šå±¤ CTE - è¤‡é›œæ¥­å‹™é‚è¼¯
# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# å¤šå±¤ CTE å¯ä»¥å°‡è¤‡é›œçš„æ¥­å‹™é‚è¼¯åˆ†è§£ç‚ºå¤šå€‹æ¸…æ™°çš„æ­¥é©Ÿã€‚
# 
# ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š
# å¤šæ­¥é©Ÿçš„è³‡æ–™è½‰æ›ã€è¤‡é›œçš„æ¥­å‹™æŒ‡æ¨™è¨ˆç®—



##### ğŸ“Œ ç¯„ä¾‹ 6: å¤šå±¤ CTE - å®¢æˆ¶åˆ†ç´šåˆ†æ

# %%

sql_multi_cte = spark.sql("""
    WITH monthly_sales AS (
        -- ç¬¬ä¸€å±¤ï¼šè¨ˆç®—æ¯å€‹å®¢æˆ¶çš„æœˆåº¦éŠ·å”®
        SELECT 
            `å®¢æˆ¶`,
            DATE_FORMAT(TO_DATE(`è¨‚å–®æ—¥æœŸ`), 'yyyy-MM') as `æœˆä»½`,
            SUM(`é‡‘é¡`) as `æœˆéŠ·å”®é¡`
        FROM orders
        GROUP BY `å®¢æˆ¶`, DATE_FORMAT(TO_DATE(`è¨‚å–®æ—¥æœŸ`), 'yyyy-MM')
    ),
    customer_summary AS (
        -- ç¬¬äºŒå±¤ï¼šåŒ¯ç¸½å®¢æˆ¶çµ±è¨ˆ
        SELECT 
            `å®¢æˆ¶`,
            COUNT(DISTINCT `æœˆä»½`) as `æ´»èºæœˆæ•¸`,
            SUM(`æœˆéŠ·å”®é¡`) as `ç¸½éŠ·å”®é¡`,
            AVG(`æœˆéŠ·å”®é¡`) as `å¹³å‡æœˆéŠ·å”®é¡`
        FROM monthly_sales
        GROUP BY `å®¢æˆ¶`
    ),
    customer_level AS (
        -- ç¬¬ä¸‰å±¤ï¼šå®¢æˆ¶åˆ†ç´š
        SELECT 
            `å®¢æˆ¶`,
            `æ´»èºæœˆæ•¸`,
            ROUND(`ç¸½éŠ·å”®é¡`, 2) as `ç¸½éŠ·å”®é¡`,
            ROUND(`å¹³å‡æœˆéŠ·å”®é¡`, 2) as `å¹³å‡æœˆéŠ·å”®é¡`,
            CASE 
                WHEN `ç¸½éŠ·å”®é¡` >= 6000 THEN 'ç™½é‡‘å®¢æˆ¶'
                WHEN `ç¸½éŠ·å”®é¡` >= 4000 THEN 'é‡‘ç‰Œå®¢æˆ¶'
                ELSE 'ä¸€èˆ¬å®¢æˆ¶'
            END as `å®¢æˆ¶ç­‰ç´š`
        FROM customer_summary
    )
    SELECT * FROM customer_level
    ORDER BY `ç¸½éŠ·å”®é¡` DESC
""")

sql_multi_cte.show()


# %% [md]
### ğŸ“š ç¬¬ä¸‰ç« ï¼šå­æŸ¥è©¢å„ªåŒ–
# 7ï¸âƒ£ ç›¸é—œå­æŸ¥è©¢ vs éç›¸é—œå­æŸ¥è©¢

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# - éç›¸é—œå­æŸ¥è©¢ï¼šå…§éƒ¨æŸ¥è©¢ç¨ç«‹åŸ·è¡Œï¼ŒåªåŸ·è¡Œä¸€æ¬¡
# - ç›¸é—œå­æŸ¥è©¢ï¼šå…§éƒ¨æŸ¥è©¢ä¾è³´å¤–éƒ¨æŸ¥è©¢ï¼Œå¯èƒ½åŸ·è¡Œå¤šæ¬¡ï¼ˆæ•ˆèƒ½è¼ƒå·®ï¼‰
# 
# ğŸ¯ å„ªåŒ–å»ºè­°ï¼š
# ç›¡é‡ä½¿ç”¨ JOIN æˆ–è¦–çª—å‡½æ•¸æ›¿ä»£ç›¸é—œå­æŸ¥è©¢



##### ğŸ“Œ ç¯„ä¾‹ 7: å­æŸ¥è©¢å„ªåŒ– - æ‰¾å‡ºé«˜æ–¼éƒ¨é–€å¹³å‡è–ªè³‡çš„å“¡å·¥

# %%

# æ–¹æ³• 1ï¼šä½¿ç”¨ç›¸é—œå­æŸ¥è©¢ï¼ˆè¼ƒæ…¢ï¼‰
sql_correlated = spark.sql("""
    SELECT 
        e1.`å§“å`,
        e1.`éƒ¨é–€`,
        e1.`è–ªè³‡`,
        (SELECT AVG(e2.`è–ªè³‡`) 
         FROM employees e2 
         WHERE e2.`éƒ¨é–€` = e1.`éƒ¨é–€`) as `éƒ¨é–€å¹³å‡è–ªè³‡`
    FROM employees e1
    WHERE e1.`è–ªè³‡` > (
        SELECT AVG(e2.`è–ªè³‡`) 
        FROM employees e2 
        WHERE e2.`éƒ¨é–€` = e1.`éƒ¨é–€`
    )
    ORDER BY e1.`éƒ¨é–€`, e1.`è–ªè³‡` DESC
""")

sql_correlated.show()

# æ–¹æ³• 2ï¼šä½¿ç”¨ JOINï¼ˆè¼ƒå¿«ï¼‰
sql_join = spark.sql("""
    WITH dept_avg AS (
        SELECT `éƒ¨é–€`, AVG(`è–ªè³‡`) as `å¹³å‡è–ªè³‡`
        FROM employees
        GROUP BY `éƒ¨é–€`
    )
    SELECT 
        e.`å§“å`,
        e.`éƒ¨é–€`,
        e.`è–ªè³‡`,
        ROUND(d.`å¹³å‡è–ªè³‡`, 2) as `éƒ¨é–€å¹³å‡è–ªè³‡`
    FROM employees e
    JOIN dept_avg d ON e.`éƒ¨é–€` = d.`éƒ¨é–€`
    WHERE e.`è–ªè³‡` > d.`å¹³å‡è–ªè³‡`
    ORDER BY e.`éƒ¨é–€`, e.`è–ªè³‡` DESC
""")

sql_join.show()

# æ–¹æ³• 3ï¼šä½¿ç”¨è¦–çª—å‡½æ•¸ï¼ˆæœ€å¿«ï¼‰
windowSpec = Window.partitionBy("éƒ¨é–€")
result_window = employees_df.withColumn("éƒ¨é–€å¹³å‡è–ªè³‡", round(avg("è–ªè³‡").over(windowSpec), 2)) \
                            .filter(col("è–ªè³‡") > col("éƒ¨é–€å¹³å‡è–ªè³‡")) \
                            .orderBy("éƒ¨é–€", desc("è–ªè³‡"))

result_window.show()


# %% [md]
### ğŸ“š ç¬¬å››ç« ï¼šé€²éš JOIN æŠ€å·§

# %% [md]
#### 8ï¸âƒ£ CROSS JOIN - ç¬›å¡çˆ¾ç©
# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# CROSS JOIN ç”¢ç”Ÿå…©å€‹è¡¨çš„ç¬›å¡çˆ¾ç©ï¼Œå³æ‰€æœ‰å¯èƒ½çš„çµ„åˆã€‚
# 
# ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š
# ç”Ÿæˆæ¸¬è©¦è³‡æ–™ã€å»ºç«‹æ™‚é–“åºåˆ—ã€ç”¢ç”Ÿæ‰€æœ‰å¯èƒ½çš„çµ„åˆ



##### ğŸ“Œ ç¯„ä¾‹ 8: CROSS JOIN - ç”¢ç”Ÿæ‰€æœ‰å¯èƒ½çš„é…å°

# %%

# å»ºç«‹ç”¢å“å’Œé¡è‰²è³‡æ–™
products = spark.createDataFrame([("æ‰‹æ©Ÿ",), ("å¹³æ¿",), ("ç­†é›»",)], ["ç”¢å“"])
colors = spark.createDataFrame([("é»‘è‰²",), ("ç™½è‰²",), ("éŠ€è‰²",)], ["é¡è‰²"])

# CROSS JOIN
cross_result = products.crossJoin(colors)

cross_result.show()

# %% \[md]
#### 9ï¸âƒ£ SELF JOIN - è‡ªæˆ‘é€£æ¥
# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# SELF JOIN æ˜¯è¡¨èˆ‡è‡ªå·±é€²è¡Œé€£æ¥ï¼Œå¸¸ç”¨æ–¼æŸ¥æ‰¾å±¤ç´šé—œä¿‚æˆ–æ¯”è¼ƒåŒè¡¨å…§çš„è¨˜éŒ„ã€‚
# 
# ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š
# å“¡å·¥-ä¸»ç®¡é—œä¿‚ã€çµ„ç¹”æ¶æ§‹ã€å°‹æ‰¾é‡è¤‡è¨˜éŒ„



##### ğŸ“Œ ç¯„ä¾‹ 9: SELF JOIN - å“¡å·¥èˆ‡ä¸»ç®¡é—œä¿‚

# %%

# å»ºç«‹åŒ…å«ä¸»ç®¡è³‡è¨Šçš„å“¡å·¥è³‡æ–™
emp_manager_data = [
    (1, "Alice", None),      # CEOï¼Œæ²’æœ‰ä¸»ç®¡
    (2, "Bob", 1),           # Bob çš„ä¸»ç®¡æ˜¯ Alice
    (3, "Charlie", 1),       # Charlie çš„ä¸»ç®¡æ˜¯ Alice
    (4, "David", 2),         # David çš„ä¸»ç®¡æ˜¯ Bob
    (5, "Eve", 2),           # Eve çš„ä¸»ç®¡æ˜¯ Bob
    (6, "Frank", 3),         # Frank çš„ä¸»ç®¡æ˜¯ Charlie
]

emp_manager_df = spark.createDataFrame(
    emp_manager_data,
    ["å“¡å·¥ç·¨è™Ÿ", "å§“å", "ä¸»ç®¡ç·¨è™Ÿ"]
)
emp_manager_df.createOrReplaceTempView("emp_manager")

# ä½¿ç”¨ SELF JOIN æŸ¥è©¢å“¡å·¥å’Œå…¶ä¸»ç®¡
sql_self_join = spark.sql("""
    SELECT 
        e.`å“¡å·¥ç·¨è™Ÿ`,
        e.`å§“å` as `å“¡å·¥å§“å`,
        COALESCE(m.`å§“å`, 'ç„¡ä¸»ç®¡') as `ä¸»ç®¡å§“å`
    FROM emp_manager e
    LEFT JOIN emp_manager m ON e.`ä¸»ç®¡ç·¨è™Ÿ` = m.`å“¡å·¥ç·¨è™Ÿ`
    ORDER BY e.`å“¡å·¥ç·¨è™Ÿ`
""")

sql_self_join.show()


# ğŸ”Ÿ ANTI JOIN & SEMI JOIN - é«˜æ•ˆéæ¿¾

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# - SEMI JOIN (LEFT SEMI): è¿”å›å·¦è¡¨ä¸­åœ¨å³è¡¨ä¸­æœ‰åŒ¹é…çš„è¨˜éŒ„
# - ANTI JOIN (LEFT ANTI): è¿”å›å·¦è¡¨ä¸­åœ¨å³è¡¨ä¸­æ²’æœ‰åŒ¹é…çš„è¨˜éŒ„
# 
# ğŸ¯ å„ªå‹¢ï¼š
# æ¯” IN / NOT IN å­æŸ¥è©¢æ•ˆèƒ½æ›´å¥½



##### ğŸ“Œ ç¯„ä¾‹ 10: ANTI JOIN & SEMI JOIN - å®¢æˆ¶è¨‚å–®åˆ†æ

# %%

# å»ºç«‹å®¢æˆ¶è³‡æ–™
customers_data = [
    (1, "Alice"),
    (2, "Bob"),
    (3, "Charlie"),
    (4, "David"),
    (5, "Eve"),
]

customers_df = spark.createDataFrame(customers_data, ["å®¢æˆ¶ç·¨è™Ÿ", "å®¢æˆ¶å§“å"])

# å»ºç«‹è¨‚å–®è³‡æ–™ï¼ˆåªæœ‰éƒ¨åˆ†å®¢æˆ¶æœ‰è¨‚å–®ï¼‰
order_customers = orders_df.select("å®¢æˆ¶").distinct()

# SEMI JOIN - æœ‰è¨‚å–®çš„å®¢æˆ¶
customers_with_orders = customers_df.join(
    order_customers,
    customers_df["å®¢æˆ¶å§“å"] == order_customers["å®¢æˆ¶"],
    "leftsemi"
)

customers_with_orders.show()

# ANTI JOIN - æ²’æœ‰è¨‚å–®çš„å®¢æˆ¶
customers_without_orders = customers_df.join(
    order_customers,
    customers_df["å®¢æˆ¶å§“å"] == order_customers["å®¢æˆ¶"],
    "leftanti"
)

customers_without_orders.show()


# %% [md]
### ğŸ“š ç¬¬äº”ç« ï¼šæ•ˆèƒ½å„ªåŒ–æŠ€å·§
# 1ï¸âƒ£1ï¸âƒ£ Broadcast Join - å»£æ’­å°è¡¨

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# ç•¶ä¸€å€‹å¤§è¡¨å’Œä¸€å€‹å°è¡¨é€²è¡Œ JOIN æ™‚ï¼Œå¯ä»¥å°‡å°è¡¨å»£æ’­åˆ°æ‰€æœ‰ç¯€é»ï¼Œ
# é¿å… shuffle æ“ä½œï¼Œå¤§å¹…æå‡æ•ˆèƒ½ã€‚
# 
# ğŸ¯ é©ç”¨å ´æ™¯ï¼š
# å°è¡¨ < 10MBï¼Œå¤§è¡¨èˆ‡ç¶­åº¦è¡¨ JOIN



##### ğŸ“Œ ç¯„ä¾‹ 11: Broadcast Join - å„ªåŒ–å¤§å°è¡¨é€£æ¥

# %%

# å»ºç«‹éƒ¨é–€è³‡è¨Šï¼ˆå°è¡¨ï¼‰
departments_data = [
    ("å·¥ç¨‹éƒ¨", "æŠ€è¡“å¤§æ¨“"),
    ("è¡ŒéŠ·éƒ¨", "è¡Œæ”¿å¤§æ¨“"),
    ("äººè³‡éƒ¨", "è¡Œæ”¿å¤§æ¨“"),
]

departments_df = spark.createDataFrame(departments_data, ["éƒ¨é–€", "è¾¦å…¬åœ°é»"])

# ä¸€èˆ¬ JOIN
normal_join = employees_df.join(departments_df, "éƒ¨é–€")

normal_join.select("å§“å", "éƒ¨é–€", "è¾¦å…¬åœ°é»").show(5)

# Broadcast JOIN
broadcast_join = employees_df.join(
    broadcast(departments_df),
    "éƒ¨é–€"
)

broadcast_join.select("å§“å", "éƒ¨é–€", "è¾¦å…¬åœ°é»").show(5)

# æŸ¥çœ‹åŸ·è¡Œè¨ˆç•«
print("ä¸€èˆ¬ JOIN æœƒæœ‰ SortMergeJoin")
print("Broadcast JOIN æœƒæœ‰ BroadcastHashJoin")


# 1ï¸âƒ£2ï¸âƒ£ åˆ†å€èˆ‡åˆ†æ¡¶ - è³‡æ–™çµ„ç¹”å„ªåŒ–

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# - Partitioning: æŒ‰æ¬„ä½å€¼å°‡è³‡æ–™åˆ†å‰²æˆå¤šå€‹ç›®éŒ„
# - Bucketing: æŒ‰ hash å€¼å°‡è³‡æ–™åˆ†å‰²æˆå›ºå®šæ•¸é‡çš„æª”æ¡ˆ
# 
# ğŸ¯ å„ªå‹¢ï¼š
# æ¸›å°‘æƒæçš„è³‡æ–™é‡ï¼Œæå‡æŸ¥è©¢æ•ˆèƒ½



##### ğŸ“Œ ç¯„ä¾‹ 12: åˆ†å€èˆ‡éæ¿¾å„ªåŒ–

# %%

# é‡æ–°åˆ†å€è³‡æ–™
employees_partitioned = employees_df.repartition(2, "éƒ¨é–€")


# ä½¿ç”¨åˆ†å€æ¬„ä½éæ¿¾ï¼ˆæ•ˆèƒ½æ›´å¥½ï¼‰
filtered = employees_partitioned.filter(col("éƒ¨é–€") == "å·¥ç¨‹éƒ¨")
filtered.show()


# 1ï¸âƒ£3ï¸âƒ£ å¿«å–èˆ‡æŒä¹…åŒ– - é¿å…é‡è¤‡è¨ˆç®—

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# å°‡å¸¸ç”¨çš„ DataFrame å¿«å–åˆ°è¨˜æ†¶é«”ä¸­ï¼Œé¿å…é‡è¤‡è¨ˆç®—ã€‚
# 
# ğŸ¯ é©ç”¨å ´æ™¯ï¼š
# å¤šæ¬¡ä½¿ç”¨ç›¸åŒçš„ä¸­é–“çµæœã€è¿­ä»£è¨ˆç®—



##### ğŸ“Œ ç¯„ä¾‹ 13: å¿«å–å„ªåŒ–

# %%

# å»ºç«‹ä¸€å€‹è¤‡é›œçš„è¨ˆç®—
complex_df = employees_df.groupBy("éƒ¨é–€") \
                        .agg(
                            count("*").alias("äººæ•¸"),
                            avg("è–ªè³‡").alias("å¹³å‡è–ªè³‡"),
                            max("è–ªè³‡").alias("æœ€é«˜è–ªè³‡")
                        )

# å¿«å–çµæœ
complex_df.cache()

# ç¬¬ä¸€æ¬¡åŸ·è¡Œï¼ˆæœƒè§¸ç™¼è¨ˆç®—ä¸¦å¿«å–ï¼‰
start = time.time()
complex_df.show()
print(f"åŸ·è¡Œæ™‚é–“: {time.time() - start:.4f} ç§’")

# ç¬¬äºŒæ¬¡åŸ·è¡Œï¼ˆå¾å¿«å–è®€å–ï¼‰
start = time.time()
complex_df.show()
print(f"åŸ·è¡Œæ™‚é–“: {time.time() - start:.4f} ç§’ï¼ˆæ‡‰è©²æ›´å¿«ï¼‰")

# é‡‹æ”¾å¿«å–
complex_df.unpersist()


# %% [md]
### ğŸ“š ç¬¬å…­ç« ï¼šè¤‡é›œè³‡æ–™è½‰æ›
# 1ï¸âƒ£4ï¸âƒ£ PIVOT - è¡Œè½‰åˆ—

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# PIVOT å°‡è¡Œè³‡æ–™è½‰æ›ç‚ºåˆ—ï¼Œå¸¸ç”¨æ–¼å»ºç«‹äº¤å‰è¡¨ã€‚
# 
# ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š
# éŠ·å”®å ±è¡¨ã€è³‡æ–™é€è¦–è¡¨ã€è¶¨å‹¢åˆ†æ



##### ğŸ“Œ ç¯„ä¾‹ 14: PIVOT - å»ºç«‹éƒ¨é–€è–ªè³‡é€è¦–è¡¨

# %%

# ä½¿ç”¨ PIVOT å»ºç«‹éƒ¨é–€-çµ±è¨ˆæŒ‡æ¨™é€è¦–è¡¨
employees_df.createOrReplaceTempView("employees")

pivot_result = spark.sql("""
    SELECT * FROM (
        SELECT `éƒ¨é–€`, `è–ªè³‡`
        FROM employees
    )
    PIVOT (
        COUNT(*) as `äººæ•¸`,
        AVG(`è–ªè³‡`) as `å¹³å‡è–ªè³‡`
        FOR `éƒ¨é–€` IN ('å·¥ç¨‹éƒ¨', 'è¡ŒéŠ·éƒ¨', 'äººè³‡éƒ¨')
    )
""")

pivot_result.show()


# 1ï¸âƒ£5ï¸âƒ£ UNPIVOT - åˆ—è½‰è¡Œ

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# UNPIVOT å°‡åˆ—è³‡æ–™è½‰æ›ç‚ºè¡Œï¼Œæ˜¯ PIVOT çš„åå‘æ“ä½œã€‚
# 
# ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š
# å°‡å¯¬è¡¨è½‰æ›ç‚ºé•·è¡¨ã€è³‡æ–™æ­£è¦åŒ–



##### ğŸ“Œ ç¯„ä¾‹ 15: UNPIVOT - åˆ—è½‰è¡Œ

# %%

# å»ºç«‹å¯¬è¡¨æ ¼å¼çš„å­£åº¦éŠ·å”®è³‡æ–™
quarterly_sales = spark.createDataFrame([
    ("ç”¢å“A", 100, 120, 115, 130),
    ("ç”¢å“B", 80, 90, 95, 100),
], ["ç”¢å“", "Q1", "Q2", "Q3", "Q4"])

quarterly_sales.createOrReplaceTempView("quarterly_sales")

# ä½¿ç”¨ UNPIVOTï¼ˆSpark 3.4+ï¼‰æˆ–ä½¿ç”¨ stack å‡½æ•¸
unpivot_result = quarterly_sales.selectExpr(
    "`ç”¢å“`",
    "stack(4, 'Q1', `Q1`, 'Q2', `Q2`, 'Q3', `Q3`, 'Q4', `Q4`) as (`å­£åº¦`, `éŠ·å”®é¡`)"
)

unpivot_result.show()


# 1ï¸âƒ£6ï¸âƒ£ é™£åˆ—èˆ‡çµæ§‹è™•ç†

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# PySpark æ”¯æ´è¤‡é›œçš„è³‡æ–™å‹æ…‹å¦‚é™£åˆ—ã€çµæ§‹é«”ã€åœ°åœ–ç­‰ã€‚
# 
# ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š
# è™•ç† JSON è³‡æ–™ã€å·¢ç‹€çµæ§‹ã€å¤šå€¼æ¬„ä½



##### ğŸ“Œ ç¯„ä¾‹ 16: é™£åˆ—æ“ä½œ

# %%

# å»ºç«‹åŒ…å«é™£åˆ—çš„è³‡æ–™
students_data = [
    (1, "Alice", ["æ•¸å­¸", "ç‰©ç†", "åŒ–å­¸"]),
    (2, "Bob", ["è‹±æ–‡", "æ­·å²"]),
    (3, "Charlie", ["æ•¸å­¸", "è‹±æ–‡", "é«”è‚²"]),
]

students_df = spark.createDataFrame(
    students_data,
    ["å­¸è™Ÿ", "å§“å", "é¸ä¿®èª²ç¨‹"]
)

students_df.show(truncate=False)

# å±•é–‹é™£åˆ—
exploded = students_df.select(
    "å­¸è™Ÿ",
    "å§“å",
    explode("é¸ä¿®èª²ç¨‹").alias("èª²ç¨‹")
)

exploded.show()

# é™£åˆ—ç›¸é—œå‡½æ•¸
array_operations = students_df.select(
    "å§“å",
    "é¸ä¿®èª²ç¨‹",
    size("é¸ä¿®èª²ç¨‹").alias("èª²ç¨‹æ•¸"),
    array_contains("é¸ä¿®èª²ç¨‹", "æ•¸å­¸").alias("æ˜¯å¦é¸ä¿®æ•¸å­¸")
)

array_operations.show(truncate=False)


# %% [md]
### ğŸ“š ç¬¬ä¸ƒç« ï¼šè³‡æ–™å“è³ªèˆ‡æ¸…ç†
# 1ï¸âƒ£7ï¸âƒ£ è™•ç† NULL å€¼

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# NULL å€¼è™•ç†æ˜¯è³‡æ–™æ¸…ç†çš„é‡è¦ç’°ç¯€ã€‚
# 
# ğŸ¯ å¸¸ç”¨æ–¹æ³•ï¼š
# fillna, dropna, coalesce, nvl



##### ğŸ“Œ ç¯„ä¾‹ 17: NULL å€¼è™•ç†

# %%

# å»ºç«‹åŒ…å« NULL çš„è³‡æ–™
data_with_null = [
    (1, "Alice", 75000, "å·¥ç¨‹éƒ¨"),
    (2, "Bob", None, "è¡ŒéŠ·éƒ¨"),
    (3, "Charlie", 68000, None),
    (4, None, 65000, "äººè³‡éƒ¨"),
]

df_null = spark.createDataFrame(
    data_with_null,
    ["å“¡å·¥ç·¨è™Ÿ", "å§“å", "è–ªè³‡", "éƒ¨é–€"]
)

df_null.show()

# æ–¹æ³• 1: å¡«å……é è¨­å€¼
filled = df_null.fillna({
    "å§“å": "æœªçŸ¥",
    "è–ªè³‡": 60000,
    "éƒ¨é–€": "å¾…åˆ†é…"
})

filled.show()

# æ–¹æ³• 2: åˆªé™¤åŒ…å« NULL çš„åˆ—
dropped = df_null.dropna()

dropped.show()

# æ–¹æ³• 3: ä½¿ç”¨ COALESCE
coalesced = df_null.select(
    "å“¡å·¥ç·¨è™Ÿ",
    coalesce(col("å§“å"), lit("æœªçŸ¥")).alias("å§“å"),
    coalesce(col("è–ªè³‡"), lit(60000)).alias("è–ªè³‡"),
    coalesce(col("éƒ¨é–€"), lit("å¾…åˆ†é…")).alias("éƒ¨é–€")
)

coalesced.show()


# 1ï¸âƒ£8ï¸âƒ£ è³‡æ–™å»é‡

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# æ‰¾å‡ºä¸¦ç§»é™¤é‡è¤‡çš„è¨˜éŒ„ã€‚
# 
# ğŸ¯ æ–¹æ³•ï¼š
# distinct, dropDuplicates, è¦–çª—å‡½æ•¸



##### ğŸ“Œ ç¯„ä¾‹ 18: è³‡æ–™å»é‡

# %%

# å»ºç«‹åŒ…å«é‡è¤‡è³‡æ–™
duplicate_data = [
    (1, "Alice", "å·¥ç¨‹éƒ¨"),
    (2, "Bob", "è¡ŒéŠ·éƒ¨"),
    (1, "Alice", "å·¥ç¨‹éƒ¨"),  # å®Œå…¨é‡è¤‡
    (3, "Alice", "äººè³‡éƒ¨"),  # å§“åé‡è¤‡
]

df_dup = spark.createDataFrame(
    duplicate_data,
    ["å“¡å·¥ç·¨è™Ÿ", "å§“å", "éƒ¨é–€"]
)

df_dup.show()

# æ–¹æ³• 1: å®Œå…¨å»é‡
dedup_all = df_dup.distinct()
dedup_all.show()

# æ–¹æ³• 2: æŒ‰ç‰¹å®šæ¬„ä½å»é‡ï¼ˆä¿ç•™ç¬¬ä¸€ç­†ï¼‰
dedup_by_name = df_dup.dropDuplicates(["å§“å"])
dedup_by_name.show()

# æ–¹æ³• 3: ä½¿ç”¨è¦–çª—å‡½æ•¸ä¿ç•™ç‰¹å®šè¦å‰‡çš„è¨˜éŒ„
windowSpec = Window.partitionBy("å§“å").orderBy("å“¡å·¥ç·¨è™Ÿ")
dedup_window = df_dup.withColumn("rn", row_number().over(windowSpec)) \
                     .filter(col("rn") == 1) \
                     .drop("rn")

dedup_window.show()


# %% [md]
### ğŸ“š ç¬¬å…«ç« ï¼šé€²éšåˆ†æå‡½æ•¸
# 1ï¸âƒ£9ï¸âƒ£ NTILE - è³‡æ–™åˆ†çµ„

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# NTILE å°‡è³‡æ–™åˆ†æˆ N å€‹å¤§è‡´ç›¸ç­‰çš„çµ„ã€‚
# 
# ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š
# å®¢æˆ¶åˆ†å±¤ã€ABC åˆ†æã€åˆ†ä½æ•¸åˆ†æ



##### ğŸ“Œ ç¯„ä¾‹ 19: NTILE - è–ªè³‡å››åˆ†ä½æ•¸

# %%

windowSpec = Window.orderBy("è–ªè³‡")

ntile_result = employees_df.withColumn(
    "è–ªè³‡åˆ†çµ„",
    ntile(4).over(windowSpec)
).withColumn(
    "åˆ†çµ„èªªæ˜",
    when(col("è–ªè³‡åˆ†çµ„") == 1, "ä½è–ªçµ„")
    .when(col("è–ªè³‡åˆ†çµ„") == 2, "ä¸­ä½è–ªçµ„")
    .when(col("è–ªè³‡åˆ†çµ„") == 3, "ä¸­é«˜è–ªçµ„")
    .otherwise("é«˜è–ªçµ„")
)

ntile_result.orderBy("è–ªè³‡").show()


# 2ï¸âƒ£0ï¸âƒ£ PERCENT_RANK - ç™¾åˆ†ä½æ’å

# ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š
# PERCENT_RANK è¨ˆç®—å€¼åœ¨è³‡æ–™é›†ä¸­çš„ç›¸å°ä½ç½®ï¼ˆ0 åˆ° 1 ä¹‹é–“ï¼‰ã€‚
# 
# ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š
# ç¸¾æ•ˆè©•ä¼°ã€æˆç¸¾åˆ†æã€ç›¸å°æ’å



##### ğŸ“Œ ç¯„ä¾‹ 20: PERCENT_RANK - è–ªè³‡ç™¾åˆ†ä½

# %%

windowSpec = Window.orderBy("è–ªè³‡")

percent_result = employees_df.withColumn(
    "è–ªè³‡ç™¾åˆ†ä½",
    round(percent_rank().over(windowSpec) * 100, 2)
).withColumn(
    "èªªæ˜",
    concat(
        lit("è–ªè³‡è¶…é "),
        round(percent_rank().over(windowSpec) * 100, 0).cast("int"),
        lit("% çš„å“¡å·¥")
    )
)

percent_result.orderBy("è–ªè³‡").select("å§“å", "è–ªè³‡", "è–ªè³‡ç™¾åˆ†ä½", "èªªæ˜").show()


# ğŸ“ èª²ç¨‹ç¸½çµ

# %% [md]
## ğŸ“ èª²ç¨‹ç¸½çµ
### ğŸŠ æ­å–œï¼ä½ å·²ç¶“å®Œæˆäº† SQL é€²éšèª²ç¨‹ï¼
# ğŸ“ é€²éšæŠ€èƒ½ç¸½çµï¼š
# 
# 1ï¸âƒ£ è¦–çª—å‡½æ•¸ï¼š
#    - ROW_NUMBER, RANK, DENSE_RANKï¼šæ’åèˆ‡ç·¨è™Ÿ
#    - LEAD, LAGï¼šè¨ªå•å‰å¾Œåˆ—è³‡æ–™
#    - SUM/AVG OVERï¼šç´¯è¨ˆè¨ˆç®—èˆ‡ç§»å‹•å¹³å‡
#    - NTILE, PERCENT_RANKï¼šåˆ†çµ„èˆ‡ç™¾åˆ†ä½
# 
# 2ï¸âƒ£ é€šç”¨è¡¨è¡¨é”å¼ (CTE)ï¼š
#    - WITH å­å¥ï¼šæé«˜æŸ¥è©¢å¯è®€æ€§
#    - å¤šå±¤ CTEï¼šåˆ†è§£è¤‡é›œé‚è¼¯
#    - éè¿´ CTEï¼šè™•ç†å±¤ç´šé—œä¿‚
# 
# 3ï¸âƒ£ å­æŸ¥è©¢å„ªåŒ–ï¼š
#    - é¿å…ç›¸é—œå­æŸ¥è©¢
#    - ä½¿ç”¨ JOIN æ›¿ä»£
#    - ä½¿ç”¨è¦–çª—å‡½æ•¸å„ªåŒ–
# 
# 4ï¸âƒ£ é€²éš JOINï¼š
#    - CROSS JOINï¼šç¬›å¡çˆ¾ç©
#    - SELF JOINï¼šè‡ªæˆ‘é€£æ¥
#    - SEMI/ANTI JOINï¼šé«˜æ•ˆéæ¿¾
# 
# 5ï¸âƒ£ æ•ˆèƒ½å„ªåŒ–ï¼š
#    - Broadcast Joinï¼šå»£æ’­å°è¡¨
#    - åˆ†å€èˆ‡åˆ†æ¡¶ï¼šè³‡æ–™çµ„ç¹”
#    - å¿«å–èˆ‡æŒä¹…åŒ–ï¼šé¿å…é‡è¤‡è¨ˆç®—
# 
# 6ï¸âƒ£ è¤‡é›œè³‡æ–™è½‰æ›ï¼š
#    - PIVOT/UNPIVOTï¼šè¡Œåˆ—è½‰æ›
#    - é™£åˆ—èˆ‡çµæ§‹ï¼šè™•ç†è¤‡é›œå‹æ…‹
#    - è³‡æ–™å±•é–‹èˆ‡èšåˆ
# 
# 7ï¸âƒ£ è³‡æ–™å“è³ªï¼š
#    - NULL å€¼è™•ç†
#    - è³‡æ–™å»é‡
#    - è³‡æ–™é©—è­‰
# 
# ğŸ’¡ é€²éšå­¸ç¿’å»ºè­°ï¼š
# - ç†è§£åŸ·è¡Œè¨ˆç•«ï¼Œå­¸æœƒæ•ˆèƒ½åˆ†æ
# - æŒæ¡è³‡æ–™åˆ†å€ç­–ç•¥
# - ç†Ÿæ‚‰ Spark UI çš„ä½¿ç”¨
# - å¯¦è¸ Delta Lake é€²éšåŠŸèƒ½
# - å­¸ç¿’ Spark Streaming
# 
# ğŸš€ ä¸‹ä¸€æ­¥ï¼š
# - æ·±å…¥ç ”ç©¶ Spark å…§éƒ¨æ©Ÿåˆ¶
# - å­¸ç¿’åˆ†æ•£å¼ç³»çµ±åŸç†
# - å¯¦è¸å¤§è¦æ¨¡è³‡æ–™è™•ç†å°ˆæ¡ˆ
# - æ¢ç´¢æ©Ÿå™¨å­¸ç¿’èˆ‡ Spark MLlib
# 
# è¨˜ä½ï¼šé€²éšæŠ€èƒ½éœ€è¦å¤§é‡å¯¦è¸ï¼Œå¤šåšå°ˆæ¡ˆã€å¤šè§£æ±ºå¯¦éš›å•é¡Œæ‰èƒ½çœŸæ­£æŒæ¡ï¼
## ğŸ“š é€²éšæ•™å­¸æ–‡ä»¶çµæŸ - ç¹¼çºŒç²¾é€²ä½ çš„æŠ€èƒ½ï¼ğŸš€

# é—œé–‰ Spark Session

# %%
# spark.stop()  # å–æ¶ˆè¨»è§£ä»¥é—œé–‰ Spark
