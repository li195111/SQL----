{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56af655b",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "# PySpark SQL é€²éšæ•™å­¸ ğŸš€\n",
    "\n",
    "\n",
    "é€™ä»½æ•™å­¸å°ˆç‚ºå·²ç¶“æŒæ¡ SQL åŸºç¤çš„ä½ è¨­è¨ˆï¼\n",
    "æˆ‘å€‘å°‡æ·±å…¥æ¢è¨é€²éšçš„ SQL æŠ€å·§ï¼ŒåŒ…æ‹¬è¦–çª—å‡½æ•¸ã€CTEã€æ•ˆèƒ½å„ªåŒ–ç­‰ä¸»é¡Œã€‚\n",
    "è®“ä½ çš„ SQL æŠ€èƒ½æ›´ä¸Šä¸€å±¤æ¨“ï¼âœ¨\n",
    "\n",
    "ä½œè€…ï¼šQChoice AI æ•™å­¸åœ˜éšŠ\n",
    "æ—¥æœŸï¼š2025-01-15\n",
    "\n",
    "\n",
    "ğŸš€ æ­¡è¿ä¾†åˆ° SQL é€²éšèª²ç¨‹ï¼\n",
    "\n",
    "--------------------------------------------------\n",
    "\n",
    "ğŸ¯ ç’°å¢ƒè¨­å®š - æº–å‚™é€²éšé–‹ç™¼ç’°å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f964a89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "# å‰µå»º Spark Session - é…ç½®é€²éšæ•ˆèƒ½åƒæ•¸\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SQLé€²éšæ•™å­¸\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c95ed",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## ğŸ“š ç¬¬ä¸€ç« ï¼šè¦–çª—å‡½æ•¸ (Window Functions)\n",
    "\n",
    "### 1ï¸âƒ£ ROW_NUMBER - ç‚ºæ¯ä¸€åˆ—åˆ†é…å”¯ä¸€çš„åºè™Ÿ\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "ROW_NUMBER() ç‚ºåˆ†å€å…§çš„æ¯ä¸€åˆ—åˆ†é…ä¸€å€‹å”¯ä¸€çš„åºè™Ÿã€‚\n",
    "å¸¸ç”¨æ–¼æ’åã€å»é‡ã€åˆ†é ç­‰å ´æ™¯ã€‚\n",
    "\n",
    "ğŸ¯ AI Prompt ç¯„ä¾‹ï¼š\n",
    "è«‹å¹«æˆ‘å¯«ä¸€å€‹ PySpark ç¨‹å¼ï¼Œç‚ºæ¯å€‹éƒ¨é–€çš„å“¡å·¥æŒ‰è–ªè³‡æ’åºä¸¦åˆ†é…åºè™Ÿã€‚\n",
    "\n",
    "#### ğŸ“Œ ç¯„ä¾‹ 1: ROW_NUMBER - æ’åèˆ‡ç·¨è™Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9010145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# å»ºç«‹å“¡å·¥è³‡æ–™\n",
    "employees_data = [\n",
    "    (1, \"Alice\", \"å·¥ç¨‹éƒ¨\", 75000),\n",
    "    (2, \"Bob\", \"å·¥ç¨‹éƒ¨\", 82000),\n",
    "    (3, \"Charlie\", \"å·¥ç¨‹éƒ¨\", 68000),\n",
    "    (4, \"David\", \"è¡ŒéŠ·éƒ¨\", 65000),\n",
    "    (5, \"Eve\", \"è¡ŒéŠ·éƒ¨\", 72000),\n",
    "    (6, \"Frank\", \"è¡ŒéŠ·éƒ¨\", 70000),\n",
    "    (7, \"Grace\", \"äººè³‡éƒ¨\", 60000),\n",
    "    (8, \"Henry\", \"äººè³‡éƒ¨\", 63000)\n",
    "]\n",
    "\n",
    "employees_df = spark.createDataFrame(\n",
    "    employees_data, \n",
    "    [\"å“¡å·¥ç·¨è™Ÿ\", \"å§“å\", \"éƒ¨é–€\", \"è–ªè³‡\"]\n",
    ")\n",
    "\n",
    "# å®šç¾©è¦–çª—è¦æ ¼ï¼šæŒ‰éƒ¨é–€åˆ†çµ„ï¼ŒæŒ‰è–ªè³‡é™åºæ’åˆ—\n",
    "windowSpec = Window.partitionBy(\"éƒ¨é–€\").orderBy(desc(\"è–ªè³‡\"))\n",
    "\n",
    "# ä½¿ç”¨ ROW_NUMBER\n",
    "result = employees_df.withColumn(\"éƒ¨é–€æ’å\", row_number().over(windowSpec))\n",
    "\n",
    "result.orderBy(\"éƒ¨é–€\", \"éƒ¨é–€æ’å\").show()\n",
    "\n",
    "# SQL å¯«æ³•\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        `å“¡å·¥ç·¨è™Ÿ`,\n",
    "        `å§“å`,\n",
    "        `éƒ¨é–€`,\n",
    "        `è–ªè³‡`,\n",
    "        ROW_NUMBER() OVER (PARTITION BY `éƒ¨é–€` ORDER BY `è–ªè³‡` DESC) as `éƒ¨é–€æ’å`\n",
    "    FROM employees\n",
    "    ORDER BY `éƒ¨é–€`, `éƒ¨é–€æ’å`\n",
    "\"\"\")\n",
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f590d228",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 2ï¸âƒ£ RANK & DENSE_RANK - è™•ç†ä¸¦åˆ—æ’å\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "RANK() å’Œ DENSE_RANK() éƒ½ç”¨æ–¼æ’åï¼Œä½†è™•ç†ç›¸åŒå€¼çš„æ–¹å¼ä¸åŒï¼š\n",
    "- RANK(): ç›¸åŒå€¼å¾—åˆ°ç›¸åŒæ’åï¼Œä¸‹ä¸€å€‹æ’åæœƒè·³è™Ÿ\n",
    "- DENSE_RANK(): ç›¸åŒå€¼å¾—åˆ°ç›¸åŒæ’åï¼Œä¸‹ä¸€å€‹æ’åé€£çºŒ\n",
    "\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š\n",
    "æˆç¸¾æ’åã€éŠ·å”®æ¥­ç¸¾æ’åç­‰éœ€è¦è™•ç†ä¸¦åˆ—æƒ…æ³çš„å ´æ™¯\n",
    "\n",
    "#### ğŸ“Œ ç¯„ä¾‹ 2: RANK vs DENSE_RANK - ä¸¦åˆ—æ’åè™•ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476a39ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# å»ºç«‹è€ƒè©¦æˆç¸¾è³‡æ–™\n",
    "scores_data = [\n",
    "    (1, \"Alice\", 95),\n",
    "    (2, \"Bob\", 95),\n",
    "    (3, \"Charlie\", 90),\n",
    "    (4, \"David\", 90),\n",
    "    (5, \"Eve\", 85),\n",
    "    (6, \"Frank\", 80),\n",
    "]\n",
    "\n",
    "scores_df = spark.createDataFrame(scores_data, [\"å­¸è™Ÿ\", \"å§“å\", \"åˆ†æ•¸\"])\n",
    "\n",
    "# å®šç¾©è¦–çª—è¦æ ¼\n",
    "windowSpec = Window.orderBy(desc(\"åˆ†æ•¸\"))\n",
    "\n",
    "result = scores_df.withColumn(\"RANK\", rank().over(windowSpec)) \\\n",
    "                  .withColumn(\"DENSE_RANK\", dense_rank().over(windowSpec)) \\\n",
    "                  .withColumn(\"ROW_NUMBER\", row_number().over(windowSpec))\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691ab04c",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "ğŸ’¡ èªªæ˜ï¼š\n",
    "- Alice å’Œ Bob éƒ½æ˜¯ 95 åˆ†ï¼ŒRANK å’Œ DENSE_RANK éƒ½æ˜¯ 1\n",
    "- Charlie å’Œ David éƒ½æ˜¯ 90 åˆ†ï¼š\n",
    "  * RANK æ˜¯ 3ï¼ˆå› ç‚ºå‰é¢æœ‰ 2 å€‹äººï¼‰\n",
    "  * DENSE_RANK æ˜¯ 2ï¼ˆé€£çºŒç·¨è™Ÿï¼‰\n",
    "- ROW_NUMBER ç‚ºæ¯å€‹äººåˆ†é…å”¯ä¸€ç·¨è™Ÿ\n",
    "\n",
    "### 3ï¸âƒ£ LEAD & LAG - è¨ªå•å‰å¾Œåˆ—çš„è³‡æ–™\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "LEAD() å’Œ LAG() å…è¨±ä½ è¨ªå•ç•¶å‰åˆ—ä¹‹å‰æˆ–ä¹‹å¾Œçš„è³‡æ–™ï¼š\n",
    "- LAG(): è¨ªå•å‰é¢çš„åˆ—\n",
    "- LEAD(): è¨ªå•å¾Œé¢çš„åˆ—\n",
    "\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š\n",
    "è¨ˆç®—å¢é•·ç‡ã€åŒæ¯”ç’°æ¯”åˆ†æã€æ™‚é–“åºåˆ—åˆ†æ\n",
    "\n",
    "#### ğŸ“Œ ç¯„ä¾‹ 3: LEAD & LAG - æ™‚é–“åºåˆ—åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bcf3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# å»ºç«‹æœˆåº¦éŠ·å”®è³‡æ–™\n",
    "sales_data = [\n",
    "    (\"2024-01\", 100000),\n",
    "    (\"2024-02\", 120000),\n",
    "    (\"2024-03\", 115000),\n",
    "    (\"2024-04\", 130000),\n",
    "    (\"2024-05\", 135000),\n",
    "    (\"2024-06\", 140000),\n",
    "]\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, [\"æœˆä»½\", \"éŠ·å”®é¡\"])\n",
    "\n",
    "# å®šç¾©è¦–çª—è¦æ ¼\n",
    "windowSpec = Window.orderBy(\"æœˆä»½\")\n",
    "\n",
    "result = sales_df.withColumn(\"ä¸ŠæœˆéŠ·å”®é¡\", lag(\"éŠ·å”®é¡\", 1).over(windowSpec)) \\\n",
    "                 .withColumn(\"ä¸‹æœˆéŠ·å”®é¡\", lead(\"éŠ·å”®é¡\", 1).over(windowSpec)) \\\n",
    "                 .withColumn(\"æœˆå¢é•·é¡\", col(\"éŠ·å”®é¡\") - lag(\"éŠ·å”®é¡\", 1).over(windowSpec)) \\\n",
    "                 .withColumn(\"æœˆå¢é•·ç‡%\", \n",
    "                            round((col(\"éŠ·å”®é¡\") - lag(\"éŠ·å”®é¡\", 1).over(windowSpec)) / \n",
    "                                  lag(\"éŠ·å”®é¡\", 1).over(windowSpec) * 100, 2))\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee624b43",
   "metadata": {
    "region_name": "md",
    "title": "\\"
   },
   "source": [
    "### 4ï¸âƒ£ ç´¯è¨ˆè¨ˆç®— - SUM/AVG OVER\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "è¦–çª—å‡½æ•¸é…åˆèšåˆå‡½æ•¸å¯ä»¥é€²è¡Œç´¯è¨ˆè¨ˆç®—ï¼Œè€Œä¸æœƒæŠ˜ç–Šè³‡æ–™åˆ—ã€‚\n",
    "\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š\n",
    "ç´¯è¨ˆéŠ·å”®é¡ã€ç§»å‹•å¹³å‡ã€æ»¾å‹•çµ±è¨ˆ\n",
    "\n",
    "\n",
    "\n",
    "#### ğŸ“Œ ç¯„ä¾‹ 4: ç´¯è¨ˆè¨ˆç®—èˆ‡ç§»å‹•å¹³å‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0b40eb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# ç¹¼çºŒä½¿ç”¨éŠ·å”®è³‡æ–™\n",
    "windowSpec = Window.orderBy(\"æœˆä»½\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "movingAvgSpec = Window.orderBy(\"æœˆä»½\").rowsBetween(-2, 0)  # 3å€‹æœˆç§»å‹•å¹³å‡\n",
    "\n",
    "result = sales_df.withColumn(\"ç´¯è¨ˆéŠ·å”®é¡\", sum(\"éŠ·å”®é¡\").over(windowSpec)) \\\n",
    "                 .withColumn(\"3æœˆç§»å‹•å¹³å‡\", round(avg(\"éŠ·å”®é¡\").over(movingAvgSpec), 2))\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b626cd",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## ğŸ“š ç¬¬äºŒç« ï¼šé€šç”¨è¡¨è¡¨é”å¼ (Common Table Expressions - CTE)\n",
    "\n",
    "### 5ï¸âƒ£ WITH å­å¥ - å»ºç«‹è‡¨æ™‚çµæœé›†\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "CTE (WITH å­å¥) è®“ä½ å»ºç«‹è‡¨æ™‚çš„å‘½åçµæœé›†ï¼Œæé«˜æŸ¥è©¢çš„å¯è®€æ€§å’Œå¯ç¶­è­·æ€§ã€‚\n",
    "å¯ä»¥æŠŠè¤‡é›œæŸ¥è©¢åˆ†è§£ç‚ºå¤šå€‹æ­¥é©Ÿã€‚\n",
    "\n",
    "ğŸ¯ å„ªå‹¢ï¼š\n",
    "1. æé«˜å¯è®€æ€§\n",
    "2. å¯é‡è¤‡ä½¿ç”¨\n",
    "3. ä¾¿æ–¼é™¤éŒ¯\n",
    "4. æ”¯æ´éè¿´æŸ¥è©¢\n",
    "\n",
    "#### ğŸ“Œ ç¯„ä¾‹ 5: CTE - ç°¡åŒ–è¤‡é›œæŸ¥è©¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479c670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# å»ºç«‹è¨‚å–®è³‡æ–™\n",
    "orders_data = [\n",
    "    (1, \"Alice\", \"2024-01-15\", 1500),\n",
    "    (2, \"Bob\", \"2024-01-20\", 2300),\n",
    "    (3, \"Alice\", \"2024-02-10\", 1800),\n",
    "    (4, \"Charlie\", \"2024-02-15\", 3200),\n",
    "    (5, \"Bob\", \"2024-03-05\", 2100),\n",
    "    (6, \"Alice\", \"2024-03-20\", 2500),\n",
    "]\n",
    "\n",
    "orders_df = spark.createDataFrame(\n",
    "    orders_data,\n",
    "    [\"è¨‚å–®ç·¨è™Ÿ\", \"å®¢æˆ¶\", \"è¨‚å–®æ—¥æœŸ\", \"é‡‘é¡\"]\n",
    ")\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "# ä½¿ç”¨ CTE è¨ˆç®—å®¢æˆ¶ç¸½æ¶ˆè²»å’Œå¹³å‡æ¶ˆè²»\n",
    "sql_with_cte = spark.sql(\"\"\"\n",
    "    WITH customer_stats AS (\n",
    "        SELECT \n",
    "            `å®¢æˆ¶`,\n",
    "            COUNT(*) as `è¨‚å–®æ•¸`,\n",
    "            SUM(`é‡‘é¡`) as `ç¸½æ¶ˆè²»`,\n",
    "            AVG(`é‡‘é¡`) as `å¹³å‡æ¶ˆè²»`\n",
    "        FROM orders\n",
    "        GROUP BY `å®¢æˆ¶`\n",
    "    ),\n",
    "    high_value_customers AS (\n",
    "        SELECT \n",
    "            `å®¢æˆ¶`,\n",
    "            `è¨‚å–®æ•¸`,\n",
    "            `ç¸½æ¶ˆè²»`,\n",
    "            ROUND(`å¹³å‡æ¶ˆè²»`, 2) as `å¹³å‡æ¶ˆè²»`\n",
    "        FROM customer_stats\n",
    "        WHERE `ç¸½æ¶ˆè²»` > 5000\n",
    "    )\n",
    "    SELECT * FROM high_value_customers\n",
    "    ORDER BY `ç¸½æ¶ˆè²»` DESC\n",
    "\"\"\")\n",
    "\n",
    "sql_with_cte.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a176aa19",
   "metadata": {
    "region_name": "md",
    "title": "\\"
   },
   "source": [
    "### 6ï¸âƒ£ å¤šå±¤ CTE - è¤‡é›œæ¥­å‹™é‚è¼¯\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "å¤šå±¤ CTE å¯ä»¥å°‡è¤‡é›œçš„æ¥­å‹™é‚è¼¯åˆ†è§£ç‚ºå¤šå€‹æ¸…æ™°çš„æ­¥é©Ÿã€‚\n",
    "\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š\n",
    "å¤šæ­¥é©Ÿçš„è³‡æ–™è½‰æ›ã€è¤‡é›œçš„æ¥­å‹™æŒ‡æ¨™è¨ˆç®—\n",
    "\n",
    "#### ğŸ“Œ ç¯„ä¾‹ 6: å¤šå±¤ CTE - å®¢æˆ¶åˆ†ç´šåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b718a6c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "sql_multi_cte = spark.sql(\"\"\"\n",
    "    WITH monthly_sales AS (\n",
    "        -- ç¬¬ä¸€å±¤ï¼šè¨ˆç®—æ¯å€‹å®¢æˆ¶çš„æœˆåº¦éŠ·å”®\n",
    "        SELECT \n",
    "            `å®¢æˆ¶`,\n",
    "            DATE_FORMAT(TO_DATE(`è¨‚å–®æ—¥æœŸ`), 'yyyy-MM') as `æœˆä»½`,\n",
    "            SUM(`é‡‘é¡`) as `æœˆéŠ·å”®é¡`\n",
    "        FROM orders\n",
    "        GROUP BY `å®¢æˆ¶`, DATE_FORMAT(TO_DATE(`è¨‚å–®æ—¥æœŸ`), 'yyyy-MM')\n",
    "    ),\n",
    "    customer_summary AS (\n",
    "        -- ç¬¬äºŒå±¤ï¼šåŒ¯ç¸½å®¢æˆ¶çµ±è¨ˆ\n",
    "        SELECT \n",
    "            `å®¢æˆ¶`,\n",
    "            COUNT(DISTINCT `æœˆä»½`) as `æ´»èºæœˆæ•¸`,\n",
    "            SUM(`æœˆéŠ·å”®é¡`) as `ç¸½éŠ·å”®é¡`,\n",
    "            AVG(`æœˆéŠ·å”®é¡`) as `å¹³å‡æœˆéŠ·å”®é¡`\n",
    "        FROM monthly_sales\n",
    "        GROUP BY `å®¢æˆ¶`\n",
    "    ),\n",
    "    customer_level AS (\n",
    "        -- ç¬¬ä¸‰å±¤ï¼šå®¢æˆ¶åˆ†ç´š\n",
    "        SELECT \n",
    "            `å®¢æˆ¶`,\n",
    "            `æ´»èºæœˆæ•¸`,\n",
    "            ROUND(`ç¸½éŠ·å”®é¡`, 2) as `ç¸½éŠ·å”®é¡`,\n",
    "            ROUND(`å¹³å‡æœˆéŠ·å”®é¡`, 2) as `å¹³å‡æœˆéŠ·å”®é¡`,\n",
    "            CASE \n",
    "                WHEN `ç¸½éŠ·å”®é¡` >= 6000 THEN 'ç™½é‡‘å®¢æˆ¶'\n",
    "                WHEN `ç¸½éŠ·å”®é¡` >= 4000 THEN 'é‡‘ç‰Œå®¢æˆ¶'\n",
    "                ELSE 'ä¸€èˆ¬å®¢æˆ¶'\n",
    "            END as `å®¢æˆ¶ç­‰ç´š`\n",
    "        FROM customer_summary\n",
    "    )\n",
    "    SELECT * FROM customer_level\n",
    "    ORDER BY `ç¸½éŠ·å”®é¡` DESC\n",
    "\"\"\")\n",
    "\n",
    "sql_multi_cte.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3047463",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## ğŸ“š ç¬¬ä¸‰ç« ï¼šå­æŸ¥è©¢å„ªåŒ–\n",
    "7ï¸âƒ£ ç›¸é—œå­æŸ¥è©¢ vs éç›¸é—œå­æŸ¥è©¢\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "- éç›¸é—œå­æŸ¥è©¢ï¼šå…§éƒ¨æŸ¥è©¢ç¨ç«‹åŸ·è¡Œï¼ŒåªåŸ·è¡Œä¸€æ¬¡\n",
    "- ç›¸é—œå­æŸ¥è©¢ï¼šå…§éƒ¨æŸ¥è©¢ä¾è³´å¤–éƒ¨æŸ¥è©¢ï¼Œå¯èƒ½åŸ·è¡Œå¤šæ¬¡ï¼ˆæ•ˆèƒ½è¼ƒå·®ï¼‰\n",
    "\n",
    "ğŸ¯ å„ªåŒ–å»ºè­°ï¼š\n",
    "ç›¡é‡ä½¿ç”¨ JOIN æˆ–è¦–çª—å‡½æ•¸æ›¿ä»£ç›¸é—œå­æŸ¥è©¢\n",
    "\n",
    "#### ğŸ“Œ ç¯„ä¾‹ 7: å­æŸ¥è©¢å„ªåŒ– - æ‰¾å‡ºé«˜æ–¼éƒ¨é–€å¹³å‡è–ªè³‡çš„å“¡å·¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec11f827",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# æ–¹æ³• 1ï¼šä½¿ç”¨ç›¸é—œå­æŸ¥è©¢ï¼ˆè¼ƒæ…¢ï¼‰\n",
    "sql_correlated = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e1.`å§“å`,\n",
    "        e1.`éƒ¨é–€`,\n",
    "        e1.`è–ªè³‡`,\n",
    "        (SELECT AVG(e2.`è–ªè³‡`) \n",
    "         FROM employees e2 \n",
    "         WHERE e2.`éƒ¨é–€` = e1.`éƒ¨é–€`) as `éƒ¨é–€å¹³å‡è–ªè³‡`\n",
    "    FROM employees e1\n",
    "    WHERE e1.`è–ªè³‡` > (\n",
    "        SELECT AVG(e2.`è–ªè³‡`) \n",
    "        FROM employees e2 \n",
    "        WHERE e2.`éƒ¨é–€` = e1.`éƒ¨é–€`\n",
    "    )\n",
    "    ORDER BY e1.`éƒ¨é–€`, e1.`è–ªè³‡` DESC\n",
    "\"\"\")\n",
    "\n",
    "sql_correlated.show()\n",
    "\n",
    "# æ–¹æ³• 2ï¼šä½¿ç”¨ JOINï¼ˆè¼ƒå¿«ï¼‰\n",
    "sql_join = spark.sql(\"\"\"\n",
    "    WITH dept_avg AS (\n",
    "        SELECT `éƒ¨é–€`, AVG(`è–ªè³‡`) as `å¹³å‡è–ªè³‡`\n",
    "        FROM employees\n",
    "        GROUP BY `éƒ¨é–€`\n",
    "    )\n",
    "    SELECT \n",
    "        e.`å§“å`,\n",
    "        e.`éƒ¨é–€`,\n",
    "        e.`è–ªè³‡`,\n",
    "        ROUND(d.`å¹³å‡è–ªè³‡`, 2) as `éƒ¨é–€å¹³å‡è–ªè³‡`\n",
    "    FROM employees e\n",
    "    JOIN dept_avg d ON e.`éƒ¨é–€` = d.`éƒ¨é–€`\n",
    "    WHERE e.`è–ªè³‡` > d.`å¹³å‡è–ªè³‡`\n",
    "    ORDER BY e.`éƒ¨é–€`, e.`è–ªè³‡` DESC\n",
    "\"\"\")\n",
    "\n",
    "sql_join.show()\n",
    "\n",
    "# æ–¹æ³• 3ï¼šä½¿ç”¨è¦–çª—å‡½æ•¸ï¼ˆæœ€å¿«ï¼‰\n",
    "windowSpec = Window.partitionBy(\"éƒ¨é–€\")\n",
    "result_window = employees_df.withColumn(\"éƒ¨é–€å¹³å‡è–ªè³‡\", round(avg(\"è–ªè³‡\").over(windowSpec), 2)) \\\n",
    "                            .filter(col(\"è–ªè³‡\") > col(\"éƒ¨é–€å¹³å‡è–ªè³‡\")) \\\n",
    "                            .orderBy(\"éƒ¨é–€\", desc(\"è–ªè³‡\"))\n",
    "\n",
    "result_window.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d188b10",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## ğŸ“š ç¬¬å››ç« ï¼šé€²éš JOIN æŠ€å·§\n",
    "\n",
    "\n",
    "### 8ï¸âƒ£ CROSS JOIN - ç¬›å¡çˆ¾ç©\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "CROSS JOIN ç”¢ç”Ÿå…©å€‹è¡¨çš„ç¬›å¡çˆ¾ç©ï¼Œå³æ‰€æœ‰å¯èƒ½çš„çµ„åˆã€‚\n",
    "\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š\n",
    "ç”Ÿæˆæ¸¬è©¦è³‡æ–™ã€å»ºç«‹æ™‚é–“åºåˆ—ã€ç”¢ç”Ÿæ‰€æœ‰å¯èƒ½çš„çµ„åˆ\n",
    "\n",
    "#### ğŸ“Œ ç¯„ä¾‹ 8: CROSS JOIN - ç”¢ç”Ÿæ‰€æœ‰å¯èƒ½çš„é…å°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86116881",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# å»ºç«‹ç”¢å“å’Œé¡è‰²è³‡æ–™\n",
    "products = spark.createDataFrame([(\"æ‰‹æ©Ÿ\",), (\"å¹³æ¿\",), (\"ç­†é›»\",)], [\"ç”¢å“\"])\n",
    "colors = spark.createDataFrame([(\"é»‘è‰²\",), (\"ç™½è‰²\",), (\"éŠ€è‰²\",)], [\"é¡è‰²\"])\n",
    "\n",
    "# CROSS JOIN\n",
    "cross_result = products.crossJoin(colors)\n",
    "\n",
    "cross_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a895f",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 9ï¸âƒ£ SELF JOIN - è‡ªæˆ‘é€£æ¥\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "SELF JOIN æ˜¯è¡¨èˆ‡è‡ªå·±é€²è¡Œé€£æ¥ï¼Œå¸¸ç”¨æ–¼æŸ¥æ‰¾å±¤ç´šé—œä¿‚æˆ–æ¯”è¼ƒåŒè¡¨å…§çš„è¨˜éŒ„ã€‚\n",
    "\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š\n",
    "å“¡å·¥-ä¸»ç®¡é—œä¿‚ã€çµ„ç¹”æ¶æ§‹ã€å°‹æ‰¾é‡è¤‡è¨˜éŒ„\n",
    "\n",
    "#### ğŸ“Œ ç¯„ä¾‹ 9: SELF JOIN - å“¡å·¥èˆ‡ä¸»ç®¡é—œä¿‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2661a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# å»ºç«‹åŒ…å«ä¸»ç®¡è³‡è¨Šçš„å“¡å·¥è³‡æ–™\n",
    "emp_manager_data = [\n",
    "    (1, \"Alice\", None),      # CEOï¼Œæ²’æœ‰ä¸»ç®¡\n",
    "    (2, \"Bob\", 1),           # Bob çš„ä¸»ç®¡æ˜¯ Alice\n",
    "    (3, \"Charlie\", 1),       # Charlie çš„ä¸»ç®¡æ˜¯ Alice\n",
    "    (4, \"David\", 2),         # David çš„ä¸»ç®¡æ˜¯ Bob\n",
    "    (5, \"Eve\", 2),           # Eve çš„ä¸»ç®¡æ˜¯ Bob\n",
    "    (6, \"Frank\", 3),         # Frank çš„ä¸»ç®¡æ˜¯ Charlie\n",
    "]\n",
    "\n",
    "emp_manager_df = spark.createDataFrame(\n",
    "    emp_manager_data,\n",
    "    [\"å“¡å·¥ç·¨è™Ÿ\", \"å§“å\", \"ä¸»ç®¡ç·¨è™Ÿ\"]\n",
    ")\n",
    "emp_manager_df.createOrReplaceTempView(\"emp_manager\")\n",
    "\n",
    "# ä½¿ç”¨ SELF JOIN æŸ¥è©¢å“¡å·¥å’Œå…¶ä¸»ç®¡\n",
    "sql_self_join = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e.`å“¡å·¥ç·¨è™Ÿ`,\n",
    "        e.`å§“å` as `å“¡å·¥å§“å`,\n",
    "        COALESCE(m.`å§“å`, 'ç„¡ä¸»ç®¡') as `ä¸»ç®¡å§“å`\n",
    "    FROM emp_manager e\n",
    "    LEFT JOIN emp_manager m ON e.`ä¸»ç®¡ç·¨è™Ÿ` = m.`å“¡å·¥ç·¨è™Ÿ`\n",
    "    ORDER BY e.`å“¡å·¥ç·¨è™Ÿ`\n",
    "\"\"\")\n",
    "\n",
    "sql_self_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c369b3ba",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### ğŸ”Ÿ ANTI JOIN & SEMI JOIN - é«˜æ•ˆéæ¿¾\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "- SEMI JOIN (LEFT SEMI): è¿”å›å·¦è¡¨ä¸­åœ¨å³è¡¨ä¸­æœ‰åŒ¹é…çš„è¨˜éŒ„\n",
    "- ANTI JOIN (LEFT ANTI): è¿”å›å·¦è¡¨ä¸­åœ¨å³è¡¨ä¸­æ²’æœ‰åŒ¹é…çš„è¨˜éŒ„\n",
    "\n",
    "ğŸ¯ å„ªå‹¢ï¼š\n",
    "æ¯” IN / NOT IN å­æŸ¥è©¢æ•ˆèƒ½æ›´å¥½\n",
    "\n",
    "#### ğŸ“Œ ç¯„ä¾‹ 10: ANTI JOIN & SEMI JOIN - å®¢æˆ¶è¨‚å–®åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcac099f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# å»ºç«‹å®¢æˆ¶è³‡æ–™\n",
    "customers_data = [\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (3, \"Charlie\"),\n",
    "    (4, \"David\"),\n",
    "    (5, \"Eve\"),\n",
    "]\n",
    "\n",
    "customers_df = spark.createDataFrame(customers_data, [\"å®¢æˆ¶ç·¨è™Ÿ\", \"å®¢æˆ¶å§“å\"])\n",
    "\n",
    "# å»ºç«‹è¨‚å–®è³‡æ–™ï¼ˆåªæœ‰éƒ¨åˆ†å®¢æˆ¶æœ‰è¨‚å–®ï¼‰\n",
    "order_customers = orders_df.select(\"å®¢æˆ¶\").distinct()\n",
    "\n",
    "# SEMI JOIN - æœ‰è¨‚å–®çš„å®¢æˆ¶\n",
    "customers_with_orders = customers_df.join(\n",
    "    order_customers,\n",
    "    customers_df[\"å®¢æˆ¶å§“å\"] == order_customers[\"å®¢æˆ¶\"],\n",
    "    \"leftsemi\"\n",
    ")\n",
    "\n",
    "customers_with_orders.show()\n",
    "\n",
    "# ANTI JOIN - æ²’æœ‰è¨‚å–®çš„å®¢æˆ¶\n",
    "customers_without_orders = customers_df.join(\n",
    "    order_customers,\n",
    "    customers_df[\"å®¢æˆ¶å§“å\"] == order_customers[\"å®¢æˆ¶\"],\n",
    "    \"leftanti\"\n",
    ")\n",
    "\n",
    "customers_without_orders.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8437164a",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## ğŸ“š ç¬¬äº”ç« ï¼šæ•ˆèƒ½å„ªåŒ–æŠ€å·§\n",
    "### 1ï¸âƒ£1ï¸âƒ£ Broadcast Join - å»£æ’­å°è¡¨\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "ç•¶ä¸€å€‹å¤§è¡¨å’Œä¸€å€‹å°è¡¨é€²è¡Œ JOIN æ™‚ï¼Œå¯ä»¥å°‡å°è¡¨å»£æ’­åˆ°æ‰€æœ‰ç¯€é»ï¼Œ\n",
    "é¿å… shuffle æ“ä½œï¼Œå¤§å¹…æå‡æ•ˆèƒ½ã€‚\n",
    "\n",
    "ğŸ¯ é©ç”¨å ´æ™¯ï¼š\n",
    "å°è¡¨ < 10MBï¼Œå¤§è¡¨èˆ‡ç¶­åº¦è¡¨ JOIN\n",
    "\n",
    "#### ğŸ“Œ ç¯„ä¾‹ 11: Broadcast Join - å„ªåŒ–å¤§å°è¡¨é€£æ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f44c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# å»ºç«‹éƒ¨é–€è³‡è¨Šï¼ˆå°è¡¨ï¼‰\n",
    "departments_data = [\n",
    "    (\"å·¥ç¨‹éƒ¨\", \"æŠ€è¡“å¤§æ¨“\"),\n",
    "    (\"è¡ŒéŠ·éƒ¨\", \"è¡Œæ”¿å¤§æ¨“\"),\n",
    "    (\"äººè³‡éƒ¨\", \"è¡Œæ”¿å¤§æ¨“\"),\n",
    "]\n",
    "\n",
    "departments_df = spark.createDataFrame(departments_data, [\"éƒ¨é–€\", \"è¾¦å…¬åœ°é»\"])\n",
    "\n",
    "# ä¸€èˆ¬ JOIN\n",
    "normal_join = employees_df.join(departments_df, \"éƒ¨é–€\")\n",
    "\n",
    "normal_join.select(\"å§“å\", \"éƒ¨é–€\", \"è¾¦å…¬åœ°é»\").show(5)\n",
    "\n",
    "# Broadcast JOIN\n",
    "broadcast_join = employees_df.join(\n",
    "    broadcast(departments_df),\n",
    "    \"éƒ¨é–€\"\n",
    ")\n",
    "\n",
    "broadcast_join.select(\"å§“å\", \"éƒ¨é–€\", \"è¾¦å…¬åœ°é»\").show(5)\n",
    "\n",
    "# æŸ¥çœ‹åŸ·è¡Œè¨ˆç•«\n",
    "print(\"ä¸€èˆ¬ JOIN æœƒæœ‰ SortMergeJoin\")\n",
    "print(\"Broadcast JOIN æœƒæœ‰ BroadcastHashJoin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ce0a5",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 1ï¸âƒ£2ï¸âƒ£ åˆ†å€èˆ‡åˆ†æ¡¶ - è³‡æ–™çµ„ç¹”å„ªåŒ–\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "- Partitioning: æŒ‰æ¬„ä½å€¼å°‡è³‡æ–™åˆ†å‰²æˆå¤šå€‹ç›®éŒ„\n",
    "- Bucketing: æŒ‰ hash å€¼å°‡è³‡æ–™åˆ†å‰²æˆå›ºå®šæ•¸é‡çš„æª”æ¡ˆ\n",
    "\n",
    "ğŸ¯ å„ªå‹¢ï¼š\n",
    "æ¸›å°‘æƒæçš„è³‡æ–™é‡ï¼Œæå‡æŸ¥è©¢æ•ˆèƒ½\n",
    "\n",
    "#### ğŸ“Œ ç¯„ä¾‹ 12: åˆ†å€èˆ‡éæ¿¾å„ªåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502f195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# é‡æ–°åˆ†å€è³‡æ–™\n",
    "employees_partitioned = employees_df.repartition(2, \"éƒ¨é–€\")\n",
    "\n",
    "\n",
    "# ä½¿ç”¨åˆ†å€æ¬„ä½éæ¿¾ï¼ˆæ•ˆèƒ½æ›´å¥½ï¼‰\n",
    "filtered = employees_partitioned.filter(col(\"éƒ¨é–€\") == \"å·¥ç¨‹éƒ¨\")\n",
    "filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d059b3d5",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "\n",
    "### 1ï¸âƒ£3ï¸âƒ£ å¿«å–èˆ‡æŒä¹…åŒ– - é¿å…é‡è¤‡è¨ˆç®—\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "å°‡å¸¸ç”¨çš„ DataFrame å¿«å–åˆ°è¨˜æ†¶é«”ä¸­ï¼Œé¿å…é‡è¤‡è¨ˆç®—ã€‚\n",
    "\n",
    "ğŸ¯ é©ç”¨å ´æ™¯ï¼š\n",
    "å¤šæ¬¡ä½¿ç”¨ç›¸åŒçš„ä¸­é–“çµæœã€è¿­ä»£è¨ˆç®—\n",
    "\n",
    "#### ğŸ“Œ ç¯„ä¾‹ 13: å¿«å–å„ªåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef55a2f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# å»ºç«‹ä¸€å€‹è¤‡é›œçš„è¨ˆç®—\n",
    "complex_df = employees_df.groupBy(\"éƒ¨é–€\") \\\n",
    "                        .agg(\n",
    "                            count(\"*\").alias(\"äººæ•¸\"),\n",
    "                            avg(\"è–ªè³‡\").alias(\"å¹³å‡è–ªè³‡\"),\n",
    "                            max(\"è–ªè³‡\").alias(\"æœ€é«˜è–ªè³‡\")\n",
    "                        )\n",
    "\n",
    "# å¿«å–çµæœ\n",
    "complex_df.cache()\n",
    "\n",
    "# ç¬¬ä¸€æ¬¡åŸ·è¡Œï¼ˆæœƒè§¸ç™¼è¨ˆç®—ä¸¦å¿«å–ï¼‰\n",
    "start = time.time()\n",
    "complex_df.show()\n",
    "print(f\"åŸ·è¡Œæ™‚é–“: {time.time() - start:.4f} ç§’\")\n",
    "\n",
    "# ç¬¬äºŒæ¬¡åŸ·è¡Œï¼ˆå¾å¿«å–è®€å–ï¼‰\n",
    "start = time.time()\n",
    "complex_df.show()\n",
    "print(f\"åŸ·è¡Œæ™‚é–“: {time.time() - start:.4f} ç§’ï¼ˆæ‡‰è©²æ›´å¿«ï¼‰\")\n",
    "\n",
    "# é‡‹æ”¾å¿«å–\n",
    "complex_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7895cc6",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## ğŸ“š ç¬¬å…­ç« ï¼šè¤‡é›œè³‡æ–™è½‰æ›\n",
    "### 1ï¸âƒ£4ï¸âƒ£ PIVOT - è¡Œè½‰åˆ—\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "PIVOT å°‡è¡Œè³‡æ–™è½‰æ›ç‚ºåˆ—ï¼Œå¸¸ç”¨æ–¼å»ºç«‹äº¤å‰è¡¨ã€‚\n",
    "\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š\n",
    "éŠ·å”®å ±è¡¨ã€è³‡æ–™é€è¦–è¡¨ã€è¶¨å‹¢åˆ†æ\n",
    "\n",
    "#### ğŸ“Œ ç¯„ä¾‹ 14: PIVOT - å»ºç«‹éƒ¨é–€è–ªè³‡é€è¦–è¡¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f6ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ä½¿ç”¨ PIVOT å»ºç«‹éƒ¨é–€-çµ±è¨ˆæŒ‡æ¨™é€è¦–è¡¨\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "pivot_result = spark.sql(\"\"\"\n",
    "    SELECT * FROM (\n",
    "        SELECT `éƒ¨é–€`, `è–ªè³‡`\n",
    "        FROM employees\n",
    "    )\n",
    "    PIVOT (\n",
    "        COUNT(*) as `äººæ•¸`,\n",
    "        AVG(`è–ªè³‡`) as `å¹³å‡è–ªè³‡`\n",
    "        FOR `éƒ¨é–€` IN ('å·¥ç¨‹éƒ¨', 'è¡ŒéŠ·éƒ¨', 'äººè³‡éƒ¨')\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "pivot_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b27372",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "\n",
    "### 1ï¸âƒ£5ï¸âƒ£ UNPIVOT - åˆ—è½‰è¡Œ\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "UNPIVOT å°‡åˆ—è³‡æ–™è½‰æ›ç‚ºè¡Œï¼Œæ˜¯ PIVOT çš„åå‘æ“ä½œã€‚\n",
    "\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š\n",
    "å°‡å¯¬è¡¨è½‰æ›ç‚ºé•·è¡¨ã€è³‡æ–™æ­£è¦åŒ–\n",
    "\n",
    "#### ğŸ“Œ ç¯„ä¾‹ 15: UNPIVOT - åˆ—è½‰è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81ccac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# å»ºç«‹å¯¬è¡¨æ ¼å¼çš„å­£åº¦éŠ·å”®è³‡æ–™\n",
    "quarterly_sales = spark.createDataFrame([\n",
    "    (\"ç”¢å“A\", 100, 120, 115, 130),\n",
    "    (\"ç”¢å“B\", 80, 90, 95, 100),\n",
    "], [\"ç”¢å“\", \"Q1\", \"Q2\", \"Q3\", \"Q4\"])\n",
    "\n",
    "quarterly_sales.createOrReplaceTempView(\"quarterly_sales\")\n",
    "\n",
    "# ä½¿ç”¨ UNPIVOTï¼ˆSpark 3.4+ï¼‰æˆ–ä½¿ç”¨ stack å‡½æ•¸\n",
    "unpivot_result = quarterly_sales.selectExpr(\n",
    "    \"`ç”¢å“`\",\n",
    "    \"stack(4, 'Q1', `Q1`, 'Q2', `Q2`, 'Q3', `Q3`, 'Q4', `Q4`) as (`å­£åº¦`, `éŠ·å”®é¡`)\"\n",
    ")\n",
    "\n",
    "unpivot_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0453e6fd",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "\n",
    "### 1ï¸âƒ£6ï¸âƒ£ é™£åˆ—èˆ‡çµæ§‹è™•ç†\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "PySpark æ”¯æ´è¤‡é›œçš„è³‡æ–™å‹æ…‹å¦‚é™£åˆ—ã€çµæ§‹é«”ã€åœ°åœ–ç­‰ã€‚\n",
    "\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š\n",
    "è™•ç† JSON è³‡æ–™ã€å·¢ç‹€çµæ§‹ã€å¤šå€¼æ¬„ä½\n",
    "\n",
    "#### ğŸ“Œ ç¯„ä¾‹ 16: é™£åˆ—æ“ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892aac92",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# å»ºç«‹åŒ…å«é™£åˆ—çš„è³‡æ–™\n",
    "students_data = [\n",
    "    (1, \"Alice\", [\"æ•¸å­¸\", \"ç‰©ç†\", \"åŒ–å­¸\"]),\n",
    "    (2, \"Bob\", [\"è‹±æ–‡\", \"æ­·å²\"]),\n",
    "    (3, \"Charlie\", [\"æ•¸å­¸\", \"è‹±æ–‡\", \"é«”è‚²\"]),\n",
    "]\n",
    "\n",
    "students_df = spark.createDataFrame(\n",
    "    students_data,\n",
    "    [\"å­¸è™Ÿ\", \"å§“å\", \"é¸ä¿®èª²ç¨‹\"]\n",
    ")\n",
    "\n",
    "students_df.show(truncate=False)\n",
    "\n",
    "# å±•é–‹é™£åˆ—\n",
    "exploded = students_df.select(\n",
    "    \"å­¸è™Ÿ\",\n",
    "    \"å§“å\",\n",
    "    explode(\"é¸ä¿®èª²ç¨‹\").alias(\"èª²ç¨‹\")\n",
    ")\n",
    "\n",
    "exploded.show()\n",
    "\n",
    "# é™£åˆ—ç›¸é—œå‡½æ•¸\n",
    "array_operations = students_df.select(\n",
    "    \"å§“å\",\n",
    "    \"é¸ä¿®èª²ç¨‹\",\n",
    "    size(\"é¸ä¿®èª²ç¨‹\").alias(\"èª²ç¨‹æ•¸\"),\n",
    "    array_contains(\"é¸ä¿®èª²ç¨‹\", \"æ•¸å­¸\").alias(\"æ˜¯å¦é¸ä¿®æ•¸å­¸\")\n",
    ")\n",
    "\n",
    "array_operations.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb50581",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## ğŸ“š ç¬¬ä¸ƒç« ï¼šè³‡æ–™å“è³ªèˆ‡æ¸…ç†\n",
    "### 1ï¸âƒ£7ï¸âƒ£ è™•ç† NULL å€¼\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "NULL å€¼è™•ç†æ˜¯è³‡æ–™æ¸…ç†çš„é‡è¦ç’°ç¯€ã€‚\n",
    "\n",
    "ğŸ¯ å¸¸ç”¨æ–¹æ³•ï¼š\n",
    "fillna, dropna, coalesce, nvl\n",
    "\n",
    "#### ğŸ“Œ ç¯„ä¾‹ 17: NULL å€¼è™•ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba11916",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# å»ºç«‹åŒ…å« NULL çš„è³‡æ–™\n",
    "data_with_null = [\n",
    "    (1, \"Alice\", 75000, \"å·¥ç¨‹éƒ¨\"),\n",
    "    (2, \"Bob\", None, \"è¡ŒéŠ·éƒ¨\"),\n",
    "    (3, \"Charlie\", 68000, None),\n",
    "    (4, None, 65000, \"äººè³‡éƒ¨\"),\n",
    "]\n",
    "\n",
    "df_null = spark.createDataFrame(\n",
    "    data_with_null,\n",
    "    [\"å“¡å·¥ç·¨è™Ÿ\", \"å§“å\", \"è–ªè³‡\", \"éƒ¨é–€\"]\n",
    ")\n",
    "\n",
    "df_null.show()\n",
    "\n",
    "# æ–¹æ³• 1: å¡«å……é è¨­å€¼\n",
    "filled = df_null.fillna({\n",
    "    \"å§“å\": \"æœªçŸ¥\",\n",
    "    \"è–ªè³‡\": 60000,\n",
    "    \"éƒ¨é–€\": \"å¾…åˆ†é…\"\n",
    "})\n",
    "\n",
    "filled.show()\n",
    "\n",
    "# æ–¹æ³• 2: åˆªé™¤åŒ…å« NULL çš„åˆ—\n",
    "dropped = df_null.dropna()\n",
    "\n",
    "dropped.show()\n",
    "\n",
    "# æ–¹æ³• 3: ä½¿ç”¨ COALESCE\n",
    "coalesced = df_null.select(\n",
    "    \"å“¡å·¥ç·¨è™Ÿ\",\n",
    "    coalesce(col(\"å§“å\"), lit(\"æœªçŸ¥\")).alias(\"å§“å\"),\n",
    "    coalesce(col(\"è–ªè³‡\"), lit(60000)).alias(\"è–ªè³‡\"),\n",
    "    coalesce(col(\"éƒ¨é–€\"), lit(\"å¾…åˆ†é…\")).alias(\"éƒ¨é–€\")\n",
    ")\n",
    "\n",
    "coalesced.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84daa24",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "\n",
    "### 1ï¸âƒ£8ï¸âƒ£ è³‡æ–™å»é‡\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "æ‰¾å‡ºä¸¦ç§»é™¤é‡è¤‡çš„è¨˜éŒ„ã€‚\n",
    "\n",
    "ğŸ¯ æ–¹æ³•ï¼š\n",
    "distinct, dropDuplicates, è¦–çª—å‡½æ•¸\n",
    "\n",
    "#### ğŸ“Œ ç¯„ä¾‹ 18: è³‡æ–™å»é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112c71a7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# å»ºç«‹åŒ…å«é‡è¤‡è³‡æ–™\n",
    "duplicate_data = [\n",
    "    (1, \"Alice\", \"å·¥ç¨‹éƒ¨\"),\n",
    "    (2, \"Bob\", \"è¡ŒéŠ·éƒ¨\"),\n",
    "    (1, \"Alice\", \"å·¥ç¨‹éƒ¨\"),  # å®Œå…¨é‡è¤‡\n",
    "    (3, \"Alice\", \"äººè³‡éƒ¨\"),  # å§“åé‡è¤‡\n",
    "]\n",
    "\n",
    "df_dup = spark.createDataFrame(\n",
    "    duplicate_data,\n",
    "    [\"å“¡å·¥ç·¨è™Ÿ\", \"å§“å\", \"éƒ¨é–€\"]\n",
    ")\n",
    "\n",
    "df_dup.show()\n",
    "\n",
    "# æ–¹æ³• 1: å®Œå…¨å»é‡\n",
    "dedup_all = df_dup.distinct()\n",
    "dedup_all.show()\n",
    "\n",
    "# æ–¹æ³• 2: æŒ‰ç‰¹å®šæ¬„ä½å»é‡ï¼ˆä¿ç•™ç¬¬ä¸€ç­†ï¼‰\n",
    "dedup_by_name = df_dup.dropDuplicates([\"å§“å\"])\n",
    "dedup_by_name.show()\n",
    "\n",
    "# æ–¹æ³• 3: ä½¿ç”¨è¦–çª—å‡½æ•¸ä¿ç•™ç‰¹å®šè¦å‰‡çš„è¨˜éŒ„\n",
    "windowSpec = Window.partitionBy(\"å§“å\").orderBy(\"å“¡å·¥ç·¨è™Ÿ\")\n",
    "dedup_window = df_dup.withColumn(\"rn\", row_number().over(windowSpec)) \\\n",
    "                     .filter(col(\"rn\") == 1) \\\n",
    "                     .drop(\"rn\")\n",
    "\n",
    "dedup_window.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99a76eb",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## ğŸ“š ç¬¬å…«ç« ï¼šé€²éšåˆ†æå‡½æ•¸\n",
    "### 1ï¸âƒ£9ï¸âƒ£ NTILE - è³‡æ–™åˆ†çµ„\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "NTILE å°‡è³‡æ–™åˆ†æˆ N å€‹å¤§è‡´ç›¸ç­‰çš„çµ„ã€‚\n",
    "\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š\n",
    "å®¢æˆ¶åˆ†å±¤ã€ABC åˆ†æã€åˆ†ä½æ•¸åˆ†æ\n",
    "\n",
    "#### ğŸ“Œ ç¯„ä¾‹ 19: NTILE - è–ªè³‡å››åˆ†ä½æ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738d2299",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "windowSpec = Window.orderBy(\"è–ªè³‡\")\n",
    "\n",
    "ntile_result = employees_df.withColumn(\n",
    "    \"è–ªè³‡åˆ†çµ„\",\n",
    "    ntile(4).over(windowSpec)\n",
    ").withColumn(\n",
    "    \"åˆ†çµ„èªªæ˜\",\n",
    "    when(col(\"è–ªè³‡åˆ†çµ„\") == 1, \"ä½è–ªçµ„\")\n",
    "    .when(col(\"è–ªè³‡åˆ†çµ„\") == 2, \"ä¸­ä½è–ªçµ„\")\n",
    "    .when(col(\"è–ªè³‡åˆ†çµ„\") == 3, \"ä¸­é«˜è–ªçµ„\")\n",
    "    .otherwise(\"é«˜è–ªçµ„\")\n",
    ")\n",
    "\n",
    "ntile_result.orderBy(\"è–ªè³‡\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8894eb",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "\n",
    "### 2ï¸âƒ£0ï¸âƒ£ PERCENT_RANK - ç™¾åˆ†ä½æ’å\n",
    "\n",
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "PERCENT_RANK è¨ˆç®—å€¼åœ¨è³‡æ–™é›†ä¸­çš„ç›¸å°ä½ç½®ï¼ˆ0 åˆ° 1 ä¹‹é–“ï¼‰ã€‚\n",
    "\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š\n",
    "ç¸¾æ•ˆè©•ä¼°ã€æˆç¸¾åˆ†æã€ç›¸å°æ’å\n",
    "\n",
    "#### ğŸ“Œ ç¯„ä¾‹ 20: PERCENT_RANK - è–ªè³‡ç™¾åˆ†ä½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fe5bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "windowSpec = Window.orderBy(\"è–ªè³‡\")\n",
    "\n",
    "percent_result = employees_df.withColumn(\n",
    "    \"è–ªè³‡ç™¾åˆ†ä½\",\n",
    "    round(percent_rank().over(windowSpec) * 100, 2)\n",
    ").withColumn(\n",
    "    \"èªªæ˜\",\n",
    "    concat(\n",
    "        lit(\"è–ªè³‡è¶…é \"),\n",
    "        round(percent_rank().over(windowSpec) * 100, 0).cast(\"int\"),\n",
    "        lit(\"% çš„å“¡å·¥\")\n",
    "    )\n",
    ")\n",
    "\n",
    "percent_result.orderBy(\"è–ªè³‡\").select(\"å§“å\", \"è–ªè³‡\", \"è–ªè³‡ç™¾åˆ†ä½\", \"èªªæ˜\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4f9f0e",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "# ğŸ“ èª²ç¨‹ç¸½çµ\n",
    "## ğŸŠ æ­å–œï¼ä½ å·²ç¶“å®Œæˆäº† SQL é€²éšèª²ç¨‹ï¼\n",
    "ğŸ“ é€²éšæŠ€èƒ½ç¸½çµï¼š\n",
    "\n",
    "1ï¸âƒ£ è¦–çª—å‡½æ•¸ï¼š\n",
    "   - ROW_NUMBER, RANK, DENSE_RANKï¼šæ’åèˆ‡ç·¨è™Ÿ\n",
    "   - LEAD, LAGï¼šè¨ªå•å‰å¾Œåˆ—è³‡æ–™\n",
    "   - SUM/AVG OVERï¼šç´¯è¨ˆè¨ˆç®—èˆ‡ç§»å‹•å¹³å‡\n",
    "   - NTILE, PERCENT_RANKï¼šåˆ†çµ„èˆ‡ç™¾åˆ†ä½\n",
    "\n",
    "2ï¸âƒ£ é€šç”¨è¡¨è¡¨é”å¼ (CTE)ï¼š\n",
    "   - WITH å­å¥ï¼šæé«˜æŸ¥è©¢å¯è®€æ€§\n",
    "   - å¤šå±¤ CTEï¼šåˆ†è§£è¤‡é›œé‚è¼¯\n",
    "   - éè¿´ CTEï¼šè™•ç†å±¤ç´šé—œä¿‚\n",
    "\n",
    "3ï¸âƒ£ å­æŸ¥è©¢å„ªåŒ–ï¼š\n",
    "   - é¿å…ç›¸é—œå­æŸ¥è©¢\n",
    "   - ä½¿ç”¨ JOIN æ›¿ä»£\n",
    "   - ä½¿ç”¨è¦–çª—å‡½æ•¸å„ªåŒ–\n",
    "\n",
    "4ï¸âƒ£ é€²éš JOINï¼š\n",
    "   - CROSS JOINï¼šç¬›å¡çˆ¾ç©\n",
    "   - SELF JOINï¼šè‡ªæˆ‘é€£æ¥\n",
    "   - SEMI/ANTI JOINï¼šé«˜æ•ˆéæ¿¾\n",
    "\n",
    "5ï¸âƒ£ æ•ˆèƒ½å„ªåŒ–ï¼š\n",
    "   - Broadcast Joinï¼šå»£æ’­å°è¡¨\n",
    "   - åˆ†å€èˆ‡åˆ†æ¡¶ï¼šè³‡æ–™çµ„ç¹”\n",
    "   - å¿«å–èˆ‡æŒä¹…åŒ–ï¼šé¿å…é‡è¤‡è¨ˆç®—\n",
    "\n",
    "6ï¸âƒ£ è¤‡é›œè³‡æ–™è½‰æ›ï¼š\n",
    "   - PIVOT/UNPIVOTï¼šè¡Œåˆ—è½‰æ›\n",
    "   - é™£åˆ—èˆ‡çµæ§‹ï¼šè™•ç†è¤‡é›œå‹æ…‹\n",
    "   - è³‡æ–™å±•é–‹èˆ‡èšåˆ\n",
    "\n",
    "7ï¸âƒ£ è³‡æ–™å“è³ªï¼š\n",
    "   - NULL å€¼è™•ç†\n",
    "   - è³‡æ–™å»é‡\n",
    "   - è³‡æ–™é©—è­‰\n",
    "\n",
    "ğŸ’¡ é€²éšå­¸ç¿’å»ºè­°ï¼š\n",
    "- ç†è§£åŸ·è¡Œè¨ˆç•«ï¼Œå­¸æœƒæ•ˆèƒ½åˆ†æ\n",
    "- æŒæ¡è³‡æ–™åˆ†å€ç­–ç•¥\n",
    "- ç†Ÿæ‚‰ Spark UI çš„ä½¿ç”¨\n",
    "- å¯¦è¸ Delta Lake é€²éšåŠŸèƒ½\n",
    "- å­¸ç¿’ Spark Streaming\n",
    "\n",
    "ğŸš€ ä¸‹ä¸€æ­¥ï¼š\n",
    "- æ·±å…¥ç ”ç©¶ Spark å…§éƒ¨æ©Ÿåˆ¶\n",
    "- å­¸ç¿’åˆ†æ•£å¼ç³»çµ±åŸç†\n",
    "- å¯¦è¸å¤§è¦æ¨¡è³‡æ–™è™•ç†å°ˆæ¡ˆ\n",
    "- æ¢ç´¢æ©Ÿå™¨å­¸ç¿’èˆ‡ Spark MLlib\n",
    "\n",
    "è¨˜ä½ï¼šé€²éšæŠ€èƒ½éœ€è¦å¤§é‡å¯¦è¸ï¼Œå¤šåšå°ˆæ¡ˆã€å¤šè§£æ±ºå¯¦éš›å•é¡Œæ‰èƒ½çœŸæ­£æŒæ¡ï¼\n",
    "# ğŸ“š é€²éšæ•™å­¸æ–‡ä»¶çµæŸ - ç¹¼çºŒç²¾é€²ä½ çš„æŠ€èƒ½ï¼ğŸš€\n",
    "\n",
    "é—œé–‰ Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6840a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,region_name,-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
