{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark SQL 進階教學 🚀\n",
    "\n",
    "這份教學專為已經掌握 SQL 基礎的你設計！\n",
    "我們將深入探討進階的 SQL 技巧，包括視窗函數、CTE、效能優化等主題。\n",
    "讓你的 SQL 技能更上一層樓！✨\n",
    "\n",
    "作者：QChoice AI 教學團隊  \n",
    "日期：2025-01-15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "# 創建 Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SQL進階教學\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"🚀 歡迎來到 SQL 進階課程！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 環境設定 - 準備進階開發環境\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "# 創建 Spark Session - 配置進階效能參數\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SQL進階教學\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "print(\"🚀 歡迎來到 SQL 進階課程！\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 第一章：視窗函數 (Window Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📚 第一章：視窗函數 (Window Functions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎈 概念解釋：\n",
    "ROW_NUMBER() 為分區內的每一列分配一個唯一的序號。\n",
    "常用於排名、去重、分頁等場景。\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，為每個部門的員工按薪資排序並分配序號。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📌 範例 1: ROW_NUMBER - 排名與編號\")\n",
    "# 建立員工資料\n",
    "employees_data = [\n",
    "    (1, \"Alice\", \"工程部\", 75000),\n",
    "    (2, \"Bob\", \"工程部\", 82000),\n",
    "    (3, \"Charlie\", \"工程部\", 68000),\n",
    "    (4, \"David\", \"行銷部\", 65000),\n",
    "    (5, \"Eve\", \"行銷部\", 72000),\n",
    "    (6, \"Frank\", \"行銷部\", 70000),\n",
    "    (7, \"Grace\", \"人資部\", 60000),\n",
    "    (8, \"Henry\", \"人資部\", 63000)\n",
    "]\n",
    "employees_df = spark.createDataFrame(\n",
    "    employees_data, \n",
    "    [\"員工編號\", \"姓名\", \"部門\", \"薪資\"]\n",
    ")\n",
    "# 定義視窗規格：按部門分組，按薪資降序排列\n",
    "windowSpec = Window.partitionBy(\"部門\").orderBy(desc(\"薪資\"))\n",
    "# 使用 ROW_NUMBER\n",
    "result = employees_df.withColumn(\"部門排名\", row_number().over(windowSpec))\n",
    "print(\"\\n使用 ROW_NUMBER 進行部門內排名：\")\n",
    "result.orderBy(\"部門\", \"部門排名\").show()\n",
    "# SQL 寫法\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        `員工編號`,\n",
    "        `姓名`,\n",
    "        `部門`,\n",
    "        `薪資`,\n",
    "        ROW_NUMBER() OVER (PARTITION BY `部門` ORDER BY `薪資` DESC) as `部門排名`\n",
    "    FROM employees\n",
    "    ORDER BY `部門`, `部門排名`\n",
    "\"\"\")\n",
    "print(\"\\nSQL 寫法結果：\")\n",
    "sql_result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎈 概念解釋：\n",
    "RANK() 和 DENSE_RANK() 都用於排名，但處理相同值的方式不同：\n",
    "- RANK(): 相同值得到相同排名，下一個排名會跳號\n",
    "- DENSE_RANK(): 相同值得到相同排名，下一個排名連續\n",
    "\n",
    "🎯 應用場景：\n",
    "成績排名、銷售業績排名等需要處理並列情況的場景\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📌 範例 2: RANK vs DENSE_RANK - 並列排名處理\")\n",
    "# 建立考試成績資料\n",
    "scores_data = [\n",
    "    (1, \"Alice\", 95),\n",
    "    (2, \"Bob\", 95),\n",
    "    (3, \"Charlie\", 90),\n",
    "    (4, \"David\", 90),\n",
    "    (5, \"Eve\", 85),\n",
    "    (6, \"Frank\", 80),\n",
    "]\n",
    "scores_df = spark.createDataFrame(scores_data, [\"學號\", \"姓名\", \"分數\"])\n",
    "# 定義視窗規格\n",
    "windowSpec = Window.orderBy(desc(\"分數\"))\n",
    "result = scores_df.withColumn(\"RANK\", rank().over(windowSpec)) \\\n",
    "                  .withColumn(\"DENSE_RANK\", dense_rank().over(windowSpec)) \\\n",
    "                  .withColumn(\"ROW_NUMBER\", row_number().over(windowSpec))\n",
    "print(\"\\n比較三種排名函數：\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 說明：\n",
    "- Alice 和 Bob 都是 95 分，RANK 和 DENSE_RANK 都是 1\n",
    "- Charlie 和 David 都是 90 分：\n",
    "  * RANK 是 3（因為前面有 2 個人）\n",
    "  * DENSE_RANK 是 2（連續編號）\n",
    "- ROW_NUMBER 為每個人分配唯一編號\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎈 概念解釋：\n",
    "LEAD() 和 LAG() 允許你訪問當前列之前或之後的資料：\n",
    "- LAG(): 訪問前面的列\n",
    "- LEAD(): 訪問後面的列\n",
    "\n",
    "🎯 應用場景：\n",
    "計算增長率、同比環比分析、時間序列分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📌 範例 3: LEAD & LAG - 時間序列分析\")\n",
    "# 建立月度銷售資料\n",
    "sales_data = [\n",
    "    (\"2024-01\", 100000),\n",
    "    (\"2024-02\", 120000),\n",
    "    (\"2024-03\", 115000),\n",
    "    (\"2024-04\", 130000),\n",
    "    (\"2024-05\", 135000),\n",
    "    (\"2024-06\", 140000),\n",
    "]\n",
    "sales_df = spark.createDataFrame(sales_data, [\"月份\", \"銷售額\"])\n",
    "# 定義視窗規格\n",
    "windowSpec = Window.orderBy(\"月份\")\n",
    "result = sales_df.withColumn(\"上月銷售額\", lag(\"銷售額\", 1).over(windowSpec)) \\\n",
    "                 .withColumn(\"下月銷售額\", lead(\"銷售額\", 1).over(windowSpec)) \\\n",
    "                 .withColumn(\"月增長額\", col(\"銷售額\") - lag(\"銷售額\", 1).over(windowSpec)) \\\n",
    "                 .withColumn(\"月增長率%\", \n",
    "                            round((col(\"銷售額\") - lag(\"銷售額\", 1).over(windowSpec)) / \n",
    "                                  lag(\"銷售額\", 1).over(windowSpec) * 100, 2))\n",
    "print(\"\\n銷售額月度分析：\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎈 概念解釋：\n",
    "視窗函數配合聚合函數可以進行累計計算，而不會折疊資料列。\n",
    "\n",
    "🎯 應用場景：\n",
    "累計銷售額、移動平均、滾動統計\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📌 範例 4: 累計計算與移動平均\")\n",
    "# 繼續使用銷售資料\n",
    "windowSpec = Window.orderBy(\"月份\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "movingAvgSpec = Window.orderBy(\"月份\").rowsBetween(-2, 0)  # 3個月移動平均\n",
    "result = sales_df.withColumn(\"累計銷售額\", sum(\"銷售額\").over(windowSpec)) \\\n",
    "                 .withColumn(\"3月移動平均\", round(avg(\"銷售額\").over(movingAvgSpec), 2))\n",
    "print(\"\\n累計銷售額與移動平均：\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💪 練習題 1\n",
    "\n",
    "**題目：**\n",
    "\n",
    "請建立一個銷售業績分析系統：\n",
    "1. 建立銷售員月度銷售資料\n",
    "2. 使用 ROW_NUMBER 為每個區域的銷售員排名\n",
    "3. 使用 LAG 計算每月環比增長率\n",
    "4. 計算每個銷售員的累計業績\n",
    "5. 計算 3 個月移動平均\n",
    "\n",
    "**提示：綜合運用 Window Functions：row_number、lag、sum、avg**\n",
    "\n",
    "<details>\n",
    "<summary>📝 點擊查看參考答案</summary>\n",
    "\n",
    "```python\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, lag, col, round, sum, avg\n",
    "\n",
    "# 1. 建立銷售資料\n",
    "sales_data = [\n",
    "    (\"張三\", \"北區\", \"2024-01\", 150000),\n",
    "    (\"張三\", \"北區\", \"2024-02\", 180000),\n",
    "    (\"張三\", \"北區\", \"2024-03\", 165000),\n",
    "    (\"李四\", \"北區\", \"2024-01\", 140000),\n",
    "    (\"李四\", \"北區\", \"2024-02\", 155000),\n",
    "    (\"李四\", \"北區\", \"2024-03\", 170000),\n",
    "    (\"王五\", \"南區\", \"2024-01\", 200000),\n",
    "    (\"王五\", \"南區\", \"2024-02\", 220000),\n",
    "    (\"王五\", \"南區\", \"2024-03\", 210000)\n",
    "]\n",
    "\n",
    "sales_df = spark.createDataFrame(\n",
    "    sales_data,\n",
    "    [\"銷售員\", \"區域\", \"月份\", \"銷售額\"]\n",
    ")\n",
    "\n",
    "print(\"=== 1. 原始銷售資料 ===\")\n",
    "sales_df.orderBy(\"區域\", \"銷售員\", \"月份\").show()\n",
    "\n",
    "# 2. 區域排名（按總業績）\n",
    "print(\"\\n=== 2. 各區域銷售員排名 ===\")\n",
    "window_rank = Window.partitionBy(\"區域\").orderBy(col(\"銷售額\").desc())\n",
    "ranked_df = sales_df.withColumn(\"區域排名\", row_number().over(window_rank))\n",
    "ranked_df.orderBy(\"區域\", \"區域排名\", \"月份\").show()\n",
    "\n",
    "# 3. 環比增長率\n",
    "print(\"\\n=== 3. 月度環比增長率 ===\")\n",
    "window_lag = Window.partitionBy(\"銷售員\").orderBy(\"月份\")\n",
    "growth_df = sales_df \\\n",
    "    .withColumn(\"上月銷售額\", lag(\"銷售額\", 1).over(window_lag)) \\\n",
    "    .withColumn(\n",
    "        \"環比增長率%\",\n",
    "        round((col(\"銷售額\") - col(\"上月銷售額\")) / col(\"上月銷售額\") * 100, 2)\n",
    "    )\n",
    "growth_df.orderBy(\"銷售員\", \"月份\").show()\n",
    "\n",
    "# 4. 累計業績\n",
    "print(\"\\n=== 4. 累計業績 ===\")\n",
    "window_cumsum = Window.partitionBy(\"銷售員\").orderBy(\"月份\") \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "cumsum_df = sales_df \\\n",
    "    .withColumn(\"累計銷售額\", sum(\"銷售額\").over(window_cumsum))\n",
    "cumsum_df.orderBy(\"銷售員\", \"月份\").show()\n",
    "\n",
    "# 5. 3個月移動平均\n",
    "print(\"\\n=== 5. 3個月移動平均 ===\")\n",
    "window_ma = Window.partitionBy(\"銷售員\").orderBy(\"月份\") \\\n",
    "    .rowsBetween(-2, Window.currentRow)\n",
    "ma_df = sales_df \\\n",
    "    .withColumn(\"3月移動平均\", round(avg(\"銷售額\").over(window_ma), 0))\n",
    "ma_df.orderBy(\"銷售員\", \"月份\").show()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 第二章：通用表表達式 (Common Table Expressions - CTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📚 第二章：通用表表達式 (Common Table Expressions - CTE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎈 概念解釋：\n",
    "CTE (WITH 子句) 讓你建立臨時的命名結果集，提高查詢的可讀性和可維護性。\n",
    "可以把複雜查詢分解為多個步驟。\n",
    "\n",
    "🎯 優勢：\n",
    "1. 提高可讀性\n",
    "2. 可重複使用\n",
    "3. 便於除錯\n",
    "4. 支援遞迴查詢\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📌 範例 5: CTE - 簡化複雜查詢\")\n",
    "# 建立訂單資料\n",
    "orders_data = [\n",
    "    (1, \"Alice\", \"2024-01-15\", 1500),\n",
    "    (2, \"Bob\", \"2024-01-20\", 2300),\n",
    "    (3, \"Alice\", \"2024-02-10\", 1800),\n",
    "    (4, \"Charlie\", \"2024-02-15\", 3200),\n",
    "    (5, \"Bob\", \"2024-03-05\", 2100),\n",
    "    (6, \"Alice\", \"2024-03-20\", 2500),\n",
    "]\n",
    "orders_df = spark.createDataFrame(\n",
    "    orders_data,\n",
    "    [\"訂單編號\", \"客戶\", \"訂單日期\", \"金額\"]\n",
    ")\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "# 使用 CTE 計算客戶總消費和平均消費\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_with_cte = spark.sql(\"\"\"\n",
    "    WITH customer_stats AS (\n",
    "        SELECT \n",
    "            `客戶`,\n",
    "            COUNT(*) as `訂單數`,\n",
    "            SUM(`金額`) as `總消費`,\n",
    "            AVG(`金額`) as `平均消費`\n",
    "        FROM orders\n",
    "        GROUP BY `客戶`\n",
    "    ),\n",
    "    high_value_customers AS (\n",
    "        SELECT \n",
    "            `客戶`,\n",
    "            `訂單數`,\n",
    "            `總消費`,\n",
    "            ROUND(`平均消費`, 2) as `平均消費`\n",
    "        FROM customer_stats\n",
    "        WHERE `總消費` > 5000\n",
    "    )\n",
    "    SELECT * FROM high_value_customers\n",
    "    ORDER BY `總消費` DESC\n",
    "\"\"\")\n",
    "print(\"\\n高價值客戶分析（使用 CTE）：\")\n",
    "sql_with_cte.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎈 概念解釋：\n",
    "多層 CTE 可以將複雜的業務邏輯分解為多個清晰的步驟。\n",
    "\n",
    "🎯 應用場景：\n",
    "多步驟的資料轉換、複雜的業務指標計算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📌 範例 6: 多層 CTE - 客戶分級分析\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_multi_cte = spark.sql(\"\"\"\n",
    "    WITH monthly_sales AS (\n",
    "        -- 第一層：計算每個客戶的月度銷售\n",
    "        SELECT \n",
    "            `客戶`,\n",
    "            DATE_FORMAT(TO_DATE(`訂單日期`), 'yyyy-MM') as `月份`,\n",
    "            SUM(`金額`) as `月銷售額`\n",
    "        FROM orders\n",
    "        GROUP BY `客戶`, DATE_FORMAT(TO_DATE(`訂單日期`), 'yyyy-MM')\n",
    "    ),\n",
    "    customer_summary AS (\n",
    "        -- 第二層：匯總客戶統計\n",
    "        SELECT \n",
    "            `客戶`,\n",
    "            COUNT(DISTINCT `月份`) as `活躍月數`,\n",
    "            SUM(`月銷售額`) as `總銷售額`,\n",
    "            AVG(`月銷售額`) as `平均月銷售額`\n",
    "        FROM monthly_sales\n",
    "        GROUP BY `客戶`\n",
    "    ),\n",
    "    customer_level AS (\n",
    "        -- 第三層：客戶分級\n",
    "        SELECT \n",
    "            `客戶`,\n",
    "            `活躍月數`,\n",
    "            ROUND(`總銷售額`, 2) as `總銷售額`,\n",
    "            ROUND(`平均月銷售額`, 2) as `平均月銷售額`,\n",
    "            CASE \n",
    "                WHEN `總銷售額` >= 6000 THEN '白金客戶'\n",
    "                WHEN `總銷售額` >= 4000 THEN '金牌客戶'\n",
    "                ELSE '一般客戶'\n",
    "            END as `客戶等級`\n",
    "        FROM customer_summary\n",
    "    )\n",
    "    SELECT * FROM customer_level\n",
    "    ORDER BY `總銷售額` DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n客戶分級結果：\")\n",
    "sql_multi_cte.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💪 練習題 2\n",
    "\n",
    "**題目：**\n",
    "\n",
    "請使用 CTE 建立一個完整的客戶價值分析系統：\n",
    "1. 計算每個客戶的 RFM 指標（最近購買、購買頻率、購買金額）\n",
    "2. 根據 RFM 分數對客戶分級\n",
    "3. 分析各級別客戶的特徵\n",
    "4. 給出營銷建議\n",
    "\n",
    "**提示：使用多層 CTE，結合 CASE WHEN 進行分級**\n",
    "\n",
    "<details>\n",
    "<summary>📝 點擊查看參考答案</summary>\n",
    "\n",
    "```python\n",
    "from datetime import date\n",
    "\n",
    "# 建立訂單資料\n",
    "orders_data = [\n",
    "    (1, \"Alice\", \"2024-01-05\", 1500),\n",
    "    (2, \"Alice\", \"2024-02-10\", 2000),\n",
    "    (3, \"Alice\", \"2024-03-15\", 1800),\n",
    "    (4, \"Bob\", \"2024-01-08\", 3000),\n",
    "    (5, \"Bob\", \"2024-01-20\", 2500),\n",
    "    (6, \"Charlie\", \"2024-02-01\", 5000),\n",
    "    (7, \"David\", \"2023-12-15\", 1000),\n",
    "    (8, \"Eve\", \"2024-03-10\", 4000),\n",
    "    (9, \"Eve\", \"2024-03-20\", 3500)\n",
    "]\n",
    "\n",
    "orders_df = spark.createDataFrame(\n",
    "    orders_data,\n",
    "    [\"訂單編號\", \"客戶\", \"訂單日期\", \"金額\"]\n",
    ")\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "# 使用多層 CTE 進行 RFM 分析\n",
    "result = spark.sql(\"\"\"\n",
    "WITH rfm_base AS (\n",
    "    -- 計算基礎 RFM 指標\n",
    "    SELECT \n",
    "        客戶,\n",
    "        DATEDIFF('2024-03-31', MAX(訂單日期)) as 最近購買天數,\n",
    "        COUNT(*) as 購買頻率,\n",
    "        SUM(金額) as 購買金額,\n",
    "        ROUND(AVG(金額), 0) as 平均訂單金額\n",
    "    FROM orders\n",
    "    GROUP BY 客戶\n",
    "),\n",
    "rfm_score AS (\n",
    "    -- 計算 RFM 分數（1-5分）\n",
    "    SELECT \n",
    "        客戶,\n",
    "        最近購買天數,\n",
    "        購買頻率,\n",
    "        購買金額,\n",
    "        平均訂單金額,\n",
    "        CASE \n",
    "            WHEN 最近購買天數 <= 15 THEN 5\n",
    "            WHEN 最近購買天數 <= 30 THEN 4\n",
    "            WHEN 最近購買天數 <= 60 THEN 3\n",
    "            WHEN 最近購買天數 <= 90 THEN 2\n",
    "            ELSE 1\n",
    "        END as R分數,\n",
    "        CASE \n",
    "            WHEN 購買頻率 >= 3 THEN 5\n",
    "            WHEN 購買頻率 >= 2 THEN 4\n",
    "            ELSE 3\n",
    "        END as F分數,\n",
    "        CASE \n",
    "            WHEN 購買金額 >= 5000 THEN 5\n",
    "            WHEN 購買金額 >= 4000 THEN 4\n",
    "            WHEN 購買金額 >= 3000 THEN 3\n",
    "            ELSE 2\n",
    "        END as M分數\n",
    "    FROM rfm_base\n",
    "),\n",
    "customer_segment AS (\n",
    "    -- 客戶分級\n",
    "    SELECT \n",
    "        客戶,\n",
    "        最近購買天數,\n",
    "        購買頻率,\n",
    "        購買金額,\n",
    "        平均訂單金額,\n",
    "        R分數,\n",
    "        F分數,\n",
    "        M分數,\n",
    "        (R分數 + F分數 + M分數) as 總分,\n",
    "        CASE \n",
    "            WHEN R分數 >= 4 AND F分數 >= 4 AND M分數 >= 4 THEN '重要價值客戶'\n",
    "            WHEN R分數 >= 4 AND F分數 < 4 THEN '重要喚回客戶'\n",
    "            WHEN R分數 < 4 AND F分數 >= 4 THEN '重要保持客戶'\n",
    "            WHEN M分數 >= 4 THEN '重要發展客戶'\n",
    "            ELSE '一般客戶'\n",
    "        END as 客戶等級\n",
    "    FROM rfm_score\n",
    ")\n",
    "SELECT \n",
    "    客戶,\n",
    "    客戶等級,\n",
    "    最近購買天數 || '天前' as 最後購買,\n",
    "    購買頻率 || '次' as 購買次數,\n",
    "    購買金額,\n",
    "    平均訂單金額,\n",
    "    總分\n",
    "FROM customer_segment\n",
    "ORDER BY 總分 DESC, 購買金額 DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== RFM 客戶價值分析 ===\")\n",
    "result.show(truncate=False)\n",
    "\n",
    "# 統計各等級客戶分布\n",
    "print(\"\\n=== 客戶等級分布 ===\")\n",
    "segment_stats = spark.sql(\"\"\"\n",
    "WITH rfm_base AS (\n",
    "    SELECT \n",
    "        客戶,\n",
    "        DATEDIFF('2024-03-31', MAX(訂單日期)) as 最近購買天數,\n",
    "        COUNT(*) as 購買頻率,\n",
    "        SUM(金額) as 購買金額\n",
    "    FROM orders\n",
    "    GROUP BY 客戶\n",
    "),\n",
    "rfm_score AS (\n",
    "    SELECT \n",
    "        客戶,\n",
    "        CASE WHEN 最近購買天數 <= 15 THEN 5 WHEN 最近購買天數 <= 30 THEN 4 WHEN 最近購買天數 <= 60 THEN 3 WHEN 最近購買天數 <= 90 THEN 2 ELSE 1 END as R分數,\n",
    "        CASE WHEN 購買頻率 >= 3 THEN 5 WHEN 購買頻率 >= 2 THEN 4 ELSE 3 END as F分數,\n",
    "        CASE WHEN 購買金額 >= 5000 THEN 5 WHEN 購買金額 >= 4000 THEN 4 WHEN 購買金額 >= 3000 THEN 3 ELSE 2 END as M分數\n",
    "    FROM rfm_base\n",
    "),\n",
    "customer_segment AS (\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN R分數 >= 4 AND F分數 >= 4 AND M分數 >= 4 THEN '重要價值客戶'\n",
    "            WHEN R分數 >= 4 AND F分數 < 4 THEN '重要喚回客戶'\n",
    "            WHEN R分數 < 4 AND F分數 >= 4 THEN '重要保持客戶'\n",
    "            WHEN M分數 >= 4 THEN '重要發展客戶'\n",
    "            ELSE '一般客戶'\n",
    "        END as 客戶等級\n",
    "    FROM rfm_score\n",
    ")\n",
    "SELECT \n",
    "    客戶等級,\n",
    "    COUNT(*) as 客戶數量\n",
    "FROM customer_segment\n",
    "GROUP BY 客戶等級\n",
    "ORDER BY 客戶數量 DESC\n",
    "\"\"\")\n",
    "segment_stats.show(truncate=False)\n",
    "\n",
    "print(\"\"\"\n",
    "💡 營銷建議：\n",
    "1. 重要價值客戶：提供 VIP 服務，專屬優惠\n",
    "2. 重要喚回客戶：發送個性化促銷，新品推薦\n",
    "3. 重要保持客戶：會員積分獎勵，購買提醒\n",
    "4. 重要發展客戶：交叉銷售，提升購買頻率\n",
    "5. 一般客戶：大眾促銷活動\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 第三章：子查詢優化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📚 第三章：子查詢優化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎈 概念解釋：\n",
    "- 非相關子查詢：內部查詢獨立執行，只執行一次\n",
    "- 相關子查詢：內部查詢依賴外部查詢，可能執行多次（效能較差）\n",
    "\n",
    "🎯 優化建議：\n",
    "盡量使用 JOIN 或視窗函數替代相關子查詢\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📌 範例 7: 子查詢優化 - 找出高於部門平均薪資的員工\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法 1：使用相關子查詢（較慢）\n",
    "sql_correlated = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e1.`姓名`,\n",
    "        e1.`部門`,\n",
    "        e1.`薪資`,\n",
    "        (SELECT AVG(e2.`薪資`) \n",
    "         FROM employees e2 \n",
    "         WHERE e2.`部門` = e1.`部門`) as `部門平均薪資`\n",
    "    FROM employees e1\n",
    "    WHERE e1.`薪資` > (\n",
    "        SELECT AVG(e2.`薪資`) \n",
    "        FROM employees e2 \n",
    "        WHERE e2.`部門` = e1.`部門`\n",
    "    )\n",
    "    ORDER BY e1.`部門`, e1.`薪資` DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n方法 1 - 相關子查詢：\")\n",
    "sql_correlated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法 2：使用 JOIN（較快）\n",
    "sql_join = spark.sql(\"\"\"\n",
    "    WITH dept_avg AS (\n",
    "        SELECT `部門`, AVG(`薪資`) as `平均薪資`\n",
    "        FROM employees\n",
    "        GROUP BY `部門`\n",
    "    )\n",
    "    SELECT \n",
    "        e.`姓名`,\n",
    "        e.`部門`,\n",
    "        e.`薪資`,\n",
    "        ROUND(d.`平均薪資`, 2) as `部門平均薪資`\n",
    "    FROM employees e\n",
    "    JOIN dept_avg d ON e.`部門` = d.`部門`\n",
    "    WHERE e.`薪資` > d.`平均薪資`\n",
    "    ORDER BY e.`部門`, e.`薪資` DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n方法 2 - 使用 JOIN（推薦）：\")\n",
    "sql_join.show()\n",
    "# 方法 3：使用視窗函數（最快）\n",
    "windowSpec = Window.partitionBy(\"部門\")\n",
    "result_window = employees_df.withColumn(\"部門平均薪資\", round(avg(\"薪資\").over(windowSpec), 2)) \\\n",
    "                            .filter(col(\"薪資\") > col(\"部門平均薪資\")) \\\n",
    "                            .orderBy(\"部門\", desc(\"薪資\"))\n",
    "print(\"\\n方法 3 - 使用視窗函數（最推薦）：\")\n",
    "result_window.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💪 練習題 3\n",
    "\n",
    "**題目：**\n",
    "\n",
    "請完成一個綜合查詢優化練習：\n",
    "1. 找出銷售額超過部門平均值的員工（用3種方法實現）\n",
    "2. 比較子查詢、CTE、JOIN 三種方法的執行計畫\n",
    "3. 使用 broadcast join 優化小表連接\n",
    "4. 總結效能優化建議\n",
    "\n",
    "**提示：使用 explain()、broadcast() 函數**\n",
    "\n",
    "<details>\n",
    "<summary>📝 點擊查看參考答案</summary>\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import broadcast, col, avg\n",
    "\n",
    "# 建立員工銷售資料\n",
    "emp_sales_data = [\n",
    "    (1, \"張三\", \"業務部\", 150000),\n",
    "    (2, \"李四\", \"業務部\", 200000),\n",
    "    (3, \"王五\", \"業務部\", 120000),\n",
    "    (4, \"趙六\", \"工程部\", 180000),\n",
    "    (5, \"錢七\", \"工程部\", 160000),\n",
    "    (6, \"孫八\", \"工程部\", 140000)\n",
    "]\n",
    "\n",
    "emp_sales_df = spark.createDataFrame(\n",
    "    emp_sales_data,\n",
    "    [\"員工編號\", \"姓名\", \"部門\", \"銷售額\"]\n",
    ")\n",
    "emp_sales_df.createOrReplaceTempView(\"employee_sales\")\n",
    "\n",
    "# ====================================================================\n",
    "# 方法 1: 使用相關子查詢\n",
    "# ====================================================================\n",
    "print(\"=== 方法 1: 相關子查詢 ===\")\n",
    "result1 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e.姓名,\n",
    "        e.部門,\n",
    "        e.銷售額,\n",
    "        (SELECT AVG(銷售額) FROM employee_sales WHERE 部門 = e.部門) as 部門平均\n",
    "    FROM employee_sales e\n",
    "    WHERE e.銷售額 > (\n",
    "        SELECT AVG(銷售額) \n",
    "        FROM employee_sales \n",
    "        WHERE 部門 = e.部門\n",
    "    )\n",
    "    ORDER BY e.部門, e.銷售額 DESC\n",
    "\"\"\")\n",
    "result1.show()\n",
    "print(\"\\n執行計畫:\")\n",
    "result1.explain(mode=\"simple\")\n",
    "\n",
    "# ====================================================================\n",
    "# 方法 2: 使用 CTE\n",
    "# ====================================================================\n",
    "print(\"\\n=== 方法 2: CTE ===\")\n",
    "result2 = spark.sql(\"\"\"\n",
    "    WITH dept_avg AS (\n",
    "        SELECT \n",
    "            部門,\n",
    "            AVG(銷售額) as 平均銷售額\n",
    "        FROM employee_sales\n",
    "        GROUP BY 部門\n",
    "    )\n",
    "    SELECT \n",
    "        e.姓名,\n",
    "        e.部門,\n",
    "        e.銷售額,\n",
    "        d.平均銷售額\n",
    "    FROM employee_sales e\n",
    "    INNER JOIN dept_avg d ON e.部門 = d.部門\n",
    "    WHERE e.銷售額 > d.平均銷售額\n",
    "    ORDER BY e.部門, e.銷售額 DESC\n",
    "\"\"\")\n",
    "result2.show()\n",
    "print(\"\\n執行計畫:\")\n",
    "result2.explain(mode=\"simple\")\n",
    "\n",
    "# ====================================================================\n",
    "# 方法 3: 使用 Window Function（最優）\n",
    "# ====================================================================\n",
    "print(\"\\n=== 方法 3: Window Function（推薦）===\")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"部門\")\n",
    "\n",
    "result3 = emp_sales_df \\\n",
    "    .withColumn(\"部門平均\", avg(\"銷售額\").over(window_spec)) \\\n",
    "    .filter(col(\"銷售額\") > col(\"部門平均\")) \\\n",
    "    .orderBy(\"部門\", col(\"銷售額\").desc())\n",
    "\n",
    "result3.show()\n",
    "print(\"\\n執行計畫:\")\n",
    "result3.explain(mode=\"simple\")\n",
    "\n",
    "# ====================================================================\n",
    "# 方法 4: Broadcast Join 優化（小表場景）\n",
    "# ====================================================================\n",
    "print(\"\\n=== 方法 4: Broadcast Join ===\")\n",
    "\n",
    "# 計算部門平均（小表）\n",
    "dept_avg_df = emp_sales_df.groupBy(\"部門\") \\\n",
    "    .agg(avg(\"銷售額\").alias(\"平均銷售額\"))\n",
    "\n",
    "# 使用 broadcast 優化\n",
    "result4 = emp_sales_df.join(\n",
    "    broadcast(dept_avg_df),\n",
    "    \"部門\"\n",
    ").filter(col(\"銷售額\") > col(\"平均銷售額\")) \\\n",
    "    .select(\"姓名\", \"部門\", \"銷售額\", \"平均銷售額\") \\\n",
    "    .orderBy(\"部門\", col(\"銷售額\").desc())\n",
    "\n",
    "result4.show()\n",
    "print(\"\\n執行計畫（注意 BroadcastHashJoin）:\")\n",
    "result4.explain(mode=\"simple\")\n",
    "\n",
    "# ====================================================================\n",
    "# 效能比較總結\n",
    "# ====================================================================\n",
    "print(\"\"\"\n",
    "\\n📊 效能優化總結：\n",
    "\n",
    "1. **相關子查詢**：\n",
    "   - ❌ 效能最差，每行都執行一次子查詢\n",
    "   - ❌ 避免在大數據集使用\n",
    "\n",
    "2. **CTE + JOIN**：\n",
    "   - ✓ 效能較好，子查詢只執行一次\n",
    "   - ✓ 程式碼可讀性高\n",
    "   \n",
    "3. **Window Function**：\n",
    "   - ✅ 效能最佳，單次掃描完成\n",
    "   - ✅ Spark 內部優化良好\n",
    "   - ✅ **推薦使用**\n",
    "\n",
    "4. **Broadcast Join**：\n",
    "   - ✅ 適用於小表（<10MB）\n",
    "   - ✅ 避免 shuffle，效能提升明顯\n",
    "   - ⚠️  需確認表大小\n",
    "\n",
    "💡 最佳實踐：\n",
    "1. 優先使用 Window Functions\n",
    "2. 避免相關子查詢\n",
    "3. 小表使用 broadcast\n",
    "4. 合理分區（partition）\n",
    "5. 使用 cache() 暫存中間結果\n",
    "6. 查看執行計畫優化查詢\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 第四章：進階 JOIN 技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📚 第四章：進階 JOIN 技巧\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎈 概念解釋：\n",
    "CROSS JOIN 產生兩個表的笛卡爾積，即所有可能的組合。\n",
    "\n",
    "🎯 應用場景：\n",
    "生成測試資料、建立時間序列、產生所有可能的組合\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📌 範例 8: CROSS JOIN - 產生所有可能的配對\")\n",
    "# 建立產品和顏色資料\n",
    "products = spark.createDataFrame([(\"手機\",), (\"平板\",), (\"筆電\",)], [\"產品\"])\n",
    "colors = spark.createDataFrame([(\"黑色\",), (\"白色\",), (\"銀色\",)], [\"顏色\"])\n",
    "# CROSS JOIN\n",
    "cross_result = products.crossJoin(colors)\n",
    "print(\"\\n產品與顏色的所有組合：\")\n",
    "cross_result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎈 概念解釋：\n",
    "SELF JOIN 是表與自己進行連接，常用於查找層級關係或比較同表內的記錄。\n",
    "\n",
    "🎯 應用場景：\n",
    "員工-主管關係、組織架構、尋找重複記錄\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📌 範例 9: SELF JOIN - 員工與主管關係\")\n",
    "# 建立包含主管資訊的員工資料\n",
    "emp_manager_data = [\n",
    "    (1, \"Alice\", None),      # CEO，沒有主管\n",
    "    (2, \"Bob\", 1),           # Bob 的主管是 Alice\n",
    "    (3, \"Charlie\", 1),       # Charlie 的主管是 Alice\n",
    "    (4, \"David\", 2),         # David 的主管是 Bob\n",
    "    (5, \"Eve\", 2),           # Eve 的主管是 Bob\n",
    "    (6, \"Frank\", 3),         # Frank 的主管是 Charlie\n",
    "]\n",
    "emp_manager_df = spark.createDataFrame(\n",
    "    emp_manager_data,\n",
    "    [\"員工編號\", \"姓名\", \"主管編號\"]\n",
    ")\n",
    "emp_manager_df.createOrReplaceTempView(\"emp_manager\")\n",
    "# 使用 SELF JOIN 查詢員工和其主管\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 SELF JOIN 查詢員工和其主管\n",
    "sql_self_join = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e.`員工編號`,\n",
    "        e.`姓名` as `員工姓名`,\n",
    "        COALESCE(m.`姓名`, '無主管') as `主管姓名`\n",
    "    FROM emp_manager e\n",
    "    LEFT JOIN emp_manager m ON e.`主管編號` = m.`員工編號`\n",
    "    ORDER BY e.`員工編號`\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n員工與主管關係：\")\n",
    "sql_self_join.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔟 ANTI JOIN & SEMI JOIN - 高效過濾\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎈 概念解釋：\n",
    "- SEMI JOIN (LEFT SEMI): 返回左表中在右表中有匹配的記錄\n",
    "- ANTI JOIN (LEFT ANTI): 返回左表中在右表中沒有匹配的記錄\n",
    "\n",
    "🎯 優勢：\n",
    "比 IN / NOT IN 子查詢效能更好\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📌 範例 10: ANTI JOIN & SEMI JOIN - 客戶訂單分析\")\n",
    "# 建立客戶資料\n",
    "customers_data = [\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (3, \"Charlie\"),\n",
    "    (4, \"David\"),\n",
    "    (5, \"Eve\"),\n",
    "]\n",
    "customers_df = spark.createDataFrame(customers_data, [\"客戶編號\", \"客戶姓名\"])\n",
    "# 建立訂單資料（只有部分客戶有訂單）\n",
    "order_customers = orders_df.select(\"客戶\").distinct()\n",
    "# SEMI JOIN - 有訂單的客戶\n",
    "customers_with_orders = customers_df.join(\n",
    "    order_customers,\n",
    "    customers_df[\"客戶姓名\"] == order_customers[\"客戶\"],\n",
    "    \"leftsemi\"\n",
    ")\n",
    "print(\"\\n有訂單的客戶（SEMI JOIN）：\")\n",
    "customers_with_orders.show()\n",
    "# ANTI JOIN - 沒有訂單的客戶\n",
    "customers_without_orders = customers_df.join(\n",
    "    order_customers,\n",
    "    customers_df[\"客戶姓名\"] == order_customers[\"客戶\"],\n",
    "    \"leftanti\"\n",
    ")\n",
    "print(\"\\n沒有訂單的客戶（ANTI JOIN）：\")\n",
    "customers_without_orders.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 第五章：效能優化技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📚 第五章：效能優化技巧\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎈 概念解釋：\n",
    "當一個大表和一個小表進行 JOIN 時，可以將小表廣播到所有節點，\n",
    "避免 shuffle 操作，大幅提升效能。\n",
    "\n",
    "🎯 適用場景：\n",
    "小表 < 10MB，大表與維度表 JOIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📌 範例 11: Broadcast Join - 優化大小表連接\")\n",
    "# 建立部門資訊（小表）\n",
    "departments_data = [\n",
    "    (\"工程部\", \"技術大樓\"),\n",
    "    (\"行銷部\", \"行政大樓\"),\n",
    "    (\"人資部\", \"行政大樓\"),\n",
    "]\n",
    "departments_df = spark.createDataFrame(departments_data, [\"部門\", \"辦公地點\"])\n",
    "# 一般 JOIN\n",
    "normal_join = employees_df.join(departments_df, \"部門\")\n",
    "print(\"\\n一般 JOIN：\")\n",
    "normal_join.select(\"姓名\", \"部門\", \"辦公地點\").show(5)\n",
    "# Broadcast JOIN\n",
    "broadcast_join = employees_df.join(\n",
    "    broadcast(departments_df),\n",
    "    \"部門\"\n",
    ")\n",
    "print(\"\\nBroadcast JOIN（效能更好）：\")\n",
    "broadcast_join.select(\"姓名\", \"部門\", \"辦公地點\").show(5)\n",
    "# 查看執行計畫\n",
    "print(\"\\n執行計畫差異：\")\n",
    "print(\"一般 JOIN 會有 SortMergeJoin\")\n",
    "print(\"Broadcast JOIN 會有 BroadcastHashJoin\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎈 概念解釋：\n",
    "- Partitioning: 按欄位值將資料分割成多個目錄\n",
    "- Bucketing: 按 hash 值將資料分割成固定數量的檔案\n",
    "\n",
    "🎯 優勢：\n",
    "減少掃描的資料量，提升查詢效能\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📌 範例 12: 分區與過濾優化\")\n",
    "# 重新分區資料\n",
    "employees_partitioned = employees_df.repartition(2, \"部門\")\n",
    "print(\"\\n重新分區後的分區數：\", employees_partitioned.rdd.getNumPartitions())\n",
    "# 使用分區欄位過濾（效能更好）\n",
    "filtered = employees_partitioned.filter(col(\"部門\") == \"工程部\")\n",
    "print(\"\\n過濾後的結果：\")\n",
    "filtered.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎈 概念解釋：\n",
    "將常用的 DataFrame 快取到記憶體中，避免重複計算。\n",
    "\n",
    "🎯 適用場景：\n",
    "多次使用相同的中間結果、迭代計算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📌 範例 13: 快取優化\")\n",
    "# 建立一個複雜的計算\n",
    "complex_df = employees_df.groupBy(\"部門\") \\\n",
    "                        .agg(\n",
    "                            count(\"*\").alias(\"人數\"),\n",
    "                            avg(\"薪資\").alias(\"平均薪資\"),\n",
    "                            max(\"薪資\").alias(\"最高薪資\")\n",
    "                        )\n",
    "# 快取結果\n",
    "complex_df.cache()\n",
    "# 第一次執行（會觸發計算並快取）\n",
    "print(\"\\n第一次查詢（計算並快取）：\")\n",
    "start = time.time()\n",
    "complex_df.show()\n",
    "print(f\"執行時間: {time.time() - start:.4f} 秒\")\n",
    "# 第二次執行（從快取讀取）\n",
    "print(\"\\n第二次查詢（從快取讀取）：\")\n",
    "start = time.time()\n",
    "complex_df.show()\n",
    "print(f\"執行時間: {time.time() - start:.4f} 秒（應該更快）\")\n",
    "# 釋放快取\n",
    "complex_df.unpersist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 第六章：複雜資料轉換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📚 第六章：複雜資料轉換\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎈 概念解釋：\n",
    "PIVOT 將行資料轉換為列，常用於建立交叉表。\n",
    "\n",
    "🎯 應用場景：\n",
    "銷售報表、資料透視表、趨勢分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📌 範例 14: PIVOT - 建立部門薪資透視表\")\n",
    "# 使用 PIVOT 建立部門-統計指標透視表\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_result = spark.sql(\"\"\"\n",
    "    SELECT * FROM (\n",
    "        SELECT `部門`, `薪資`\n",
    "        FROM employees\n",
    "    )\n",
    "    PIVOT (\n",
    "        COUNT(*) as `人數`,\n",
    "        AVG(`薪資`) as `平均薪資`\n",
    "        FOR `部門` IN ('工程部', '行銷部', '人資部')\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n部門薪資透視表：\")\n",
    "pivot_result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎈 概念解釋：\n",
    "UNPIVOT 將列資料轉換為行，是 PIVOT 的反向操作。\n",
    "\n",
    "🎯 應用場景：\n",
    "將寬表轉換為長表、資料正規化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96586251",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📌 範例 15: UNPIVOT - 列轉行\")\n",
    "# 建立寬表格式的季度銷售資料\n",
    "quarterly_sales = spark.createDataFrame([\n",
    "    (\"產品A\", 100, 120, 115, 130),\n",
    "    (\"產品B\", 80, 90, 95, 100),\n",
    "], [\"產品\", \"Q1\", \"Q2\", \"Q3\", \"Q4\"])\n",
    "quarterly_sales.createOrReplaceTempView(\"quarterly_sales\")\n",
    "# 使用 UNPIVOT（Spark 3.4+）或使用 stack 函數\n",
    "unpivot_result = quarterly_sales.selectExpr(\n",
    "    \"`產品`\",\n",
    "    \"stack(4, 'Q1', `Q1`, 'Q2', `Q2`, 'Q3', `Q3`, 'Q4', `Q4`) as (`季度`, `銷售額`)\"\n",
    ")\n",
    "print(\"\\n轉換為長表格式：\")\n",
    "unpivot_result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎈 概念解釋：\n",
    "PySpark 支援複雜的資料型態如陣列、結構體、地圖等。\n",
    "\n",
    "🎯 應用場景：\n",
    "處理 JSON 資料、巢狀結構、多值欄位\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📌 範例 16: 陣列操作\")\n",
    "# 建立包含陣列的資料\n",
    "students_data = [\n",
    "    (1, \"Alice\", [\"數學\", \"物理\", \"化學\"]),\n",
    "    (2, \"Bob\", [\"英文\", \"歷史\"]),\n",
    "    (3, \"Charlie\", [\"數學\", \"英文\", \"體育\"]),\n",
    "]\n",
    "students_df = spark.createDataFrame(\n",
    "    students_data,\n",
    "    [\"學號\", \"姓名\", \"選修課程\"]\n",
    ")\n",
    "print(\"\\n原始資料（包含陣列）：\")\n",
    "students_df.show(truncate=False)\n",
    "# 展開陣列\n",
    "exploded = students_df.select(\n",
    "    \"學號\",\n",
    "    \"姓名\",\n",
    "    explode(\"選修課程\").alias(\"課程\")\n",
    ")\n",
    "print(\"\\n展開陣列後：\")\n",
    "exploded.show()\n",
    "# 陣列相關函數\n",
    "array_operations = students_df.select(\n",
    "    \"姓名\",\n",
    "    \"選修課程\",\n",
    "    size(\"選修課程\").alias(\"課程數\"),\n",
    "    array_contains(\"選修課程\", \"數學\").alias(\"是否選修數學\")\n",
    ")\n",
    "print(\"\\n陣列操作：\")\n",
    "array_operations.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 第七章：資料品質與清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📚 第七章：資料品質與清理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎈 概念解釋：\n",
    "NULL 值處理是資料清理的重要環節。\n",
    "\n",
    "🎯 常用方法：\n",
    "fillna, dropna, coalesce, nvl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📌 範例 17: NULL 值處理\")\n",
    "# 建立包含 NULL 的資料\n",
    "data_with_null = [\n",
    "    (1, \"Alice\", 75000, \"工程部\"),\n",
    "    (2, \"Bob\", None, \"行銷部\"),\n",
    "    (3, \"Charlie\", 68000, None),\n",
    "    (4, None, 65000, \"人資部\"),\n",
    "]\n",
    "df_null = spark.createDataFrame(\n",
    "    data_with_null,\n",
    "    [\"員工編號\", \"姓名\", \"薪資\", \"部門\"]\n",
    ")\n",
    "print(\"\\n原始資料（包含 NULL）：\")\n",
    "df_null.show()\n",
    "# 方法 1: 填充預設值\n",
    "filled = df_null.fillna({\n",
    "    \"姓名\": \"未知\",\n",
    "    \"薪資\": 60000,\n",
    "    \"部門\": \"待分配\"\n",
    "})\n",
    "print(\"\\n填充預設值後：\")\n",
    "filled.show()\n",
    "# 方法 2: 刪除包含 NULL 的列\n",
    "dropped = df_null.dropna()\n",
    "print(\"\\n刪除 NULL 列後：\")\n",
    "dropped.show()\n",
    "# 方法 3: 使用 COALESCE\n",
    "coalesced = df_null.select(\n",
    "    \"員工編號\",\n",
    "    coalesce(col(\"姓名\"), lit(\"未知\")).alias(\"姓名\"),\n",
    "    coalesce(col(\"薪資\"), lit(60000)).alias(\"薪資\"),\n",
    "    coalesce(col(\"部門\"), lit(\"待分配\")).alias(\"部門\")\n",
    ")\n",
    "print(\"\\n使用 COALESCE 處理：\")\n",
    "coalesced.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎈 概念解釋：\n",
    "找出並移除重複的記錄。\n",
    "\n",
    "🎯 方法：\n",
    "distinct, dropDuplicates, 視窗函數\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📌 範例 18: 資料去重\")\n",
    "# 建立包含重複資料\n",
    "duplicate_data = [\n",
    "    (1, \"Alice\", \"工程部\"),\n",
    "    (2, \"Bob\", \"行銷部\"),\n",
    "    (1, \"Alice\", \"工程部\"),  # 完全重複\n",
    "    (3, \"Alice\", \"人資部\"),  # 姓名重複\n",
    "]\n",
    "df_dup = spark.createDataFrame(\n",
    "    duplicate_data,\n",
    "    [\"員工編號\", \"姓名\", \"部門\"]\n",
    ")\n",
    "print(\"\\n原始資料（包含重複）：\")\n",
    "df_dup.show()\n",
    "# 方法 1: 完全去重\n",
    "dedup_all = df_dup.distinct()\n",
    "print(\"\\n完全去重：\")\n",
    "dedup_all.show()\n",
    "# 方法 2: 按特定欄位去重（保留第一筆）\n",
    "dedup_by_name = df_dup.dropDuplicates([\"姓名\"])\n",
    "print(\"\\n按姓名去重：\")\n",
    "dedup_by_name.show()\n",
    "# 方法 3: 使用視窗函數保留特定規則的記錄\n",
    "windowSpec = Window.partitionBy(\"姓名\").orderBy(\"員工編號\")\n",
    "dedup_window = df_dup.withColumn(\"rn\", row_number().over(windowSpec)) \\\n",
    "                     .filter(col(\"rn\") == 1) \\\n",
    "                     .drop(\"rn\")\n",
    "print(\"\\n使用視窗函數去重（保留編號最小的）：\")\n",
    "dedup_window.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 第八章：進階分析函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📚 第八章：進階分析函數\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎈 概念解釋：\n",
    "NTILE 將資料分成 N 個大致相等的組。\n",
    "\n",
    "🎯 應用場景：\n",
    "客戶分層、ABC 分析、分位數分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📌 範例 19: NTILE - 薪資四分位數\")\n",
    "windowSpec = Window.orderBy(\"薪資\")\n",
    "ntile_result = employees_df.withColumn(\n",
    "    \"薪資分組\",\n",
    "    ntile(4).over(windowSpec)\n",
    ").withColumn(\n",
    "    \"分組說明\",\n",
    "    when(col(\"薪資分組\") == 1, \"低薪組\")\n",
    "    .when(col(\"薪資分組\") == 2, \"中低薪組\")\n",
    "    .when(col(\"薪資分組\") == 3, \"中高薪組\")\n",
    "    .otherwise(\"高薪組\")\n",
    ")\n",
    "print(\"\\n薪資四分位數分組：\")\n",
    "ntile_result.orderBy(\"薪資\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎈 概念解釋：\n",
    "PERCENT_RANK 計算值在資料集中的相對位置（0 到 1 之間）。\n",
    "\n",
    "🎯 應用場景：\n",
    "績效評估、成績分析、相對排名\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📌 範例 20: PERCENT_RANK - 薪資百分位\")\n",
    "windowSpec = Window.orderBy(\"薪資\")\n",
    "percent_result = employees_df.withColumn(\n",
    "    \"薪資百分位\",\n",
    "    round(percent_rank().over(windowSpec) * 100, 2)\n",
    ").withColumn(\n",
    "    \"說明\",\n",
    "    concat(\n",
    "        lit(\"薪資超過 \"),\n",
    "        round(percent_rank().over(windowSpec) * 100, 0).cast(\"int\"),\n",
    "        lit(\"% 的員工\")\n",
    "    )\n",
    ")\n",
    "print(\"\\n薪資百分位排名：\")\n",
    "percent_result.orderBy(\"薪資\").select(\"姓名\", \"薪資\", \"薪資百分位\", \"說明\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 課程總結"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎓 課程總結\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎊 恭喜！你已經完成了 SQL 進階課程！\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📝 進階技能總結：\n",
    "\n",
    "1️⃣ 視窗函數：\n",
    "   - ROW_NUMBER, RANK, DENSE_RANK：排名與編號\n",
    "   - LEAD, LAG：訪問前後列資料\n",
    "   - SUM/AVG OVER：累計計算與移動平均\n",
    "   - NTILE, PERCENT_RANK：分組與百分位\n",
    "\n",
    "2️⃣ 通用表表達式 (CTE)：\n",
    "   - WITH 子句：提高查詢可讀性\n",
    "   - 多層 CTE：分解複雜邏輯\n",
    "   - 遞迴 CTE：處理層級關係\n",
    "\n",
    "3️⃣ 子查詢優化：\n",
    "   - 避免相關子查詢\n",
    "   - 使用 JOIN 替代\n",
    "   - 使用視窗函數優化\n",
    "\n",
    "4️⃣ 進階 JOIN：\n",
    "   - CROSS JOIN：笛卡爾積\n",
    "   - SELF JOIN：自我連接\n",
    "   - SEMI/ANTI JOIN：高效過濾\n",
    "\n",
    "5️⃣ 效能優化：\n",
    "   - Broadcast Join：廣播小表\n",
    "   - 分區與分桶：資料組織\n",
    "   - 快取與持久化：避免重複計算\n",
    "\n",
    "6️⃣ 複雜資料轉換：\n",
    "   - PIVOT/UNPIVOT：行列轉換\n",
    "   - 陣列與結構：處理複雜型態\n",
    "   - 資料展開與聚合\n",
    "\n",
    "7️⃣ 資料品質：\n",
    "   - NULL 值處理\n",
    "   - 資料去重\n",
    "   - 資料驗證\n",
    "\n",
    "💡 進階學習建議：\n",
    "- 理解執行計畫，學會效能分析\n",
    "- 掌握資料分區策略\n",
    "- 熟悉 Spark UI 的使用\n",
    "- 實踐 Delta Lake 進階功能\n",
    "- 學習 Spark Streaming\n",
    "\n",
    "🚀 下一步：\n",
    "- 深入研究 Spark 內部機制\n",
    "- 學習分散式系統原理\n",
    "- 實踐大規模資料處理專案\n",
    "- 探索機器學習與 Spark MLlib\n",
    "\n",
    "記住：進階技能需要大量實踐，多做專案、多解決實際問題才能真正掌握！\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"📚 進階教學文件結束 - 繼續精進你的技能！🚀\")\n",
    "print(\"=\" * 60)\n",
    "# 關閉 Spark Session\n",
    "# spark.stop()  # 取消註解以關閉 Spark\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fju",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
