{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark SQL é€²éšæ•™å­¸ ğŸš€\n",
    "\n",
    "é€™ä»½æ•™å­¸å°ˆç‚ºå·²ç¶“æŒæ¡ SQL åŸºç¤çš„ä½ è¨­è¨ˆï¼\n",
    "æˆ‘å€‘å°‡æ·±å…¥æ¢è¨é€²éšçš„ SQL æŠ€å·§ï¼ŒåŒ…æ‹¬è¦–çª—å‡½æ•¸ã€CTEã€æ•ˆèƒ½å„ªåŒ–ç­‰ä¸»é¡Œã€‚\n",
    "è®“ä½ çš„ SQL æŠ€èƒ½æ›´ä¸Šä¸€å±¤æ¨“ï¼âœ¨\n",
    "\n",
    "ä½œè€…ï¼šQChoice AI æ•™å­¸åœ˜éšŠ  \n",
    "æ—¥æœŸï¼š2025-01-15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ç’°å¢ƒè¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "# å‰µå»º Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SQLé€²éšæ•™å­¸\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"ğŸš€ æ­¡è¿ä¾†åˆ° SQL é€²éšèª²ç¨‹ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ ç’°å¢ƒè¨­å®š - æº–å‚™é€²éšé–‹ç™¼ç’°å¢ƒ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "# å‰µå»º Spark Session - é…ç½®é€²éšæ•ˆèƒ½åƒæ•¸\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SQLé€²éšæ•™å­¸\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "print(\"ğŸš€ æ­¡è¿ä¾†åˆ° SQL é€²éšèª²ç¨‹ï¼\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ç¬¬ä¸€ç« ï¼šè¦–çª—å‡½æ•¸ (Window Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š ç¬¬ä¸€ç« ï¼šè¦–çª—å‡½æ•¸ (Window Functions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "ROW_NUMBER() ç‚ºåˆ†å€å…§çš„æ¯ä¸€åˆ—åˆ†é…ä¸€å€‹å”¯ä¸€çš„åºè™Ÿã€‚\n",
    "å¸¸ç”¨æ–¼æ’åã€å»é‡ã€åˆ†é ç­‰å ´æ™¯ã€‚\n",
    "\n",
    "ğŸ¯ AI Prompt ç¯„ä¾‹ï¼š\n",
    "è«‹å¹«æˆ‘å¯«ä¸€å€‹ PySpark ç¨‹å¼ï¼Œç‚ºæ¯å€‹éƒ¨é–€çš„å“¡å·¥æŒ‰è–ªè³‡æ’åºä¸¦åˆ†é…åºè™Ÿã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Œ ç¯„ä¾‹ 1: ROW_NUMBER - æ’åèˆ‡ç·¨è™Ÿ\")\n",
    "# å»ºç«‹å“¡å·¥è³‡æ–™\n",
    "employees_data = [\n",
    "    (1, \"Alice\", \"å·¥ç¨‹éƒ¨\", 75000),\n",
    "    (2, \"Bob\", \"å·¥ç¨‹éƒ¨\", 82000),\n",
    "    (3, \"Charlie\", \"å·¥ç¨‹éƒ¨\", 68000),\n",
    "    (4, \"David\", \"è¡ŒéŠ·éƒ¨\", 65000),\n",
    "    (5, \"Eve\", \"è¡ŒéŠ·éƒ¨\", 72000),\n",
    "    (6, \"Frank\", \"è¡ŒéŠ·éƒ¨\", 70000),\n",
    "    (7, \"Grace\", \"äººè³‡éƒ¨\", 60000),\n",
    "    (8, \"Henry\", \"äººè³‡éƒ¨\", 63000)\n",
    "]\n",
    "employees_df = spark.createDataFrame(\n",
    "    employees_data, \n",
    "    [\"å“¡å·¥ç·¨è™Ÿ\", \"å§“å\", \"éƒ¨é–€\", \"è–ªè³‡\"]\n",
    ")\n",
    "# å®šç¾©è¦–çª—è¦æ ¼ï¼šæŒ‰éƒ¨é–€åˆ†çµ„ï¼ŒæŒ‰è–ªè³‡é™åºæ’åˆ—\n",
    "windowSpec = Window.partitionBy(\"éƒ¨é–€\").orderBy(desc(\"è–ªè³‡\"))\n",
    "# ä½¿ç”¨ ROW_NUMBER\n",
    "result = employees_df.withColumn(\"éƒ¨é–€æ’å\", row_number().over(windowSpec))\n",
    "print(\"\\nä½¿ç”¨ ROW_NUMBER é€²è¡Œéƒ¨é–€å…§æ’åï¼š\")\n",
    "result.orderBy(\"éƒ¨é–€\", \"éƒ¨é–€æ’å\").show()\n",
    "# SQL å¯«æ³•\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        `å“¡å·¥ç·¨è™Ÿ`,\n",
    "        `å§“å`,\n",
    "        `éƒ¨é–€`,\n",
    "        `è–ªè³‡`,\n",
    "        ROW_NUMBER() OVER (PARTITION BY `éƒ¨é–€` ORDER BY `è–ªè³‡` DESC) as `éƒ¨é–€æ’å`\n",
    "    FROM employees\n",
    "    ORDER BY `éƒ¨é–€`, `éƒ¨é–€æ’å`\n",
    "\"\"\")\n",
    "print(\"\\nSQL å¯«æ³•çµæœï¼š\")\n",
    "sql_result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "RANK() å’Œ DENSE_RANK() éƒ½ç”¨æ–¼æ’åï¼Œä½†è™•ç†ç›¸åŒå€¼çš„æ–¹å¼ä¸åŒï¼š\n",
    "- RANK(): ç›¸åŒå€¼å¾—åˆ°ç›¸åŒæ’åï¼Œä¸‹ä¸€å€‹æ’åæœƒè·³è™Ÿ\n",
    "- DENSE_RANK(): ç›¸åŒå€¼å¾—åˆ°ç›¸åŒæ’åï¼Œä¸‹ä¸€å€‹æ’åé€£çºŒ\n",
    "\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š\n",
    "æˆç¸¾æ’åã€éŠ·å”®æ¥­ç¸¾æ’åç­‰éœ€è¦è™•ç†ä¸¦åˆ—æƒ…æ³çš„å ´æ™¯\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Œ ç¯„ä¾‹ 2: RANK vs DENSE_RANK - ä¸¦åˆ—æ’åè™•ç†\")\n",
    "# å»ºç«‹è€ƒè©¦æˆç¸¾è³‡æ–™\n",
    "scores_data = [\n",
    "    (1, \"Alice\", 95),\n",
    "    (2, \"Bob\", 95),\n",
    "    (3, \"Charlie\", 90),\n",
    "    (4, \"David\", 90),\n",
    "    (5, \"Eve\", 85),\n",
    "    (6, \"Frank\", 80),\n",
    "]\n",
    "scores_df = spark.createDataFrame(scores_data, [\"å­¸è™Ÿ\", \"å§“å\", \"åˆ†æ•¸\"])\n",
    "# å®šç¾©è¦–çª—è¦æ ¼\n",
    "windowSpec = Window.orderBy(desc(\"åˆ†æ•¸\"))\n",
    "result = scores_df.withColumn(\"RANK\", rank().over(windowSpec)) \\\n",
    "                  .withColumn(\"DENSE_RANK\", dense_rank().over(windowSpec)) \\\n",
    "                  .withColumn(\"ROW_NUMBER\", row_number().over(windowSpec))\n",
    "print(\"\\næ¯”è¼ƒä¸‰ç¨®æ’åå‡½æ•¸ï¼š\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ’¡ èªªæ˜ï¼š\n",
    "- Alice å’Œ Bob éƒ½æ˜¯ 95 åˆ†ï¼ŒRANK å’Œ DENSE_RANK éƒ½æ˜¯ 1\n",
    "- Charlie å’Œ David éƒ½æ˜¯ 90 åˆ†ï¼š\n",
    "  * RANK æ˜¯ 3ï¼ˆå› ç‚ºå‰é¢æœ‰ 2 å€‹äººï¼‰\n",
    "  * DENSE_RANK æ˜¯ 2ï¼ˆé€£çºŒç·¨è™Ÿï¼‰\n",
    "- ROW_NUMBER ç‚ºæ¯å€‹äººåˆ†é…å”¯ä¸€ç·¨è™Ÿ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "LEAD() å’Œ LAG() å…è¨±ä½ è¨ªå•ç•¶å‰åˆ—ä¹‹å‰æˆ–ä¹‹å¾Œçš„è³‡æ–™ï¼š\n",
    "- LAG(): è¨ªå•å‰é¢çš„åˆ—\n",
    "- LEAD(): è¨ªå•å¾Œé¢çš„åˆ—\n",
    "\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š\n",
    "è¨ˆç®—å¢é•·ç‡ã€åŒæ¯”ç’°æ¯”åˆ†æã€æ™‚é–“åºåˆ—åˆ†æ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Œ ç¯„ä¾‹ 3: LEAD & LAG - æ™‚é–“åºåˆ—åˆ†æ\")\n",
    "# å»ºç«‹æœˆåº¦éŠ·å”®è³‡æ–™\n",
    "sales_data = [\n",
    "    (\"2024-01\", 100000),\n",
    "    (\"2024-02\", 120000),\n",
    "    (\"2024-03\", 115000),\n",
    "    (\"2024-04\", 130000),\n",
    "    (\"2024-05\", 135000),\n",
    "    (\"2024-06\", 140000),\n",
    "]\n",
    "sales_df = spark.createDataFrame(sales_data, [\"æœˆä»½\", \"éŠ·å”®é¡\"])\n",
    "# å®šç¾©è¦–çª—è¦æ ¼\n",
    "windowSpec = Window.orderBy(\"æœˆä»½\")\n",
    "result = sales_df.withColumn(\"ä¸ŠæœˆéŠ·å”®é¡\", lag(\"éŠ·å”®é¡\", 1).over(windowSpec)) \\\n",
    "                 .withColumn(\"ä¸‹æœˆéŠ·å”®é¡\", lead(\"éŠ·å”®é¡\", 1).over(windowSpec)) \\\n",
    "                 .withColumn(\"æœˆå¢é•·é¡\", col(\"éŠ·å”®é¡\") - lag(\"éŠ·å”®é¡\", 1).over(windowSpec)) \\\n",
    "                 .withColumn(\"æœˆå¢é•·ç‡%\", \n",
    "                            round((col(\"éŠ·å”®é¡\") - lag(\"éŠ·å”®é¡\", 1).over(windowSpec)) / \n",
    "                                  lag(\"éŠ·å”®é¡\", 1).over(windowSpec) * 100, 2))\n",
    "print(\"\\néŠ·å”®é¡æœˆåº¦åˆ†æï¼š\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "è¦–çª—å‡½æ•¸é…åˆèšåˆå‡½æ•¸å¯ä»¥é€²è¡Œç´¯è¨ˆè¨ˆç®—ï¼Œè€Œä¸æœƒæŠ˜ç–Šè³‡æ–™åˆ—ã€‚\n",
    "\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š\n",
    "ç´¯è¨ˆéŠ·å”®é¡ã€ç§»å‹•å¹³å‡ã€æ»¾å‹•çµ±è¨ˆ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Œ ç¯„ä¾‹ 4: ç´¯è¨ˆè¨ˆç®—èˆ‡ç§»å‹•å¹³å‡\")\n",
    "# ç¹¼çºŒä½¿ç”¨éŠ·å”®è³‡æ–™\n",
    "windowSpec = Window.orderBy(\"æœˆä»½\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "movingAvgSpec = Window.orderBy(\"æœˆä»½\").rowsBetween(-2, 0)  # 3å€‹æœˆç§»å‹•å¹³å‡\n",
    "result = sales_df.withColumn(\"ç´¯è¨ˆéŠ·å”®é¡\", sum(\"éŠ·å”®é¡\").over(windowSpec)) \\\n",
    "                 .withColumn(\"3æœˆç§»å‹•å¹³å‡\", round(avg(\"éŠ·å”®é¡\").over(movingAvgSpec), 2))\n",
    "print(\"\\nç´¯è¨ˆéŠ·å”®é¡èˆ‡ç§»å‹•å¹³å‡ï¼š\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’ª ç·´ç¿’é¡Œ 1\n",
    "\n",
    "**é¡Œç›®ï¼š**\n",
    "\n",
    "è«‹å»ºç«‹ä¸€å€‹éŠ·å”®æ¥­ç¸¾åˆ†æç³»çµ±ï¼š\n",
    "1. å»ºç«‹éŠ·å”®å“¡æœˆåº¦éŠ·å”®è³‡æ–™\n",
    "2. ä½¿ç”¨ ROW_NUMBER ç‚ºæ¯å€‹å€åŸŸçš„éŠ·å”®å“¡æ’å\n",
    "3. ä½¿ç”¨ LAG è¨ˆç®—æ¯æœˆç’°æ¯”å¢é•·ç‡\n",
    "4. è¨ˆç®—æ¯å€‹éŠ·å”®å“¡çš„ç´¯è¨ˆæ¥­ç¸¾\n",
    "5. è¨ˆç®— 3 å€‹æœˆç§»å‹•å¹³å‡\n",
    "\n",
    "**æç¤ºï¼šç¶œåˆé‹ç”¨ Window Functionsï¼šrow_numberã€lagã€sumã€avg**\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ“ é»æ“ŠæŸ¥çœ‹åƒè€ƒç­”æ¡ˆ</summary>\n",
    "\n",
    "```python\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, lag, col, round, sum, avg\n",
    "\n",
    "# 1. å»ºç«‹éŠ·å”®è³‡æ–™\n",
    "sales_data = [\n",
    "    (\"å¼µä¸‰\", \"åŒ—å€\", \"2024-01\", 150000),\n",
    "    (\"å¼µä¸‰\", \"åŒ—å€\", \"2024-02\", 180000),\n",
    "    (\"å¼µä¸‰\", \"åŒ—å€\", \"2024-03\", 165000),\n",
    "    (\"æå››\", \"åŒ—å€\", \"2024-01\", 140000),\n",
    "    (\"æå››\", \"åŒ—å€\", \"2024-02\", 155000),\n",
    "    (\"æå››\", \"åŒ—å€\", \"2024-03\", 170000),\n",
    "    (\"ç‹äº”\", \"å—å€\", \"2024-01\", 200000),\n",
    "    (\"ç‹äº”\", \"å—å€\", \"2024-02\", 220000),\n",
    "    (\"ç‹äº”\", \"å—å€\", \"2024-03\", 210000)\n",
    "]\n",
    "\n",
    "sales_df = spark.createDataFrame(\n",
    "    sales_data,\n",
    "    [\"éŠ·å”®å“¡\", \"å€åŸŸ\", \"æœˆä»½\", \"éŠ·å”®é¡\"]\n",
    ")\n",
    "\n",
    "print(\"=== 1. åŸå§‹éŠ·å”®è³‡æ–™ ===\")\n",
    "sales_df.orderBy(\"å€åŸŸ\", \"éŠ·å”®å“¡\", \"æœˆä»½\").show()\n",
    "\n",
    "# 2. å€åŸŸæ’åï¼ˆæŒ‰ç¸½æ¥­ç¸¾ï¼‰\n",
    "print(\"\\n=== 2. å„å€åŸŸéŠ·å”®å“¡æ’å ===\")\n",
    "window_rank = Window.partitionBy(\"å€åŸŸ\").orderBy(col(\"éŠ·å”®é¡\").desc())\n",
    "ranked_df = sales_df.withColumn(\"å€åŸŸæ’å\", row_number().over(window_rank))\n",
    "ranked_df.orderBy(\"å€åŸŸ\", \"å€åŸŸæ’å\", \"æœˆä»½\").show()\n",
    "\n",
    "# 3. ç’°æ¯”å¢é•·ç‡\n",
    "print(\"\\n=== 3. æœˆåº¦ç’°æ¯”å¢é•·ç‡ ===\")\n",
    "window_lag = Window.partitionBy(\"éŠ·å”®å“¡\").orderBy(\"æœˆä»½\")\n",
    "growth_df = sales_df \\\n",
    "    .withColumn(\"ä¸ŠæœˆéŠ·å”®é¡\", lag(\"éŠ·å”®é¡\", 1).over(window_lag)) \\\n",
    "    .withColumn(\n",
    "        \"ç’°æ¯”å¢é•·ç‡%\",\n",
    "        round((col(\"éŠ·å”®é¡\") - col(\"ä¸ŠæœˆéŠ·å”®é¡\")) / col(\"ä¸ŠæœˆéŠ·å”®é¡\") * 100, 2)\n",
    "    )\n",
    "growth_df.orderBy(\"éŠ·å”®å“¡\", \"æœˆä»½\").show()\n",
    "\n",
    "# 4. ç´¯è¨ˆæ¥­ç¸¾\n",
    "print(\"\\n=== 4. ç´¯è¨ˆæ¥­ç¸¾ ===\")\n",
    "window_cumsum = Window.partitionBy(\"éŠ·å”®å“¡\").orderBy(\"æœˆä»½\") \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "cumsum_df = sales_df \\\n",
    "    .withColumn(\"ç´¯è¨ˆéŠ·å”®é¡\", sum(\"éŠ·å”®é¡\").over(window_cumsum))\n",
    "cumsum_df.orderBy(\"éŠ·å”®å“¡\", \"æœˆä»½\").show()\n",
    "\n",
    "# 5. 3å€‹æœˆç§»å‹•å¹³å‡\n",
    "print(\"\\n=== 5. 3å€‹æœˆç§»å‹•å¹³å‡ ===\")\n",
    "window_ma = Window.partitionBy(\"éŠ·å”®å“¡\").orderBy(\"æœˆä»½\") \\\n",
    "    .rowsBetween(-2, Window.currentRow)\n",
    "ma_df = sales_df \\\n",
    "    .withColumn(\"3æœˆç§»å‹•å¹³å‡\", round(avg(\"éŠ·å”®é¡\").over(window_ma), 0))\n",
    "ma_df.orderBy(\"éŠ·å”®å“¡\", \"æœˆä»½\").show()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ç¬¬äºŒç« ï¼šé€šç”¨è¡¨è¡¨é”å¼ (Common Table Expressions - CTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š ç¬¬äºŒç« ï¼šé€šç”¨è¡¨è¡¨é”å¼ (Common Table Expressions - CTE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "CTE (WITH å­å¥) è®“ä½ å»ºç«‹è‡¨æ™‚çš„å‘½åçµæœé›†ï¼Œæé«˜æŸ¥è©¢çš„å¯è®€æ€§å’Œå¯ç¶­è­·æ€§ã€‚\n",
    "å¯ä»¥æŠŠè¤‡é›œæŸ¥è©¢åˆ†è§£ç‚ºå¤šå€‹æ­¥é©Ÿã€‚\n",
    "\n",
    "ğŸ¯ å„ªå‹¢ï¼š\n",
    "1. æé«˜å¯è®€æ€§\n",
    "2. å¯é‡è¤‡ä½¿ç”¨\n",
    "3. ä¾¿æ–¼é™¤éŒ¯\n",
    "4. æ”¯æ´éè¿´æŸ¥è©¢\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Œ ç¯„ä¾‹ 5: CTE - ç°¡åŒ–è¤‡é›œæŸ¥è©¢\")\n",
    "# å»ºç«‹è¨‚å–®è³‡æ–™\n",
    "orders_data = [\n",
    "    (1, \"Alice\", \"2024-01-15\", 1500),\n",
    "    (2, \"Bob\", \"2024-01-20\", 2300),\n",
    "    (3, \"Alice\", \"2024-02-10\", 1800),\n",
    "    (4, \"Charlie\", \"2024-02-15\", 3200),\n",
    "    (5, \"Bob\", \"2024-03-05\", 2100),\n",
    "    (6, \"Alice\", \"2024-03-20\", 2500),\n",
    "]\n",
    "orders_df = spark.createDataFrame(\n",
    "    orders_data,\n",
    "    [\"è¨‚å–®ç·¨è™Ÿ\", \"å®¢æˆ¶\", \"è¨‚å–®æ—¥æœŸ\", \"é‡‘é¡\"]\n",
    ")\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "# ä½¿ç”¨ CTE è¨ˆç®—å®¢æˆ¶ç¸½æ¶ˆè²»å’Œå¹³å‡æ¶ˆè²»\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_with_cte = spark.sql(\"\"\"\n",
    "    WITH customer_stats AS (\n",
    "        SELECT \n",
    "            `å®¢æˆ¶`,\n",
    "            COUNT(*) as `è¨‚å–®æ•¸`,\n",
    "            SUM(`é‡‘é¡`) as `ç¸½æ¶ˆè²»`,\n",
    "            AVG(`é‡‘é¡`) as `å¹³å‡æ¶ˆè²»`\n",
    "        FROM orders\n",
    "        GROUP BY `å®¢æˆ¶`\n",
    "    ),\n",
    "    high_value_customers AS (\n",
    "        SELECT \n",
    "            `å®¢æˆ¶`,\n",
    "            `è¨‚å–®æ•¸`,\n",
    "            `ç¸½æ¶ˆè²»`,\n",
    "            ROUND(`å¹³å‡æ¶ˆè²»`, 2) as `å¹³å‡æ¶ˆè²»`\n",
    "        FROM customer_stats\n",
    "        WHERE `ç¸½æ¶ˆè²»` > 5000\n",
    "    )\n",
    "    SELECT * FROM high_value_customers\n",
    "    ORDER BY `ç¸½æ¶ˆè²»` DESC\n",
    "\"\"\")\n",
    "print(\"\\né«˜åƒ¹å€¼å®¢æˆ¶åˆ†æï¼ˆä½¿ç”¨ CTEï¼‰ï¼š\")\n",
    "sql_with_cte.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "å¤šå±¤ CTE å¯ä»¥å°‡è¤‡é›œçš„æ¥­å‹™é‚è¼¯åˆ†è§£ç‚ºå¤šå€‹æ¸…æ™°çš„æ­¥é©Ÿã€‚\n",
    "\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š\n",
    "å¤šæ­¥é©Ÿçš„è³‡æ–™è½‰æ›ã€è¤‡é›œçš„æ¥­å‹™æŒ‡æ¨™è¨ˆç®—\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Œ ç¯„ä¾‹ 6: å¤šå±¤ CTE - å®¢æˆ¶åˆ†ç´šåˆ†æ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_multi_cte = spark.sql(\"\"\"\n",
    "    WITH monthly_sales AS (\n",
    "        -- ç¬¬ä¸€å±¤ï¼šè¨ˆç®—æ¯å€‹å®¢æˆ¶çš„æœˆåº¦éŠ·å”®\n",
    "        SELECT \n",
    "            `å®¢æˆ¶`,\n",
    "            DATE_FORMAT(TO_DATE(`è¨‚å–®æ—¥æœŸ`), 'yyyy-MM') as `æœˆä»½`,\n",
    "            SUM(`é‡‘é¡`) as `æœˆéŠ·å”®é¡`\n",
    "        FROM orders\n",
    "        GROUP BY `å®¢æˆ¶`, DATE_FORMAT(TO_DATE(`è¨‚å–®æ—¥æœŸ`), 'yyyy-MM')\n",
    "    ),\n",
    "    customer_summary AS (\n",
    "        -- ç¬¬äºŒå±¤ï¼šåŒ¯ç¸½å®¢æˆ¶çµ±è¨ˆ\n",
    "        SELECT \n",
    "            `å®¢æˆ¶`,\n",
    "            COUNT(DISTINCT `æœˆä»½`) as `æ´»èºæœˆæ•¸`,\n",
    "            SUM(`æœˆéŠ·å”®é¡`) as `ç¸½éŠ·å”®é¡`,\n",
    "            AVG(`æœˆéŠ·å”®é¡`) as `å¹³å‡æœˆéŠ·å”®é¡`\n",
    "        FROM monthly_sales\n",
    "        GROUP BY `å®¢æˆ¶`\n",
    "    ),\n",
    "    customer_level AS (\n",
    "        -- ç¬¬ä¸‰å±¤ï¼šå®¢æˆ¶åˆ†ç´š\n",
    "        SELECT \n",
    "            `å®¢æˆ¶`,\n",
    "            `æ´»èºæœˆæ•¸`,\n",
    "            ROUND(`ç¸½éŠ·å”®é¡`, 2) as `ç¸½éŠ·å”®é¡`,\n",
    "            ROUND(`å¹³å‡æœˆéŠ·å”®é¡`, 2) as `å¹³å‡æœˆéŠ·å”®é¡`,\n",
    "            CASE \n",
    "                WHEN `ç¸½éŠ·å”®é¡` >= 6000 THEN 'ç™½é‡‘å®¢æˆ¶'\n",
    "                WHEN `ç¸½éŠ·å”®é¡` >= 4000 THEN 'é‡‘ç‰Œå®¢æˆ¶'\n",
    "                ELSE 'ä¸€èˆ¬å®¢æˆ¶'\n",
    "            END as `å®¢æˆ¶ç­‰ç´š`\n",
    "        FROM customer_summary\n",
    "    )\n",
    "    SELECT * FROM customer_level\n",
    "    ORDER BY `ç¸½éŠ·å”®é¡` DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nå®¢æˆ¶åˆ†ç´šçµæœï¼š\")\n",
    "sql_multi_cte.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’ª ç·´ç¿’é¡Œ 2\n",
    "\n",
    "**é¡Œç›®ï¼š**\n",
    "\n",
    "è«‹ä½¿ç”¨ CTE å»ºç«‹ä¸€å€‹å®Œæ•´çš„å®¢æˆ¶åƒ¹å€¼åˆ†æç³»çµ±ï¼š\n",
    "1. è¨ˆç®—æ¯å€‹å®¢æˆ¶çš„ RFM æŒ‡æ¨™ï¼ˆæœ€è¿‘è³¼è²·ã€è³¼è²·é »ç‡ã€è³¼è²·é‡‘é¡ï¼‰\n",
    "2. æ ¹æ“š RFM åˆ†æ•¸å°å®¢æˆ¶åˆ†ç´š\n",
    "3. åˆ†æå„ç´šåˆ¥å®¢æˆ¶çš„ç‰¹å¾µ\n",
    "4. çµ¦å‡ºç‡ŸéŠ·å»ºè­°\n",
    "\n",
    "**æç¤ºï¼šä½¿ç”¨å¤šå±¤ CTEï¼Œçµåˆ CASE WHEN é€²è¡Œåˆ†ç´š**\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ“ é»æ“ŠæŸ¥çœ‹åƒè€ƒç­”æ¡ˆ</summary>\n",
    "\n",
    "```python\n",
    "from datetime import date\n",
    "\n",
    "# å»ºç«‹è¨‚å–®è³‡æ–™\n",
    "orders_data = [\n",
    "    (1, \"Alice\", \"2024-01-05\", 1500),\n",
    "    (2, \"Alice\", \"2024-02-10\", 2000),\n",
    "    (3, \"Alice\", \"2024-03-15\", 1800),\n",
    "    (4, \"Bob\", \"2024-01-08\", 3000),\n",
    "    (5, \"Bob\", \"2024-01-20\", 2500),\n",
    "    (6, \"Charlie\", \"2024-02-01\", 5000),\n",
    "    (7, \"David\", \"2023-12-15\", 1000),\n",
    "    (8, \"Eve\", \"2024-03-10\", 4000),\n",
    "    (9, \"Eve\", \"2024-03-20\", 3500)\n",
    "]\n",
    "\n",
    "orders_df = spark.createDataFrame(\n",
    "    orders_data,\n",
    "    [\"è¨‚å–®ç·¨è™Ÿ\", \"å®¢æˆ¶\", \"è¨‚å–®æ—¥æœŸ\", \"é‡‘é¡\"]\n",
    ")\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "# ä½¿ç”¨å¤šå±¤ CTE é€²è¡Œ RFM åˆ†æ\n",
    "result = spark.sql(\"\"\"\n",
    "WITH rfm_base AS (\n",
    "    -- è¨ˆç®—åŸºç¤ RFM æŒ‡æ¨™\n",
    "    SELECT \n",
    "        å®¢æˆ¶,\n",
    "        DATEDIFF('2024-03-31', MAX(è¨‚å–®æ—¥æœŸ)) as æœ€è¿‘è³¼è²·å¤©æ•¸,\n",
    "        COUNT(*) as è³¼è²·é »ç‡,\n",
    "        SUM(é‡‘é¡) as è³¼è²·é‡‘é¡,\n",
    "        ROUND(AVG(é‡‘é¡), 0) as å¹³å‡è¨‚å–®é‡‘é¡\n",
    "    FROM orders\n",
    "    GROUP BY å®¢æˆ¶\n",
    "),\n",
    "rfm_score AS (\n",
    "    -- è¨ˆç®— RFM åˆ†æ•¸ï¼ˆ1-5åˆ†ï¼‰\n",
    "    SELECT \n",
    "        å®¢æˆ¶,\n",
    "        æœ€è¿‘è³¼è²·å¤©æ•¸,\n",
    "        è³¼è²·é »ç‡,\n",
    "        è³¼è²·é‡‘é¡,\n",
    "        å¹³å‡è¨‚å–®é‡‘é¡,\n",
    "        CASE \n",
    "            WHEN æœ€è¿‘è³¼è²·å¤©æ•¸ <= 15 THEN 5\n",
    "            WHEN æœ€è¿‘è³¼è²·å¤©æ•¸ <= 30 THEN 4\n",
    "            WHEN æœ€è¿‘è³¼è²·å¤©æ•¸ <= 60 THEN 3\n",
    "            WHEN æœ€è¿‘è³¼è²·å¤©æ•¸ <= 90 THEN 2\n",
    "            ELSE 1\n",
    "        END as Råˆ†æ•¸,\n",
    "        CASE \n",
    "            WHEN è³¼è²·é »ç‡ >= 3 THEN 5\n",
    "            WHEN è³¼è²·é »ç‡ >= 2 THEN 4\n",
    "            ELSE 3\n",
    "        END as Fåˆ†æ•¸,\n",
    "        CASE \n",
    "            WHEN è³¼è²·é‡‘é¡ >= 5000 THEN 5\n",
    "            WHEN è³¼è²·é‡‘é¡ >= 4000 THEN 4\n",
    "            WHEN è³¼è²·é‡‘é¡ >= 3000 THEN 3\n",
    "            ELSE 2\n",
    "        END as Måˆ†æ•¸\n",
    "    FROM rfm_base\n",
    "),\n",
    "customer_segment AS (\n",
    "    -- å®¢æˆ¶åˆ†ç´š\n",
    "    SELECT \n",
    "        å®¢æˆ¶,\n",
    "        æœ€è¿‘è³¼è²·å¤©æ•¸,\n",
    "        è³¼è²·é »ç‡,\n",
    "        è³¼è²·é‡‘é¡,\n",
    "        å¹³å‡è¨‚å–®é‡‘é¡,\n",
    "        Råˆ†æ•¸,\n",
    "        Fåˆ†æ•¸,\n",
    "        Måˆ†æ•¸,\n",
    "        (Råˆ†æ•¸ + Fåˆ†æ•¸ + Måˆ†æ•¸) as ç¸½åˆ†,\n",
    "        CASE \n",
    "            WHEN Råˆ†æ•¸ >= 4 AND Fåˆ†æ•¸ >= 4 AND Måˆ†æ•¸ >= 4 THEN 'é‡è¦åƒ¹å€¼å®¢æˆ¶'\n",
    "            WHEN Råˆ†æ•¸ >= 4 AND Fåˆ†æ•¸ < 4 THEN 'é‡è¦å–šå›å®¢æˆ¶'\n",
    "            WHEN Råˆ†æ•¸ < 4 AND Fåˆ†æ•¸ >= 4 THEN 'é‡è¦ä¿æŒå®¢æˆ¶'\n",
    "            WHEN Måˆ†æ•¸ >= 4 THEN 'é‡è¦ç™¼å±•å®¢æˆ¶'\n",
    "            ELSE 'ä¸€èˆ¬å®¢æˆ¶'\n",
    "        END as å®¢æˆ¶ç­‰ç´š\n",
    "    FROM rfm_score\n",
    ")\n",
    "SELECT \n",
    "    å®¢æˆ¶,\n",
    "    å®¢æˆ¶ç­‰ç´š,\n",
    "    æœ€è¿‘è³¼è²·å¤©æ•¸ || 'å¤©å‰' as æœ€å¾Œè³¼è²·,\n",
    "    è³¼è²·é »ç‡ || 'æ¬¡' as è³¼è²·æ¬¡æ•¸,\n",
    "    è³¼è²·é‡‘é¡,\n",
    "    å¹³å‡è¨‚å–®é‡‘é¡,\n",
    "    ç¸½åˆ†\n",
    "FROM customer_segment\n",
    "ORDER BY ç¸½åˆ† DESC, è³¼è²·é‡‘é¡ DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== RFM å®¢æˆ¶åƒ¹å€¼åˆ†æ ===\")\n",
    "result.show(truncate=False)\n",
    "\n",
    "# çµ±è¨ˆå„ç­‰ç´šå®¢æˆ¶åˆ†å¸ƒ\n",
    "print(\"\\n=== å®¢æˆ¶ç­‰ç´šåˆ†å¸ƒ ===\")\n",
    "segment_stats = spark.sql(\"\"\"\n",
    "WITH rfm_base AS (\n",
    "    SELECT \n",
    "        å®¢æˆ¶,\n",
    "        DATEDIFF('2024-03-31', MAX(è¨‚å–®æ—¥æœŸ)) as æœ€è¿‘è³¼è²·å¤©æ•¸,\n",
    "        COUNT(*) as è³¼è²·é »ç‡,\n",
    "        SUM(é‡‘é¡) as è³¼è²·é‡‘é¡\n",
    "    FROM orders\n",
    "    GROUP BY å®¢æˆ¶\n",
    "),\n",
    "rfm_score AS (\n",
    "    SELECT \n",
    "        å®¢æˆ¶,\n",
    "        CASE WHEN æœ€è¿‘è³¼è²·å¤©æ•¸ <= 15 THEN 5 WHEN æœ€è¿‘è³¼è²·å¤©æ•¸ <= 30 THEN 4 WHEN æœ€è¿‘è³¼è²·å¤©æ•¸ <= 60 THEN 3 WHEN æœ€è¿‘è³¼è²·å¤©æ•¸ <= 90 THEN 2 ELSE 1 END as Råˆ†æ•¸,\n",
    "        CASE WHEN è³¼è²·é »ç‡ >= 3 THEN 5 WHEN è³¼è²·é »ç‡ >= 2 THEN 4 ELSE 3 END as Fåˆ†æ•¸,\n",
    "        CASE WHEN è³¼è²·é‡‘é¡ >= 5000 THEN 5 WHEN è³¼è²·é‡‘é¡ >= 4000 THEN 4 WHEN è³¼è²·é‡‘é¡ >= 3000 THEN 3 ELSE 2 END as Måˆ†æ•¸\n",
    "    FROM rfm_base\n",
    "),\n",
    "customer_segment AS (\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN Råˆ†æ•¸ >= 4 AND Fåˆ†æ•¸ >= 4 AND Måˆ†æ•¸ >= 4 THEN 'é‡è¦åƒ¹å€¼å®¢æˆ¶'\n",
    "            WHEN Råˆ†æ•¸ >= 4 AND Fåˆ†æ•¸ < 4 THEN 'é‡è¦å–šå›å®¢æˆ¶'\n",
    "            WHEN Råˆ†æ•¸ < 4 AND Fåˆ†æ•¸ >= 4 THEN 'é‡è¦ä¿æŒå®¢æˆ¶'\n",
    "            WHEN Måˆ†æ•¸ >= 4 THEN 'é‡è¦ç™¼å±•å®¢æˆ¶'\n",
    "            ELSE 'ä¸€èˆ¬å®¢æˆ¶'\n",
    "        END as å®¢æˆ¶ç­‰ç´š\n",
    "    FROM rfm_score\n",
    ")\n",
    "SELECT \n",
    "    å®¢æˆ¶ç­‰ç´š,\n",
    "    COUNT(*) as å®¢æˆ¶æ•¸é‡\n",
    "FROM customer_segment\n",
    "GROUP BY å®¢æˆ¶ç­‰ç´š\n",
    "ORDER BY å®¢æˆ¶æ•¸é‡ DESC\n",
    "\"\"\")\n",
    "segment_stats.show(truncate=False)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ’¡ ç‡ŸéŠ·å»ºè­°ï¼š\n",
    "1. é‡è¦åƒ¹å€¼å®¢æˆ¶ï¼šæä¾› VIP æœå‹™ï¼Œå°ˆå±¬å„ªæƒ \n",
    "2. é‡è¦å–šå›å®¢æˆ¶ï¼šç™¼é€å€‹æ€§åŒ–ä¿ƒéŠ·ï¼Œæ–°å“æ¨è–¦\n",
    "3. é‡è¦ä¿æŒå®¢æˆ¶ï¼šæœƒå“¡ç©åˆ†çå‹µï¼Œè³¼è²·æé†’\n",
    "4. é‡è¦ç™¼å±•å®¢æˆ¶ï¼šäº¤å‰éŠ·å”®ï¼Œæå‡è³¼è²·é »ç‡\n",
    "5. ä¸€èˆ¬å®¢æˆ¶ï¼šå¤§çœ¾ä¿ƒéŠ·æ´»å‹•\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ç¬¬ä¸‰ç« ï¼šå­æŸ¥è©¢å„ªåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š ç¬¬ä¸‰ç« ï¼šå­æŸ¥è©¢å„ªåŒ–\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "- éç›¸é—œå­æŸ¥è©¢ï¼šå…§éƒ¨æŸ¥è©¢ç¨ç«‹åŸ·è¡Œï¼ŒåªåŸ·è¡Œä¸€æ¬¡\n",
    "- ç›¸é—œå­æŸ¥è©¢ï¼šå…§éƒ¨æŸ¥è©¢ä¾è³´å¤–éƒ¨æŸ¥è©¢ï¼Œå¯èƒ½åŸ·è¡Œå¤šæ¬¡ï¼ˆæ•ˆèƒ½è¼ƒå·®ï¼‰\n",
    "\n",
    "ğŸ¯ å„ªåŒ–å»ºè­°ï¼š\n",
    "ç›¡é‡ä½¿ç”¨ JOIN æˆ–è¦–çª—å‡½æ•¸æ›¿ä»£ç›¸é—œå­æŸ¥è©¢\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Œ ç¯„ä¾‹ 7: å­æŸ¥è©¢å„ªåŒ– - æ‰¾å‡ºé«˜æ–¼éƒ¨é–€å¹³å‡è–ªè³‡çš„å“¡å·¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–¹æ³• 1ï¼šä½¿ç”¨ç›¸é—œå­æŸ¥è©¢ï¼ˆè¼ƒæ…¢ï¼‰\n",
    "sql_correlated = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e1.`å§“å`,\n",
    "        e1.`éƒ¨é–€`,\n",
    "        e1.`è–ªè³‡`,\n",
    "        (SELECT AVG(e2.`è–ªè³‡`) \n",
    "         FROM employees e2 \n",
    "         WHERE e2.`éƒ¨é–€` = e1.`éƒ¨é–€`) as `éƒ¨é–€å¹³å‡è–ªè³‡`\n",
    "    FROM employees e1\n",
    "    WHERE e1.`è–ªè³‡` > (\n",
    "        SELECT AVG(e2.`è–ªè³‡`) \n",
    "        FROM employees e2 \n",
    "        WHERE e2.`éƒ¨é–€` = e1.`éƒ¨é–€`\n",
    "    )\n",
    "    ORDER BY e1.`éƒ¨é–€`, e1.`è–ªè³‡` DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\næ–¹æ³• 1 - ç›¸é—œå­æŸ¥è©¢ï¼š\")\n",
    "sql_correlated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–¹æ³• 2ï¼šä½¿ç”¨ JOINï¼ˆè¼ƒå¿«ï¼‰\n",
    "sql_join = spark.sql(\"\"\"\n",
    "    WITH dept_avg AS (\n",
    "        SELECT `éƒ¨é–€`, AVG(`è–ªè³‡`) as `å¹³å‡è–ªè³‡`\n",
    "        FROM employees\n",
    "        GROUP BY `éƒ¨é–€`\n",
    "    )\n",
    "    SELECT \n",
    "        e.`å§“å`,\n",
    "        e.`éƒ¨é–€`,\n",
    "        e.`è–ªè³‡`,\n",
    "        ROUND(d.`å¹³å‡è–ªè³‡`, 2) as `éƒ¨é–€å¹³å‡è–ªè³‡`\n",
    "    FROM employees e\n",
    "    JOIN dept_avg d ON e.`éƒ¨é–€` = d.`éƒ¨é–€`\n",
    "    WHERE e.`è–ªè³‡` > d.`å¹³å‡è–ªè³‡`\n",
    "    ORDER BY e.`éƒ¨é–€`, e.`è–ªè³‡` DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\næ–¹æ³• 2 - ä½¿ç”¨ JOINï¼ˆæ¨è–¦ï¼‰ï¼š\")\n",
    "sql_join.show()\n",
    "# æ–¹æ³• 3ï¼šä½¿ç”¨è¦–çª—å‡½æ•¸ï¼ˆæœ€å¿«ï¼‰\n",
    "windowSpec = Window.partitionBy(\"éƒ¨é–€\")\n",
    "result_window = employees_df.withColumn(\"éƒ¨é–€å¹³å‡è–ªè³‡\", round(avg(\"è–ªè³‡\").over(windowSpec), 2)) \\\n",
    "                            .filter(col(\"è–ªè³‡\") > col(\"éƒ¨é–€å¹³å‡è–ªè³‡\")) \\\n",
    "                            .orderBy(\"éƒ¨é–€\", desc(\"è–ªè³‡\"))\n",
    "print(\"\\næ–¹æ³• 3 - ä½¿ç”¨è¦–çª—å‡½æ•¸ï¼ˆæœ€æ¨è–¦ï¼‰ï¼š\")\n",
    "result_window.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’ª ç·´ç¿’é¡Œ 3\n",
    "\n",
    "**é¡Œç›®ï¼š**\n",
    "\n",
    "è«‹å®Œæˆä¸€å€‹ç¶œåˆæŸ¥è©¢å„ªåŒ–ç·´ç¿’ï¼š\n",
    "1. æ‰¾å‡ºéŠ·å”®é¡è¶…ééƒ¨é–€å¹³å‡å€¼çš„å“¡å·¥ï¼ˆç”¨3ç¨®æ–¹æ³•å¯¦ç¾ï¼‰\n",
    "2. æ¯”è¼ƒå­æŸ¥è©¢ã€CTEã€JOIN ä¸‰ç¨®æ–¹æ³•çš„åŸ·è¡Œè¨ˆç•«\n",
    "3. ä½¿ç”¨ broadcast join å„ªåŒ–å°è¡¨é€£æ¥\n",
    "4. ç¸½çµæ•ˆèƒ½å„ªåŒ–å»ºè­°\n",
    "\n",
    "**æç¤ºï¼šä½¿ç”¨ explain()ã€broadcast() å‡½æ•¸**\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ“ é»æ“ŠæŸ¥çœ‹åƒè€ƒç­”æ¡ˆ</summary>\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import broadcast, col, avg\n",
    "\n",
    "# å»ºç«‹å“¡å·¥éŠ·å”®è³‡æ–™\n",
    "emp_sales_data = [\n",
    "    (1, \"å¼µä¸‰\", \"æ¥­å‹™éƒ¨\", 150000),\n",
    "    (2, \"æå››\", \"æ¥­å‹™éƒ¨\", 200000),\n",
    "    (3, \"ç‹äº”\", \"æ¥­å‹™éƒ¨\", 120000),\n",
    "    (4, \"è¶™å…­\", \"å·¥ç¨‹éƒ¨\", 180000),\n",
    "    (5, \"éŒ¢ä¸ƒ\", \"å·¥ç¨‹éƒ¨\", 160000),\n",
    "    (6, \"å­«å…«\", \"å·¥ç¨‹éƒ¨\", 140000)\n",
    "]\n",
    "\n",
    "emp_sales_df = spark.createDataFrame(\n",
    "    emp_sales_data,\n",
    "    [\"å“¡å·¥ç·¨è™Ÿ\", \"å§“å\", \"éƒ¨é–€\", \"éŠ·å”®é¡\"]\n",
    ")\n",
    "emp_sales_df.createOrReplaceTempView(\"employee_sales\")\n",
    "\n",
    "# ====================================================================\n",
    "# æ–¹æ³• 1: ä½¿ç”¨ç›¸é—œå­æŸ¥è©¢\n",
    "# ====================================================================\n",
    "print(\"=== æ–¹æ³• 1: ç›¸é—œå­æŸ¥è©¢ ===\")\n",
    "result1 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e.å§“å,\n",
    "        e.éƒ¨é–€,\n",
    "        e.éŠ·å”®é¡,\n",
    "        (SELECT AVG(éŠ·å”®é¡) FROM employee_sales WHERE éƒ¨é–€ = e.éƒ¨é–€) as éƒ¨é–€å¹³å‡\n",
    "    FROM employee_sales e\n",
    "    WHERE e.éŠ·å”®é¡ > (\n",
    "        SELECT AVG(éŠ·å”®é¡) \n",
    "        FROM employee_sales \n",
    "        WHERE éƒ¨é–€ = e.éƒ¨é–€\n",
    "    )\n",
    "    ORDER BY e.éƒ¨é–€, e.éŠ·å”®é¡ DESC\n",
    "\"\"\")\n",
    "result1.show()\n",
    "print(\"\\nåŸ·è¡Œè¨ˆç•«:\")\n",
    "result1.explain(mode=\"simple\")\n",
    "\n",
    "# ====================================================================\n",
    "# æ–¹æ³• 2: ä½¿ç”¨ CTE\n",
    "# ====================================================================\n",
    "print(\"\\n=== æ–¹æ³• 2: CTE ===\")\n",
    "result2 = spark.sql(\"\"\"\n",
    "    WITH dept_avg AS (\n",
    "        SELECT \n",
    "            éƒ¨é–€,\n",
    "            AVG(éŠ·å”®é¡) as å¹³å‡éŠ·å”®é¡\n",
    "        FROM employee_sales\n",
    "        GROUP BY éƒ¨é–€\n",
    "    )\n",
    "    SELECT \n",
    "        e.å§“å,\n",
    "        e.éƒ¨é–€,\n",
    "        e.éŠ·å”®é¡,\n",
    "        d.å¹³å‡éŠ·å”®é¡\n",
    "    FROM employee_sales e\n",
    "    INNER JOIN dept_avg d ON e.éƒ¨é–€ = d.éƒ¨é–€\n",
    "    WHERE e.éŠ·å”®é¡ > d.å¹³å‡éŠ·å”®é¡\n",
    "    ORDER BY e.éƒ¨é–€, e.éŠ·å”®é¡ DESC\n",
    "\"\"\")\n",
    "result2.show()\n",
    "print(\"\\nåŸ·è¡Œè¨ˆç•«:\")\n",
    "result2.explain(mode=\"simple\")\n",
    "\n",
    "# ====================================================================\n",
    "# æ–¹æ³• 3: ä½¿ç”¨ Window Functionï¼ˆæœ€å„ªï¼‰\n",
    "# ====================================================================\n",
    "print(\"\\n=== æ–¹æ³• 3: Window Functionï¼ˆæ¨è–¦ï¼‰===\")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"éƒ¨é–€\")\n",
    "\n",
    "result3 = emp_sales_df \\\n",
    "    .withColumn(\"éƒ¨é–€å¹³å‡\", avg(\"éŠ·å”®é¡\").over(window_spec)) \\\n",
    "    .filter(col(\"éŠ·å”®é¡\") > col(\"éƒ¨é–€å¹³å‡\")) \\\n",
    "    .orderBy(\"éƒ¨é–€\", col(\"éŠ·å”®é¡\").desc())\n",
    "\n",
    "result3.show()\n",
    "print(\"\\nåŸ·è¡Œè¨ˆç•«:\")\n",
    "result3.explain(mode=\"simple\")\n",
    "\n",
    "# ====================================================================\n",
    "# æ–¹æ³• 4: Broadcast Join å„ªåŒ–ï¼ˆå°è¡¨å ´æ™¯ï¼‰\n",
    "# ====================================================================\n",
    "print(\"\\n=== æ–¹æ³• 4: Broadcast Join ===\")\n",
    "\n",
    "# è¨ˆç®—éƒ¨é–€å¹³å‡ï¼ˆå°è¡¨ï¼‰\n",
    "dept_avg_df = emp_sales_df.groupBy(\"éƒ¨é–€\") \\\n",
    "    .agg(avg(\"éŠ·å”®é¡\").alias(\"å¹³å‡éŠ·å”®é¡\"))\n",
    "\n",
    "# ä½¿ç”¨ broadcast å„ªåŒ–\n",
    "result4 = emp_sales_df.join(\n",
    "    broadcast(dept_avg_df),\n",
    "    \"éƒ¨é–€\"\n",
    ").filter(col(\"éŠ·å”®é¡\") > col(\"å¹³å‡éŠ·å”®é¡\")) \\\n",
    "    .select(\"å§“å\", \"éƒ¨é–€\", \"éŠ·å”®é¡\", \"å¹³å‡éŠ·å”®é¡\") \\\n",
    "    .orderBy(\"éƒ¨é–€\", col(\"éŠ·å”®é¡\").desc())\n",
    "\n",
    "result4.show()\n",
    "print(\"\\nåŸ·è¡Œè¨ˆç•«ï¼ˆæ³¨æ„ BroadcastHashJoinï¼‰:\")\n",
    "result4.explain(mode=\"simple\")\n",
    "\n",
    "# ====================================================================\n",
    "# æ•ˆèƒ½æ¯”è¼ƒç¸½çµ\n",
    "# ====================================================================\n",
    "print(\"\"\"\n",
    "\\nğŸ“Š æ•ˆèƒ½å„ªåŒ–ç¸½çµï¼š\n",
    "\n",
    "1. **ç›¸é—œå­æŸ¥è©¢**ï¼š\n",
    "   - âŒ æ•ˆèƒ½æœ€å·®ï¼Œæ¯è¡Œéƒ½åŸ·è¡Œä¸€æ¬¡å­æŸ¥è©¢\n",
    "   - âŒ é¿å…åœ¨å¤§æ•¸æ“šé›†ä½¿ç”¨\n",
    "\n",
    "2. **CTE + JOIN**ï¼š\n",
    "   - âœ“ æ•ˆèƒ½è¼ƒå¥½ï¼Œå­æŸ¥è©¢åªåŸ·è¡Œä¸€æ¬¡\n",
    "   - âœ“ ç¨‹å¼ç¢¼å¯è®€æ€§é«˜\n",
    "   \n",
    "3. **Window Function**ï¼š\n",
    "   - âœ… æ•ˆèƒ½æœ€ä½³ï¼Œå–®æ¬¡æƒæå®Œæˆ\n",
    "   - âœ… Spark å…§éƒ¨å„ªåŒ–è‰¯å¥½\n",
    "   - âœ… **æ¨è–¦ä½¿ç”¨**\n",
    "\n",
    "4. **Broadcast Join**ï¼š\n",
    "   - âœ… é©ç”¨æ–¼å°è¡¨ï¼ˆ<10MBï¼‰\n",
    "   - âœ… é¿å… shuffleï¼Œæ•ˆèƒ½æå‡æ˜é¡¯\n",
    "   - âš ï¸  éœ€ç¢ºèªè¡¨å¤§å°\n",
    "\n",
    "ğŸ’¡ æœ€ä½³å¯¦è¸ï¼š\n",
    "1. å„ªå…ˆä½¿ç”¨ Window Functions\n",
    "2. é¿å…ç›¸é—œå­æŸ¥è©¢\n",
    "3. å°è¡¨ä½¿ç”¨ broadcast\n",
    "4. åˆç†åˆ†å€ï¼ˆpartitionï¼‰\n",
    "5. ä½¿ç”¨ cache() æš«å­˜ä¸­é–“çµæœ\n",
    "6. æŸ¥çœ‹åŸ·è¡Œè¨ˆç•«å„ªåŒ–æŸ¥è©¢\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ç¬¬å››ç« ï¼šé€²éš JOIN æŠ€å·§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š ç¬¬å››ç« ï¼šé€²éš JOIN æŠ€å·§\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "CROSS JOIN ç”¢ç”Ÿå…©å€‹è¡¨çš„ç¬›å¡çˆ¾ç©ï¼Œå³æ‰€æœ‰å¯èƒ½çš„çµ„åˆã€‚\n",
    "\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š\n",
    "ç”Ÿæˆæ¸¬è©¦è³‡æ–™ã€å»ºç«‹æ™‚é–“åºåˆ—ã€ç”¢ç”Ÿæ‰€æœ‰å¯èƒ½çš„çµ„åˆ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Œ ç¯„ä¾‹ 8: CROSS JOIN - ç”¢ç”Ÿæ‰€æœ‰å¯èƒ½çš„é…å°\")\n",
    "# å»ºç«‹ç”¢å“å’Œé¡è‰²è³‡æ–™\n",
    "products = spark.createDataFrame([(\"æ‰‹æ©Ÿ\",), (\"å¹³æ¿\",), (\"ç­†é›»\",)], [\"ç”¢å“\"])\n",
    "colors = spark.createDataFrame([(\"é»‘è‰²\",), (\"ç™½è‰²\",), (\"éŠ€è‰²\",)], [\"é¡è‰²\"])\n",
    "# CROSS JOIN\n",
    "cross_result = products.crossJoin(colors)\n",
    "print(\"\\nç”¢å“èˆ‡é¡è‰²çš„æ‰€æœ‰çµ„åˆï¼š\")\n",
    "cross_result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "SELF JOIN æ˜¯è¡¨èˆ‡è‡ªå·±é€²è¡Œé€£æ¥ï¼Œå¸¸ç”¨æ–¼æŸ¥æ‰¾å±¤ç´šé—œä¿‚æˆ–æ¯”è¼ƒåŒè¡¨å…§çš„è¨˜éŒ„ã€‚\n",
    "\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š\n",
    "å“¡å·¥-ä¸»ç®¡é—œä¿‚ã€çµ„ç¹”æ¶æ§‹ã€å°‹æ‰¾é‡è¤‡è¨˜éŒ„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Œ ç¯„ä¾‹ 9: SELF JOIN - å“¡å·¥èˆ‡ä¸»ç®¡é—œä¿‚\")\n",
    "# å»ºç«‹åŒ…å«ä¸»ç®¡è³‡è¨Šçš„å“¡å·¥è³‡æ–™\n",
    "emp_manager_data = [\n",
    "    (1, \"Alice\", None),      # CEOï¼Œæ²’æœ‰ä¸»ç®¡\n",
    "    (2, \"Bob\", 1),           # Bob çš„ä¸»ç®¡æ˜¯ Alice\n",
    "    (3, \"Charlie\", 1),       # Charlie çš„ä¸»ç®¡æ˜¯ Alice\n",
    "    (4, \"David\", 2),         # David çš„ä¸»ç®¡æ˜¯ Bob\n",
    "    (5, \"Eve\", 2),           # Eve çš„ä¸»ç®¡æ˜¯ Bob\n",
    "    (6, \"Frank\", 3),         # Frank çš„ä¸»ç®¡æ˜¯ Charlie\n",
    "]\n",
    "emp_manager_df = spark.createDataFrame(\n",
    "    emp_manager_data,\n",
    "    [\"å“¡å·¥ç·¨è™Ÿ\", \"å§“å\", \"ä¸»ç®¡ç·¨è™Ÿ\"]\n",
    ")\n",
    "emp_manager_df.createOrReplaceTempView(\"emp_manager\")\n",
    "# ä½¿ç”¨ SELF JOIN æŸ¥è©¢å“¡å·¥å’Œå…¶ä¸»ç®¡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ SELF JOIN æŸ¥è©¢å“¡å·¥å’Œå…¶ä¸»ç®¡\n",
    "sql_self_join = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e.`å“¡å·¥ç·¨è™Ÿ`,\n",
    "        e.`å§“å` as `å“¡å·¥å§“å`,\n",
    "        COALESCE(m.`å§“å`, 'ç„¡ä¸»ç®¡') as `ä¸»ç®¡å§“å`\n",
    "    FROM emp_manager e\n",
    "    LEFT JOIN emp_manager m ON e.`ä¸»ç®¡ç·¨è™Ÿ` = m.`å“¡å·¥ç·¨è™Ÿ`\n",
    "    ORDER BY e.`å“¡å·¥ç·¨è™Ÿ`\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nå“¡å·¥èˆ‡ä¸»ç®¡é—œä¿‚ï¼š\")\n",
    "sql_self_join.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”Ÿ ANTI JOIN & SEMI JOIN - é«˜æ•ˆéæ¿¾\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "- SEMI JOIN (LEFT SEMI): è¿”å›å·¦è¡¨ä¸­åœ¨å³è¡¨ä¸­æœ‰åŒ¹é…çš„è¨˜éŒ„\n",
    "- ANTI JOIN (LEFT ANTI): è¿”å›å·¦è¡¨ä¸­åœ¨å³è¡¨ä¸­æ²’æœ‰åŒ¹é…çš„è¨˜éŒ„\n",
    "\n",
    "ğŸ¯ å„ªå‹¢ï¼š\n",
    "æ¯” IN / NOT IN å­æŸ¥è©¢æ•ˆèƒ½æ›´å¥½\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Œ ç¯„ä¾‹ 10: ANTI JOIN & SEMI JOIN - å®¢æˆ¶è¨‚å–®åˆ†æ\")\n",
    "# å»ºç«‹å®¢æˆ¶è³‡æ–™\n",
    "customers_data = [\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (3, \"Charlie\"),\n",
    "    (4, \"David\"),\n",
    "    (5, \"Eve\"),\n",
    "]\n",
    "customers_df = spark.createDataFrame(customers_data, [\"å®¢æˆ¶ç·¨è™Ÿ\", \"å®¢æˆ¶å§“å\"])\n",
    "# å»ºç«‹è¨‚å–®è³‡æ–™ï¼ˆåªæœ‰éƒ¨åˆ†å®¢æˆ¶æœ‰è¨‚å–®ï¼‰\n",
    "order_customers = orders_df.select(\"å®¢æˆ¶\").distinct()\n",
    "# SEMI JOIN - æœ‰è¨‚å–®çš„å®¢æˆ¶\n",
    "customers_with_orders = customers_df.join(\n",
    "    order_customers,\n",
    "    customers_df[\"å®¢æˆ¶å§“å\"] == order_customers[\"å®¢æˆ¶\"],\n",
    "    \"leftsemi\"\n",
    ")\n",
    "print(\"\\næœ‰è¨‚å–®çš„å®¢æˆ¶ï¼ˆSEMI JOINï¼‰ï¼š\")\n",
    "customers_with_orders.show()\n",
    "# ANTI JOIN - æ²’æœ‰è¨‚å–®çš„å®¢æˆ¶\n",
    "customers_without_orders = customers_df.join(\n",
    "    order_customers,\n",
    "    customers_df[\"å®¢æˆ¶å§“å\"] == order_customers[\"å®¢æˆ¶\"],\n",
    "    \"leftanti\"\n",
    ")\n",
    "print(\"\\næ²’æœ‰è¨‚å–®çš„å®¢æˆ¶ï¼ˆANTI JOINï¼‰ï¼š\")\n",
    "customers_without_orders.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ç¬¬äº”ç« ï¼šæ•ˆèƒ½å„ªåŒ–æŠ€å·§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š ç¬¬äº”ç« ï¼šæ•ˆèƒ½å„ªåŒ–æŠ€å·§\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "ç•¶ä¸€å€‹å¤§è¡¨å’Œä¸€å€‹å°è¡¨é€²è¡Œ JOIN æ™‚ï¼Œå¯ä»¥å°‡å°è¡¨å»£æ’­åˆ°æ‰€æœ‰ç¯€é»ï¼Œ\n",
    "é¿å… shuffle æ“ä½œï¼Œå¤§å¹…æå‡æ•ˆèƒ½ã€‚\n",
    "\n",
    "ğŸ¯ é©ç”¨å ´æ™¯ï¼š\n",
    "å°è¡¨ < 10MBï¼Œå¤§è¡¨èˆ‡ç¶­åº¦è¡¨ JOIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Œ ç¯„ä¾‹ 11: Broadcast Join - å„ªåŒ–å¤§å°è¡¨é€£æ¥\")\n",
    "# å»ºç«‹éƒ¨é–€è³‡è¨Šï¼ˆå°è¡¨ï¼‰\n",
    "departments_data = [\n",
    "    (\"å·¥ç¨‹éƒ¨\", \"æŠ€è¡“å¤§æ¨“\"),\n",
    "    (\"è¡ŒéŠ·éƒ¨\", \"è¡Œæ”¿å¤§æ¨“\"),\n",
    "    (\"äººè³‡éƒ¨\", \"è¡Œæ”¿å¤§æ¨“\"),\n",
    "]\n",
    "departments_df = spark.createDataFrame(departments_data, [\"éƒ¨é–€\", \"è¾¦å…¬åœ°é»\"])\n",
    "# ä¸€èˆ¬ JOIN\n",
    "normal_join = employees_df.join(departments_df, \"éƒ¨é–€\")\n",
    "print(\"\\nä¸€èˆ¬ JOINï¼š\")\n",
    "normal_join.select(\"å§“å\", \"éƒ¨é–€\", \"è¾¦å…¬åœ°é»\").show(5)\n",
    "# Broadcast JOIN\n",
    "broadcast_join = employees_df.join(\n",
    "    broadcast(departments_df),\n",
    "    \"éƒ¨é–€\"\n",
    ")\n",
    "print(\"\\nBroadcast JOINï¼ˆæ•ˆèƒ½æ›´å¥½ï¼‰ï¼š\")\n",
    "broadcast_join.select(\"å§“å\", \"éƒ¨é–€\", \"è¾¦å…¬åœ°é»\").show(5)\n",
    "# æŸ¥çœ‹åŸ·è¡Œè¨ˆç•«\n",
    "print(\"\\nåŸ·è¡Œè¨ˆç•«å·®ç•°ï¼š\")\n",
    "print(\"ä¸€èˆ¬ JOIN æœƒæœ‰ SortMergeJoin\")\n",
    "print(\"Broadcast JOIN æœƒæœ‰ BroadcastHashJoin\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "- Partitioning: æŒ‰æ¬„ä½å€¼å°‡è³‡æ–™åˆ†å‰²æˆå¤šå€‹ç›®éŒ„\n",
    "- Bucketing: æŒ‰ hash å€¼å°‡è³‡æ–™åˆ†å‰²æˆå›ºå®šæ•¸é‡çš„æª”æ¡ˆ\n",
    "\n",
    "ğŸ¯ å„ªå‹¢ï¼š\n",
    "æ¸›å°‘æƒæçš„è³‡æ–™é‡ï¼Œæå‡æŸ¥è©¢æ•ˆèƒ½\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Œ ç¯„ä¾‹ 12: åˆ†å€èˆ‡éæ¿¾å„ªåŒ–\")\n",
    "# é‡æ–°åˆ†å€è³‡æ–™\n",
    "employees_partitioned = employees_df.repartition(2, \"éƒ¨é–€\")\n",
    "print(\"\\né‡æ–°åˆ†å€å¾Œçš„åˆ†å€æ•¸ï¼š\", employees_partitioned.rdd.getNumPartitions())\n",
    "# ä½¿ç”¨åˆ†å€æ¬„ä½éæ¿¾ï¼ˆæ•ˆèƒ½æ›´å¥½ï¼‰\n",
    "filtered = employees_partitioned.filter(col(\"éƒ¨é–€\") == \"å·¥ç¨‹éƒ¨\")\n",
    "print(\"\\néæ¿¾å¾Œçš„çµæœï¼š\")\n",
    "filtered.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "å°‡å¸¸ç”¨çš„ DataFrame å¿«å–åˆ°è¨˜æ†¶é«”ä¸­ï¼Œé¿å…é‡è¤‡è¨ˆç®—ã€‚\n",
    "\n",
    "ğŸ¯ é©ç”¨å ´æ™¯ï¼š\n",
    "å¤šæ¬¡ä½¿ç”¨ç›¸åŒçš„ä¸­é–“çµæœã€è¿­ä»£è¨ˆç®—\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Œ ç¯„ä¾‹ 13: å¿«å–å„ªåŒ–\")\n",
    "# å»ºç«‹ä¸€å€‹è¤‡é›œçš„è¨ˆç®—\n",
    "complex_df = employees_df.groupBy(\"éƒ¨é–€\") \\\n",
    "                        .agg(\n",
    "                            count(\"*\").alias(\"äººæ•¸\"),\n",
    "                            avg(\"è–ªè³‡\").alias(\"å¹³å‡è–ªè³‡\"),\n",
    "                            max(\"è–ªè³‡\").alias(\"æœ€é«˜è–ªè³‡\")\n",
    "                        )\n",
    "# å¿«å–çµæœ\n",
    "complex_df.cache()\n",
    "# ç¬¬ä¸€æ¬¡åŸ·è¡Œï¼ˆæœƒè§¸ç™¼è¨ˆç®—ä¸¦å¿«å–ï¼‰\n",
    "print(\"\\nç¬¬ä¸€æ¬¡æŸ¥è©¢ï¼ˆè¨ˆç®—ä¸¦å¿«å–ï¼‰ï¼š\")\n",
    "start = time.time()\n",
    "complex_df.show()\n",
    "print(f\"åŸ·è¡Œæ™‚é–“: {time.time() - start:.4f} ç§’\")\n",
    "# ç¬¬äºŒæ¬¡åŸ·è¡Œï¼ˆå¾å¿«å–è®€å–ï¼‰\n",
    "print(\"\\nç¬¬äºŒæ¬¡æŸ¥è©¢ï¼ˆå¾å¿«å–è®€å–ï¼‰ï¼š\")\n",
    "start = time.time()\n",
    "complex_df.show()\n",
    "print(f\"åŸ·è¡Œæ™‚é–“: {time.time() - start:.4f} ç§’ï¼ˆæ‡‰è©²æ›´å¿«ï¼‰\")\n",
    "# é‡‹æ”¾å¿«å–\n",
    "complex_df.unpersist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ç¬¬å…­ç« ï¼šè¤‡é›œè³‡æ–™è½‰æ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š ç¬¬å…­ç« ï¼šè¤‡é›œè³‡æ–™è½‰æ›\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "PIVOT å°‡è¡Œè³‡æ–™è½‰æ›ç‚ºåˆ—ï¼Œå¸¸ç”¨æ–¼å»ºç«‹äº¤å‰è¡¨ã€‚\n",
    "\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š\n",
    "éŠ·å”®å ±è¡¨ã€è³‡æ–™é€è¦–è¡¨ã€è¶¨å‹¢åˆ†æ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Œ ç¯„ä¾‹ 14: PIVOT - å»ºç«‹éƒ¨é–€è–ªè³‡é€è¦–è¡¨\")\n",
    "# ä½¿ç”¨ PIVOT å»ºç«‹éƒ¨é–€-çµ±è¨ˆæŒ‡æ¨™é€è¦–è¡¨\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_result = spark.sql(\"\"\"\n",
    "    SELECT * FROM (\n",
    "        SELECT `éƒ¨é–€`, `è–ªè³‡`\n",
    "        FROM employees\n",
    "    )\n",
    "    PIVOT (\n",
    "        COUNT(*) as `äººæ•¸`,\n",
    "        AVG(`è–ªè³‡`) as `å¹³å‡è–ªè³‡`\n",
    "        FOR `éƒ¨é–€` IN ('å·¥ç¨‹éƒ¨', 'è¡ŒéŠ·éƒ¨', 'äººè³‡éƒ¨')\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\néƒ¨é–€è–ªè³‡é€è¦–è¡¨ï¼š\")\n",
    "pivot_result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "UNPIVOT å°‡åˆ—è³‡æ–™è½‰æ›ç‚ºè¡Œï¼Œæ˜¯ PIVOT çš„åå‘æ“ä½œã€‚\n",
    "\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š\n",
    "å°‡å¯¬è¡¨è½‰æ›ç‚ºé•·è¡¨ã€è³‡æ–™æ­£è¦åŒ–\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96586251",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Œ ç¯„ä¾‹ 15: UNPIVOT - åˆ—è½‰è¡Œ\")\n",
    "# å»ºç«‹å¯¬è¡¨æ ¼å¼çš„å­£åº¦éŠ·å”®è³‡æ–™\n",
    "quarterly_sales = spark.createDataFrame([\n",
    "    (\"ç”¢å“A\", 100, 120, 115, 130),\n",
    "    (\"ç”¢å“B\", 80, 90, 95, 100),\n",
    "], [\"ç”¢å“\", \"Q1\", \"Q2\", \"Q3\", \"Q4\"])\n",
    "quarterly_sales.createOrReplaceTempView(\"quarterly_sales\")\n",
    "# ä½¿ç”¨ UNPIVOTï¼ˆSpark 3.4+ï¼‰æˆ–ä½¿ç”¨ stack å‡½æ•¸\n",
    "unpivot_result = quarterly_sales.selectExpr(\n",
    "    \"`ç”¢å“`\",\n",
    "    \"stack(4, 'Q1', `Q1`, 'Q2', `Q2`, 'Q3', `Q3`, 'Q4', `Q4`) as (`å­£åº¦`, `éŠ·å”®é¡`)\"\n",
    ")\n",
    "print(\"\\nè½‰æ›ç‚ºé•·è¡¨æ ¼å¼ï¼š\")\n",
    "unpivot_result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "PySpark æ”¯æ´è¤‡é›œçš„è³‡æ–™å‹æ…‹å¦‚é™£åˆ—ã€çµæ§‹é«”ã€åœ°åœ–ç­‰ã€‚\n",
    "\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š\n",
    "è™•ç† JSON è³‡æ–™ã€å·¢ç‹€çµæ§‹ã€å¤šå€¼æ¬„ä½\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Œ ç¯„ä¾‹ 16: é™£åˆ—æ“ä½œ\")\n",
    "# å»ºç«‹åŒ…å«é™£åˆ—çš„è³‡æ–™\n",
    "students_data = [\n",
    "    (1, \"Alice\", [\"æ•¸å­¸\", \"ç‰©ç†\", \"åŒ–å­¸\"]),\n",
    "    (2, \"Bob\", [\"è‹±æ–‡\", \"æ­·å²\"]),\n",
    "    (3, \"Charlie\", [\"æ•¸å­¸\", \"è‹±æ–‡\", \"é«”è‚²\"]),\n",
    "]\n",
    "students_df = spark.createDataFrame(\n",
    "    students_data,\n",
    "    [\"å­¸è™Ÿ\", \"å§“å\", \"é¸ä¿®èª²ç¨‹\"]\n",
    ")\n",
    "print(\"\\nåŸå§‹è³‡æ–™ï¼ˆåŒ…å«é™£åˆ—ï¼‰ï¼š\")\n",
    "students_df.show(truncate=False)\n",
    "# å±•é–‹é™£åˆ—\n",
    "exploded = students_df.select(\n",
    "    \"å­¸è™Ÿ\",\n",
    "    \"å§“å\",\n",
    "    explode(\"é¸ä¿®èª²ç¨‹\").alias(\"èª²ç¨‹\")\n",
    ")\n",
    "print(\"\\nå±•é–‹é™£åˆ—å¾Œï¼š\")\n",
    "exploded.show()\n",
    "# é™£åˆ—ç›¸é—œå‡½æ•¸\n",
    "array_operations = students_df.select(\n",
    "    \"å§“å\",\n",
    "    \"é¸ä¿®èª²ç¨‹\",\n",
    "    size(\"é¸ä¿®èª²ç¨‹\").alias(\"èª²ç¨‹æ•¸\"),\n",
    "    array_contains(\"é¸ä¿®èª²ç¨‹\", \"æ•¸å­¸\").alias(\"æ˜¯å¦é¸ä¿®æ•¸å­¸\")\n",
    ")\n",
    "print(\"\\né™£åˆ—æ“ä½œï¼š\")\n",
    "array_operations.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ç¬¬ä¸ƒç« ï¼šè³‡æ–™å“è³ªèˆ‡æ¸…ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š ç¬¬ä¸ƒç« ï¼šè³‡æ–™å“è³ªèˆ‡æ¸…ç†\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "NULL å€¼è™•ç†æ˜¯è³‡æ–™æ¸…ç†çš„é‡è¦ç’°ç¯€ã€‚\n",
    "\n",
    "ğŸ¯ å¸¸ç”¨æ–¹æ³•ï¼š\n",
    "fillna, dropna, coalesce, nvl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Œ ç¯„ä¾‹ 17: NULL å€¼è™•ç†\")\n",
    "# å»ºç«‹åŒ…å« NULL çš„è³‡æ–™\n",
    "data_with_null = [\n",
    "    (1, \"Alice\", 75000, \"å·¥ç¨‹éƒ¨\"),\n",
    "    (2, \"Bob\", None, \"è¡ŒéŠ·éƒ¨\"),\n",
    "    (3, \"Charlie\", 68000, None),\n",
    "    (4, None, 65000, \"äººè³‡éƒ¨\"),\n",
    "]\n",
    "df_null = spark.createDataFrame(\n",
    "    data_with_null,\n",
    "    [\"å“¡å·¥ç·¨è™Ÿ\", \"å§“å\", \"è–ªè³‡\", \"éƒ¨é–€\"]\n",
    ")\n",
    "print(\"\\nåŸå§‹è³‡æ–™ï¼ˆåŒ…å« NULLï¼‰ï¼š\")\n",
    "df_null.show()\n",
    "# æ–¹æ³• 1: å¡«å……é è¨­å€¼\n",
    "filled = df_null.fillna({\n",
    "    \"å§“å\": \"æœªçŸ¥\",\n",
    "    \"è–ªè³‡\": 60000,\n",
    "    \"éƒ¨é–€\": \"å¾…åˆ†é…\"\n",
    "})\n",
    "print(\"\\nå¡«å……é è¨­å€¼å¾Œï¼š\")\n",
    "filled.show()\n",
    "# æ–¹æ³• 2: åˆªé™¤åŒ…å« NULL çš„åˆ—\n",
    "dropped = df_null.dropna()\n",
    "print(\"\\nåˆªé™¤ NULL åˆ—å¾Œï¼š\")\n",
    "dropped.show()\n",
    "# æ–¹æ³• 3: ä½¿ç”¨ COALESCE\n",
    "coalesced = df_null.select(\n",
    "    \"å“¡å·¥ç·¨è™Ÿ\",\n",
    "    coalesce(col(\"å§“å\"), lit(\"æœªçŸ¥\")).alias(\"å§“å\"),\n",
    "    coalesce(col(\"è–ªè³‡\"), lit(60000)).alias(\"è–ªè³‡\"),\n",
    "    coalesce(col(\"éƒ¨é–€\"), lit(\"å¾…åˆ†é…\")).alias(\"éƒ¨é–€\")\n",
    ")\n",
    "print(\"\\nä½¿ç”¨ COALESCE è™•ç†ï¼š\")\n",
    "coalesced.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "æ‰¾å‡ºä¸¦ç§»é™¤é‡è¤‡çš„è¨˜éŒ„ã€‚\n",
    "\n",
    "ğŸ¯ æ–¹æ³•ï¼š\n",
    "distinct, dropDuplicates, è¦–çª—å‡½æ•¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Œ ç¯„ä¾‹ 18: è³‡æ–™å»é‡\")\n",
    "# å»ºç«‹åŒ…å«é‡è¤‡è³‡æ–™\n",
    "duplicate_data = [\n",
    "    (1, \"Alice\", \"å·¥ç¨‹éƒ¨\"),\n",
    "    (2, \"Bob\", \"è¡ŒéŠ·éƒ¨\"),\n",
    "    (1, \"Alice\", \"å·¥ç¨‹éƒ¨\"),  # å®Œå…¨é‡è¤‡\n",
    "    (3, \"Alice\", \"äººè³‡éƒ¨\"),  # å§“åé‡è¤‡\n",
    "]\n",
    "df_dup = spark.createDataFrame(\n",
    "    duplicate_data,\n",
    "    [\"å“¡å·¥ç·¨è™Ÿ\", \"å§“å\", \"éƒ¨é–€\"]\n",
    ")\n",
    "print(\"\\nåŸå§‹è³‡æ–™ï¼ˆåŒ…å«é‡è¤‡ï¼‰ï¼š\")\n",
    "df_dup.show()\n",
    "# æ–¹æ³• 1: å®Œå…¨å»é‡\n",
    "dedup_all = df_dup.distinct()\n",
    "print(\"\\nå®Œå…¨å»é‡ï¼š\")\n",
    "dedup_all.show()\n",
    "# æ–¹æ³• 2: æŒ‰ç‰¹å®šæ¬„ä½å»é‡ï¼ˆä¿ç•™ç¬¬ä¸€ç­†ï¼‰\n",
    "dedup_by_name = df_dup.dropDuplicates([\"å§“å\"])\n",
    "print(\"\\næŒ‰å§“åå»é‡ï¼š\")\n",
    "dedup_by_name.show()\n",
    "# æ–¹æ³• 3: ä½¿ç”¨è¦–çª—å‡½æ•¸ä¿ç•™ç‰¹å®šè¦å‰‡çš„è¨˜éŒ„\n",
    "windowSpec = Window.partitionBy(\"å§“å\").orderBy(\"å“¡å·¥ç·¨è™Ÿ\")\n",
    "dedup_window = df_dup.withColumn(\"rn\", row_number().over(windowSpec)) \\\n",
    "                     .filter(col(\"rn\") == 1) \\\n",
    "                     .drop(\"rn\")\n",
    "print(\"\\nä½¿ç”¨è¦–çª—å‡½æ•¸å»é‡ï¼ˆä¿ç•™ç·¨è™Ÿæœ€å°çš„ï¼‰ï¼š\")\n",
    "dedup_window.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ç¬¬å…«ç« ï¼šé€²éšåˆ†æå‡½æ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š ç¬¬å…«ç« ï¼šé€²éšåˆ†æå‡½æ•¸\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "NTILE å°‡è³‡æ–™åˆ†æˆ N å€‹å¤§è‡´ç›¸ç­‰çš„çµ„ã€‚\n",
    "\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š\n",
    "å®¢æˆ¶åˆ†å±¤ã€ABC åˆ†æã€åˆ†ä½æ•¸åˆ†æ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Œ ç¯„ä¾‹ 19: NTILE - è–ªè³‡å››åˆ†ä½æ•¸\")\n",
    "windowSpec = Window.orderBy(\"è–ªè³‡\")\n",
    "ntile_result = employees_df.withColumn(\n",
    "    \"è–ªè³‡åˆ†çµ„\",\n",
    "    ntile(4).over(windowSpec)\n",
    ").withColumn(\n",
    "    \"åˆ†çµ„èªªæ˜\",\n",
    "    when(col(\"è–ªè³‡åˆ†çµ„\") == 1, \"ä½è–ªçµ„\")\n",
    "    .when(col(\"è–ªè³‡åˆ†çµ„\") == 2, \"ä¸­ä½è–ªçµ„\")\n",
    "    .when(col(\"è–ªè³‡åˆ†çµ„\") == 3, \"ä¸­é«˜è–ªçµ„\")\n",
    "    .otherwise(\"é«˜è–ªçµ„\")\n",
    ")\n",
    "print(\"\\nè–ªè³‡å››åˆ†ä½æ•¸åˆ†çµ„ï¼š\")\n",
    "ntile_result.orderBy(\"è–ªè³‡\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸˆ æ¦‚å¿µè§£é‡‹ï¼š\n",
    "PERCENT_RANK è¨ˆç®—å€¼åœ¨è³‡æ–™é›†ä¸­çš„ç›¸å°ä½ç½®ï¼ˆ0 åˆ° 1 ä¹‹é–“ï¼‰ã€‚\n",
    "\n",
    "ğŸ¯ æ‡‰ç”¨å ´æ™¯ï¼š\n",
    "ç¸¾æ•ˆè©•ä¼°ã€æˆç¸¾åˆ†æã€ç›¸å°æ’å\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“Œ ç¯„ä¾‹ 20: PERCENT_RANK - è–ªè³‡ç™¾åˆ†ä½\")\n",
    "windowSpec = Window.orderBy(\"è–ªè³‡\")\n",
    "percent_result = employees_df.withColumn(\n",
    "    \"è–ªè³‡ç™¾åˆ†ä½\",\n",
    "    round(percent_rank().over(windowSpec) * 100, 2)\n",
    ").withColumn(\n",
    "    \"èªªæ˜\",\n",
    "    concat(\n",
    "        lit(\"è–ªè³‡è¶…é \"),\n",
    "        round(percent_rank().over(windowSpec) * 100, 0).cast(\"int\"),\n",
    "        lit(\"% çš„å“¡å·¥\")\n",
    "    )\n",
    ")\n",
    "print(\"\\nè–ªè³‡ç™¾åˆ†ä½æ’åï¼š\")\n",
    "percent_result.orderBy(\"è–ªè³‡\").select(\"å§“å\", \"è–ªè³‡\", \"è–ªè³‡ç™¾åˆ†ä½\", \"èªªæ˜\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ èª²ç¨‹ç¸½çµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ èª²ç¨‹ç¸½çµ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸŠ æ­å–œï¼ä½ å·²ç¶“å®Œæˆäº† SQL é€²éšèª²ç¨‹ï¼\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“ é€²éšæŠ€èƒ½ç¸½çµï¼š\n",
    "\n",
    "1ï¸âƒ£ è¦–çª—å‡½æ•¸ï¼š\n",
    "   - ROW_NUMBER, RANK, DENSE_RANKï¼šæ’åèˆ‡ç·¨è™Ÿ\n",
    "   - LEAD, LAGï¼šè¨ªå•å‰å¾Œåˆ—è³‡æ–™\n",
    "   - SUM/AVG OVERï¼šç´¯è¨ˆè¨ˆç®—èˆ‡ç§»å‹•å¹³å‡\n",
    "   - NTILE, PERCENT_RANKï¼šåˆ†çµ„èˆ‡ç™¾åˆ†ä½\n",
    "\n",
    "2ï¸âƒ£ é€šç”¨è¡¨è¡¨é”å¼ (CTE)ï¼š\n",
    "   - WITH å­å¥ï¼šæé«˜æŸ¥è©¢å¯è®€æ€§\n",
    "   - å¤šå±¤ CTEï¼šåˆ†è§£è¤‡é›œé‚è¼¯\n",
    "   - éè¿´ CTEï¼šè™•ç†å±¤ç´šé—œä¿‚\n",
    "\n",
    "3ï¸âƒ£ å­æŸ¥è©¢å„ªåŒ–ï¼š\n",
    "   - é¿å…ç›¸é—œå­æŸ¥è©¢\n",
    "   - ä½¿ç”¨ JOIN æ›¿ä»£\n",
    "   - ä½¿ç”¨è¦–çª—å‡½æ•¸å„ªåŒ–\n",
    "\n",
    "4ï¸âƒ£ é€²éš JOINï¼š\n",
    "   - CROSS JOINï¼šç¬›å¡çˆ¾ç©\n",
    "   - SELF JOINï¼šè‡ªæˆ‘é€£æ¥\n",
    "   - SEMI/ANTI JOINï¼šé«˜æ•ˆéæ¿¾\n",
    "\n",
    "5ï¸âƒ£ æ•ˆèƒ½å„ªåŒ–ï¼š\n",
    "   - Broadcast Joinï¼šå»£æ’­å°è¡¨\n",
    "   - åˆ†å€èˆ‡åˆ†æ¡¶ï¼šè³‡æ–™çµ„ç¹”\n",
    "   - å¿«å–èˆ‡æŒä¹…åŒ–ï¼šé¿å…é‡è¤‡è¨ˆç®—\n",
    "\n",
    "6ï¸âƒ£ è¤‡é›œè³‡æ–™è½‰æ›ï¼š\n",
    "   - PIVOT/UNPIVOTï¼šè¡Œåˆ—è½‰æ›\n",
    "   - é™£åˆ—èˆ‡çµæ§‹ï¼šè™•ç†è¤‡é›œå‹æ…‹\n",
    "   - è³‡æ–™å±•é–‹èˆ‡èšåˆ\n",
    "\n",
    "7ï¸âƒ£ è³‡æ–™å“è³ªï¼š\n",
    "   - NULL å€¼è™•ç†\n",
    "   - è³‡æ–™å»é‡\n",
    "   - è³‡æ–™é©—è­‰\n",
    "\n",
    "ğŸ’¡ é€²éšå­¸ç¿’å»ºè­°ï¼š\n",
    "- ç†è§£åŸ·è¡Œè¨ˆç•«ï¼Œå­¸æœƒæ•ˆèƒ½åˆ†æ\n",
    "- æŒæ¡è³‡æ–™åˆ†å€ç­–ç•¥\n",
    "- ç†Ÿæ‚‰ Spark UI çš„ä½¿ç”¨\n",
    "- å¯¦è¸ Delta Lake é€²éšåŠŸèƒ½\n",
    "- å­¸ç¿’ Spark Streaming\n",
    "\n",
    "ğŸš€ ä¸‹ä¸€æ­¥ï¼š\n",
    "- æ·±å…¥ç ”ç©¶ Spark å…§éƒ¨æ©Ÿåˆ¶\n",
    "- å­¸ç¿’åˆ†æ•£å¼ç³»çµ±åŸç†\n",
    "- å¯¦è¸å¤§è¦æ¨¡è³‡æ–™è™•ç†å°ˆæ¡ˆ\n",
    "- æ¢ç´¢æ©Ÿå™¨å­¸ç¿’èˆ‡ Spark MLlib\n",
    "\n",
    "è¨˜ä½ï¼šé€²éšæŠ€èƒ½éœ€è¦å¤§é‡å¯¦è¸ï¼Œå¤šåšå°ˆæ¡ˆã€å¤šè§£æ±ºå¯¦éš›å•é¡Œæ‰èƒ½çœŸæ­£æŒæ¡ï¼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“š é€²éšæ•™å­¸æ–‡ä»¶çµæŸ - ç¹¼çºŒç²¾é€²ä½ çš„æŠ€èƒ½ï¼ğŸš€\")\n",
    "print(\"=\" * 60)\n",
    "# é—œé–‰ Spark Session\n",
    "# spark.stop()  # å–æ¶ˆè¨»è§£ä»¥é—œé–‰ Spark\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fju",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
