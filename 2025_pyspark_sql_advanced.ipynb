{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56af655b",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "# PySpark SQL 進階教學 🚀\n",
    "\n",
    "\n",
    "這份教學專為已經掌握 SQL 基礎的你設計！\n",
    "我們將深入探討進階的 SQL 技巧，包括視窗函數、CTE、效能優化等主題。\n",
    "讓你的 SQL 技能更上一層樓！✨\n",
    "\n",
    "作者：QChoice AI 教學團隊\n",
    "日期：2025-01-15\n",
    "\n",
    "\n",
    "🚀 歡迎來到 SQL 進階課程！\n",
    "\n",
    "--------------------------------------------------\n",
    "\n",
    "🎯 環境設定 - 準備進階開發環境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f964a89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "# 創建 Spark Session - 配置進階效能參數\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SQL進階教學\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c95ed",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 📚 第一章：視窗函數 (Window Functions)\n",
    "\n",
    "### 1️⃣ ROW_NUMBER - 為每一列分配唯一的序號\n",
    "\n",
    "🎈 概念解釋：\n",
    "ROW_NUMBER() 為分區內的每一列分配一個唯一的序號。\n",
    "常用於排名、去重、分頁等場景。\n",
    "\n",
    "🎯 AI Prompt 範例：\n",
    "請幫我寫一個 PySpark 程式，為每個部門的員工按薪資排序並分配序號。\n",
    "\n",
    "#### 📌 範例 1: ROW_NUMBER - 排名與編號"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9010145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 建立員工資料\n",
    "employees_data = [\n",
    "    (1, \"Alice\", \"工程部\", 75000),\n",
    "    (2, \"Bob\", \"工程部\", 82000),\n",
    "    (3, \"Charlie\", \"工程部\", 68000),\n",
    "    (4, \"David\", \"行銷部\", 65000),\n",
    "    (5, \"Eve\", \"行銷部\", 72000),\n",
    "    (6, \"Frank\", \"行銷部\", 70000),\n",
    "    (7, \"Grace\", \"人資部\", 60000),\n",
    "    (8, \"Henry\", \"人資部\", 63000)\n",
    "]\n",
    "\n",
    "employees_df = spark.createDataFrame(\n",
    "    employees_data, \n",
    "    [\"員工編號\", \"姓名\", \"部門\", \"薪資\"]\n",
    ")\n",
    "\n",
    "# 定義視窗規格：按部門分組，按薪資降序排列\n",
    "windowSpec = Window.partitionBy(\"部門\").orderBy(desc(\"薪資\"))\n",
    "\n",
    "# 使用 ROW_NUMBER\n",
    "result = employees_df.withColumn(\"部門排名\", row_number().over(windowSpec))\n",
    "\n",
    "result.orderBy(\"部門\", \"部門排名\").show()\n",
    "\n",
    "# SQL 寫法\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        `員工編號`,\n",
    "        `姓名`,\n",
    "        `部門`,\n",
    "        `薪資`,\n",
    "        ROW_NUMBER() OVER (PARTITION BY `部門` ORDER BY `薪資` DESC) as `部門排名`\n",
    "    FROM employees\n",
    "    ORDER BY `部門`, `部門排名`\n",
    "\"\"\")\n",
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f590d228",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 2️⃣ RANK & DENSE_RANK - 處理並列排名\n",
    "\n",
    "🎈 概念解釋：\n",
    "RANK() 和 DENSE_RANK() 都用於排名，但處理相同值的方式不同：\n",
    "- RANK(): 相同值得到相同排名，下一個排名會跳號\n",
    "- DENSE_RANK(): 相同值得到相同排名，下一個排名連續\n",
    "\n",
    "🎯 應用場景：\n",
    "成績排名、銷售業績排名等需要處理並列情況的場景\n",
    "\n",
    "#### 📌 範例 2: RANK vs DENSE_RANK - 並列排名處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476a39ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 建立考試成績資料\n",
    "scores_data = [\n",
    "    (1, \"Alice\", 95),\n",
    "    (2, \"Bob\", 95),\n",
    "    (3, \"Charlie\", 90),\n",
    "    (4, \"David\", 90),\n",
    "    (5, \"Eve\", 85),\n",
    "    (6, \"Frank\", 80),\n",
    "]\n",
    "\n",
    "scores_df = spark.createDataFrame(scores_data, [\"學號\", \"姓名\", \"分數\"])\n",
    "\n",
    "# 定義視窗規格\n",
    "windowSpec = Window.orderBy(desc(\"分數\"))\n",
    "\n",
    "result = scores_df.withColumn(\"RANK\", rank().over(windowSpec)) \\\n",
    "                  .withColumn(\"DENSE_RANK\", dense_rank().over(windowSpec)) \\\n",
    "                  .withColumn(\"ROW_NUMBER\", row_number().over(windowSpec))\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691ab04c",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "💡 說明：\n",
    "- Alice 和 Bob 都是 95 分，RANK 和 DENSE_RANK 都是 1\n",
    "- Charlie 和 David 都是 90 分：\n",
    "  * RANK 是 3（因為前面有 2 個人）\n",
    "  * DENSE_RANK 是 2（連續編號）\n",
    "- ROW_NUMBER 為每個人分配唯一編號\n",
    "\n",
    "### 3️⃣ LEAD & LAG - 訪問前後列的資料\n",
    "🎈 概念解釋：\n",
    "LEAD() 和 LAG() 允許你訪問當前列之前或之後的資料：\n",
    "- LAG(): 訪問前面的列\n",
    "- LEAD(): 訪問後面的列\n",
    "\n",
    "🎯 應用場景：\n",
    "計算增長率、同比環比分析、時間序列分析\n",
    "\n",
    "#### 📌 範例 3: LEAD & LAG - 時間序列分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bcf3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 建立月度銷售資料\n",
    "sales_data = [\n",
    "    (\"2024-01\", 100000),\n",
    "    (\"2024-02\", 120000),\n",
    "    (\"2024-03\", 115000),\n",
    "    (\"2024-04\", 130000),\n",
    "    (\"2024-05\", 135000),\n",
    "    (\"2024-06\", 140000),\n",
    "]\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, [\"月份\", \"銷售額\"])\n",
    "\n",
    "# 定義視窗規格\n",
    "windowSpec = Window.orderBy(\"月份\")\n",
    "\n",
    "result = sales_df.withColumn(\"上月銷售額\", lag(\"銷售額\", 1).over(windowSpec)) \\\n",
    "                 .withColumn(\"下月銷售額\", lead(\"銷售額\", 1).over(windowSpec)) \\\n",
    "                 .withColumn(\"月增長額\", col(\"銷售額\") - lag(\"銷售額\", 1).over(windowSpec)) \\\n",
    "                 .withColumn(\"月增長率%\", \n",
    "                            round((col(\"銷售額\") - lag(\"銷售額\", 1).over(windowSpec)) / \n",
    "                                  lag(\"銷售額\", 1).over(windowSpec) * 100, 2))\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee624b43",
   "metadata": {
    "region_name": "md",
    "title": "\\"
   },
   "source": [
    "### 4️⃣ 累計計算 - SUM/AVG OVER\n",
    "🎈 概念解釋：\n",
    "視窗函數配合聚合函數可以進行累計計算，而不會折疊資料列。\n",
    "\n",
    "🎯 應用場景：\n",
    "累計銷售額、移動平均、滾動統計\n",
    "\n",
    "\n",
    "\n",
    "#### 📌 範例 4: 累計計算與移動平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0b40eb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# 繼續使用銷售資料\n",
    "windowSpec = Window.orderBy(\"月份\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "movingAvgSpec = Window.orderBy(\"月份\").rowsBetween(-2, 0)  # 3個月移動平均\n",
    "\n",
    "result = sales_df.withColumn(\"累計銷售額\", sum(\"銷售額\").over(windowSpec)) \\\n",
    "                 .withColumn(\"3月移動平均\", round(avg(\"銷售額\").over(movingAvgSpec), 2))\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b626cd",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 📚 第二章：通用表表達式 (Common Table Expressions - CTE)\n",
    "\n",
    "### 5️⃣ WITH 子句 - 建立臨時結果集\n",
    "🎈 概念解釋：\n",
    "CTE (WITH 子句) 讓你建立臨時的命名結果集，提高查詢的可讀性和可維護性。\n",
    "可以把複雜查詢分解為多個步驟。\n",
    "\n",
    "🎯 優勢：\n",
    "1. 提高可讀性\n",
    "2. 可重複使用\n",
    "3. 便於除錯\n",
    "4. 支援遞迴查詢\n",
    "\n",
    "#### 📌 範例 5: CTE - 簡化複雜查詢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479c670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 建立訂單資料\n",
    "orders_data = [\n",
    "    (1, \"Alice\", \"2024-01-15\", 1500),\n",
    "    (2, \"Bob\", \"2024-01-20\", 2300),\n",
    "    (3, \"Alice\", \"2024-02-10\", 1800),\n",
    "    (4, \"Charlie\", \"2024-02-15\", 3200),\n",
    "    (5, \"Bob\", \"2024-03-05\", 2100),\n",
    "    (6, \"Alice\", \"2024-03-20\", 2500),\n",
    "]\n",
    "\n",
    "orders_df = spark.createDataFrame(\n",
    "    orders_data,\n",
    "    [\"訂單編號\", \"客戶\", \"訂單日期\", \"金額\"]\n",
    ")\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "# 使用 CTE 計算客戶總消費和平均消費\n",
    "sql_with_cte = spark.sql(\"\"\"\n",
    "    WITH customer_stats AS (\n",
    "        SELECT \n",
    "            `客戶`,\n",
    "            COUNT(*) as `訂單數`,\n",
    "            SUM(`金額`) as `總消費`,\n",
    "            AVG(`金額`) as `平均消費`\n",
    "        FROM orders\n",
    "        GROUP BY `客戶`\n",
    "    ),\n",
    "    high_value_customers AS (\n",
    "        SELECT \n",
    "            `客戶`,\n",
    "            `訂單數`,\n",
    "            `總消費`,\n",
    "            ROUND(`平均消費`, 2) as `平均消費`\n",
    "        FROM customer_stats\n",
    "        WHERE `總消費` > 5000\n",
    "    )\n",
    "    SELECT * FROM high_value_customers\n",
    "    ORDER BY `總消費` DESC\n",
    "\"\"\")\n",
    "\n",
    "sql_with_cte.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a176aa19",
   "metadata": {
    "region_name": "md",
    "title": "\\"
   },
   "source": [
    "### 6️⃣ 多層 CTE - 複雜業務邏輯\n",
    "🎈 概念解釋：\n",
    "多層 CTE 可以將複雜的業務邏輯分解為多個清晰的步驟。\n",
    "\n",
    "🎯 應用場景：\n",
    "多步驟的資料轉換、複雜的業務指標計算\n",
    "\n",
    "#### 📌 範例 6: 多層 CTE - 客戶分級分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b718a6c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "sql_multi_cte = spark.sql(\"\"\"\n",
    "    WITH monthly_sales AS (\n",
    "        -- 第一層：計算每個客戶的月度銷售\n",
    "        SELECT \n",
    "            `客戶`,\n",
    "            DATE_FORMAT(TO_DATE(`訂單日期`), 'yyyy-MM') as `月份`,\n",
    "            SUM(`金額`) as `月銷售額`\n",
    "        FROM orders\n",
    "        GROUP BY `客戶`, DATE_FORMAT(TO_DATE(`訂單日期`), 'yyyy-MM')\n",
    "    ),\n",
    "    customer_summary AS (\n",
    "        -- 第二層：匯總客戶統計\n",
    "        SELECT \n",
    "            `客戶`,\n",
    "            COUNT(DISTINCT `月份`) as `活躍月數`,\n",
    "            SUM(`月銷售額`) as `總銷售額`,\n",
    "            AVG(`月銷售額`) as `平均月銷售額`\n",
    "        FROM monthly_sales\n",
    "        GROUP BY `客戶`\n",
    "    ),\n",
    "    customer_level AS (\n",
    "        -- 第三層：客戶分級\n",
    "        SELECT \n",
    "            `客戶`,\n",
    "            `活躍月數`,\n",
    "            ROUND(`總銷售額`, 2) as `總銷售額`,\n",
    "            ROUND(`平均月銷售額`, 2) as `平均月銷售額`,\n",
    "            CASE \n",
    "                WHEN `總銷售額` >= 6000 THEN '白金客戶'\n",
    "                WHEN `總銷售額` >= 4000 THEN '金牌客戶'\n",
    "                ELSE '一般客戶'\n",
    "            END as `客戶等級`\n",
    "        FROM customer_summary\n",
    "    )\n",
    "    SELECT * FROM customer_level\n",
    "    ORDER BY `總銷售額` DESC\n",
    "\"\"\")\n",
    "\n",
    "sql_multi_cte.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3047463",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 📚 第三章：子查詢優化\n",
    "7️⃣ 相關子查詢 vs 非相關子查詢\n",
    "\n",
    "🎈 概念解釋：\n",
    "- 非相關子查詢：內部查詢獨立執行，只執行一次\n",
    "- 相關子查詢：內部查詢依賴外部查詢，可能執行多次（效能較差）\n",
    "\n",
    "🎯 優化建議：\n",
    "盡量使用 JOIN 或視窗函數替代相關子查詢\n",
    "\n",
    "#### 📌 範例 7: 子查詢優化 - 找出高於部門平均薪資的員工"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec11f827",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# 方法 1：使用相關子查詢（較慢）\n",
    "sql_correlated = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e1.`姓名`,\n",
    "        e1.`部門`,\n",
    "        e1.`薪資`,\n",
    "        (SELECT AVG(e2.`薪資`) \n",
    "         FROM employees e2 \n",
    "         WHERE e2.`部門` = e1.`部門`) as `部門平均薪資`\n",
    "    FROM employees e1\n",
    "    WHERE e1.`薪資` > (\n",
    "        SELECT AVG(e2.`薪資`) \n",
    "        FROM employees e2 \n",
    "        WHERE e2.`部門` = e1.`部門`\n",
    "    )\n",
    "    ORDER BY e1.`部門`, e1.`薪資` DESC\n",
    "\"\"\")\n",
    "\n",
    "sql_correlated.show()\n",
    "\n",
    "# 方法 2：使用 JOIN（較快）\n",
    "sql_join = spark.sql(\"\"\"\n",
    "    WITH dept_avg AS (\n",
    "        SELECT `部門`, AVG(`薪資`) as `平均薪資`\n",
    "        FROM employees\n",
    "        GROUP BY `部門`\n",
    "    )\n",
    "    SELECT \n",
    "        e.`姓名`,\n",
    "        e.`部門`,\n",
    "        e.`薪資`,\n",
    "        ROUND(d.`平均薪資`, 2) as `部門平均薪資`\n",
    "    FROM employees e\n",
    "    JOIN dept_avg d ON e.`部門` = d.`部門`\n",
    "    WHERE e.`薪資` > d.`平均薪資`\n",
    "    ORDER BY e.`部門`, e.`薪資` DESC\n",
    "\"\"\")\n",
    "\n",
    "sql_join.show()\n",
    "\n",
    "# 方法 3：使用視窗函數（最快）\n",
    "windowSpec = Window.partitionBy(\"部門\")\n",
    "result_window = employees_df.withColumn(\"部門平均薪資\", round(avg(\"薪資\").over(windowSpec), 2)) \\\n",
    "                            .filter(col(\"薪資\") > col(\"部門平均薪資\")) \\\n",
    "                            .orderBy(\"部門\", desc(\"薪資\"))\n",
    "\n",
    "result_window.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d188b10",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 📚 第四章：進階 JOIN 技巧\n",
    "\n",
    "\n",
    "### 8️⃣ CROSS JOIN - 笛卡爾積\n",
    "🎈 概念解釋：\n",
    "CROSS JOIN 產生兩個表的笛卡爾積，即所有可能的組合。\n",
    "\n",
    "🎯 應用場景：\n",
    "生成測試資料、建立時間序列、產生所有可能的組合\n",
    "\n",
    "#### 📌 範例 8: CROSS JOIN - 產生所有可能的配對"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86116881",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 建立產品和顏色資料\n",
    "products = spark.createDataFrame([(\"手機\",), (\"平板\",), (\"筆電\",)], [\"產品\"])\n",
    "colors = spark.createDataFrame([(\"黑色\",), (\"白色\",), (\"銀色\",)], [\"顏色\"])\n",
    "\n",
    "# CROSS JOIN\n",
    "cross_result = products.crossJoin(colors)\n",
    "\n",
    "cross_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a895f",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 9️⃣ SELF JOIN - 自我連接\n",
    "🎈 概念解釋：\n",
    "SELF JOIN 是表與自己進行連接，常用於查找層級關係或比較同表內的記錄。\n",
    "\n",
    "🎯 應用場景：\n",
    "員工-主管關係、組織架構、尋找重複記錄\n",
    "\n",
    "#### 📌 範例 9: SELF JOIN - 員工與主管關係"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2661a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 建立包含主管資訊的員工資料\n",
    "emp_manager_data = [\n",
    "    (1, \"Alice\", None),      # CEO，沒有主管\n",
    "    (2, \"Bob\", 1),           # Bob 的主管是 Alice\n",
    "    (3, \"Charlie\", 1),       # Charlie 的主管是 Alice\n",
    "    (4, \"David\", 2),         # David 的主管是 Bob\n",
    "    (5, \"Eve\", 2),           # Eve 的主管是 Bob\n",
    "    (6, \"Frank\", 3),         # Frank 的主管是 Charlie\n",
    "]\n",
    "\n",
    "emp_manager_df = spark.createDataFrame(\n",
    "    emp_manager_data,\n",
    "    [\"員工編號\", \"姓名\", \"主管編號\"]\n",
    ")\n",
    "emp_manager_df.createOrReplaceTempView(\"emp_manager\")\n",
    "\n",
    "# 使用 SELF JOIN 查詢員工和其主管\n",
    "sql_self_join = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e.`員工編號`,\n",
    "        e.`姓名` as `員工姓名`,\n",
    "        COALESCE(m.`姓名`, '無主管') as `主管姓名`\n",
    "    FROM emp_manager e\n",
    "    LEFT JOIN emp_manager m ON e.`主管編號` = m.`員工編號`\n",
    "    ORDER BY e.`員工編號`\n",
    "\"\"\")\n",
    "\n",
    "sql_self_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c369b3ba",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 🔟 ANTI JOIN & SEMI JOIN - 高效過濾\n",
    "\n",
    "🎈 概念解釋：\n",
    "- SEMI JOIN (LEFT SEMI): 返回左表中在右表中有匹配的記錄\n",
    "- ANTI JOIN (LEFT ANTI): 返回左表中在右表中沒有匹配的記錄\n",
    "\n",
    "🎯 優勢：\n",
    "比 IN / NOT IN 子查詢效能更好\n",
    "\n",
    "#### 📌 範例 10: ANTI JOIN & SEMI JOIN - 客戶訂單分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcac099f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# 建立客戶資料\n",
    "customers_data = [\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (3, \"Charlie\"),\n",
    "    (4, \"David\"),\n",
    "    (5, \"Eve\"),\n",
    "]\n",
    "\n",
    "customers_df = spark.createDataFrame(customers_data, [\"客戶編號\", \"客戶姓名\"])\n",
    "\n",
    "# 建立訂單資料（只有部分客戶有訂單）\n",
    "order_customers = orders_df.select(\"客戶\").distinct()\n",
    "\n",
    "# SEMI JOIN - 有訂單的客戶\n",
    "customers_with_orders = customers_df.join(\n",
    "    order_customers,\n",
    "    customers_df[\"客戶姓名\"] == order_customers[\"客戶\"],\n",
    "    \"leftsemi\"\n",
    ")\n",
    "\n",
    "customers_with_orders.show()\n",
    "\n",
    "# ANTI JOIN - 沒有訂單的客戶\n",
    "customers_without_orders = customers_df.join(\n",
    "    order_customers,\n",
    "    customers_df[\"客戶姓名\"] == order_customers[\"客戶\"],\n",
    "    \"leftanti\"\n",
    ")\n",
    "\n",
    "customers_without_orders.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8437164a",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 📚 第五章：效能優化技巧\n",
    "### 1️⃣1️⃣ Broadcast Join - 廣播小表\n",
    "\n",
    "🎈 概念解釋：\n",
    "當一個大表和一個小表進行 JOIN 時，可以將小表廣播到所有節點，\n",
    "避免 shuffle 操作，大幅提升效能。\n",
    "\n",
    "🎯 適用場景：\n",
    "小表 < 10MB，大表與維度表 JOIN\n",
    "\n",
    "#### 📌 範例 11: Broadcast Join - 優化大小表連接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f44c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 建立部門資訊（小表）\n",
    "departments_data = [\n",
    "    (\"工程部\", \"技術大樓\"),\n",
    "    (\"行銷部\", \"行政大樓\"),\n",
    "    (\"人資部\", \"行政大樓\"),\n",
    "]\n",
    "\n",
    "departments_df = spark.createDataFrame(departments_data, [\"部門\", \"辦公地點\"])\n",
    "\n",
    "# 一般 JOIN\n",
    "normal_join = employees_df.join(departments_df, \"部門\")\n",
    "\n",
    "normal_join.select(\"姓名\", \"部門\", \"辦公地點\").show(5)\n",
    "\n",
    "# Broadcast JOIN\n",
    "broadcast_join = employees_df.join(\n",
    "    broadcast(departments_df),\n",
    "    \"部門\"\n",
    ")\n",
    "\n",
    "broadcast_join.select(\"姓名\", \"部門\", \"辦公地點\").show(5)\n",
    "\n",
    "# 查看執行計畫\n",
    "print(\"一般 JOIN 會有 SortMergeJoin\")\n",
    "print(\"Broadcast JOIN 會有 BroadcastHashJoin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ce0a5",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 1️⃣2️⃣ 分區與分桶 - 資料組織優化\n",
    "\n",
    "🎈 概念解釋：\n",
    "- Partitioning: 按欄位值將資料分割成多個目錄\n",
    "- Bucketing: 按 hash 值將資料分割成固定數量的檔案\n",
    "\n",
    "🎯 優勢：\n",
    "減少掃描的資料量，提升查詢效能\n",
    "\n",
    "#### 📌 範例 12: 分區與過濾優化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502f195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 重新分區資料\n",
    "employees_partitioned = employees_df.repartition(2, \"部門\")\n",
    "\n",
    "\n",
    "# 使用分區欄位過濾（效能更好）\n",
    "filtered = employees_partitioned.filter(col(\"部門\") == \"工程部\")\n",
    "filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d059b3d5",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "\n",
    "### 1️⃣3️⃣ 快取與持久化 - 避免重複計算\n",
    "\n",
    "🎈 概念解釋：\n",
    "將常用的 DataFrame 快取到記憶體中，避免重複計算。\n",
    "\n",
    "🎯 適用場景：\n",
    "多次使用相同的中間結果、迭代計算\n",
    "\n",
    "#### 📌 範例 13: 快取優化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef55a2f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# 建立一個複雜的計算\n",
    "complex_df = employees_df.groupBy(\"部門\") \\\n",
    "                        .agg(\n",
    "                            count(\"*\").alias(\"人數\"),\n",
    "                            avg(\"薪資\").alias(\"平均薪資\"),\n",
    "                            max(\"薪資\").alias(\"最高薪資\")\n",
    "                        )\n",
    "\n",
    "# 快取結果\n",
    "complex_df.cache()\n",
    "\n",
    "# 第一次執行（會觸發計算並快取）\n",
    "start = time.time()\n",
    "complex_df.show()\n",
    "print(f\"執行時間: {time.time() - start:.4f} 秒\")\n",
    "\n",
    "# 第二次執行（從快取讀取）\n",
    "start = time.time()\n",
    "complex_df.show()\n",
    "print(f\"執行時間: {time.time() - start:.4f} 秒（應該更快）\")\n",
    "\n",
    "# 釋放快取\n",
    "complex_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7895cc6",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 📚 第六章：複雜資料轉換\n",
    "### 1️⃣4️⃣ PIVOT - 行轉列\n",
    "\n",
    "🎈 概念解釋：\n",
    "PIVOT 將行資料轉換為列，常用於建立交叉表。\n",
    "\n",
    "🎯 應用場景：\n",
    "銷售報表、資料透視表、趨勢分析\n",
    "\n",
    "#### 📌 範例 14: PIVOT - 建立部門薪資透視表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f6ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 使用 PIVOT 建立部門-統計指標透視表\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "pivot_result = spark.sql(\"\"\"\n",
    "    SELECT * FROM (\n",
    "        SELECT `部門`, `薪資`\n",
    "        FROM employees\n",
    "    )\n",
    "    PIVOT (\n",
    "        COUNT(*) as `人數`,\n",
    "        AVG(`薪資`) as `平均薪資`\n",
    "        FOR `部門` IN ('工程部', '行銷部', '人資部')\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "pivot_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b27372",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "\n",
    "### 1️⃣5️⃣ UNPIVOT - 列轉行\n",
    "\n",
    "🎈 概念解釋：\n",
    "UNPIVOT 將列資料轉換為行，是 PIVOT 的反向操作。\n",
    "\n",
    "🎯 應用場景：\n",
    "將寬表轉換為長表、資料正規化\n",
    "\n",
    "#### 📌 範例 15: UNPIVOT - 列轉行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81ccac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 建立寬表格式的季度銷售資料\n",
    "quarterly_sales = spark.createDataFrame([\n",
    "    (\"產品A\", 100, 120, 115, 130),\n",
    "    (\"產品B\", 80, 90, 95, 100),\n",
    "], [\"產品\", \"Q1\", \"Q2\", \"Q3\", \"Q4\"])\n",
    "\n",
    "quarterly_sales.createOrReplaceTempView(\"quarterly_sales\")\n",
    "\n",
    "# 使用 UNPIVOT（Spark 3.4+）或使用 stack 函數\n",
    "unpivot_result = quarterly_sales.selectExpr(\n",
    "    \"`產品`\",\n",
    "    \"stack(4, 'Q1', `Q1`, 'Q2', `Q2`, 'Q3', `Q3`, 'Q4', `Q4`) as (`季度`, `銷售額`)\"\n",
    ")\n",
    "\n",
    "unpivot_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0453e6fd",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "\n",
    "### 1️⃣6️⃣ 陣列與結構處理\n",
    "\n",
    "🎈 概念解釋：\n",
    "PySpark 支援複雜的資料型態如陣列、結構體、地圖等。\n",
    "\n",
    "🎯 應用場景：\n",
    "處理 JSON 資料、巢狀結構、多值欄位\n",
    "\n",
    "#### 📌 範例 16: 陣列操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892aac92",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# 建立包含陣列的資料\n",
    "students_data = [\n",
    "    (1, \"Alice\", [\"數學\", \"物理\", \"化學\"]),\n",
    "    (2, \"Bob\", [\"英文\", \"歷史\"]),\n",
    "    (3, \"Charlie\", [\"數學\", \"英文\", \"體育\"]),\n",
    "]\n",
    "\n",
    "students_df = spark.createDataFrame(\n",
    "    students_data,\n",
    "    [\"學號\", \"姓名\", \"選修課程\"]\n",
    ")\n",
    "\n",
    "students_df.show(truncate=False)\n",
    "\n",
    "# 展開陣列\n",
    "exploded = students_df.select(\n",
    "    \"學號\",\n",
    "    \"姓名\",\n",
    "    explode(\"選修課程\").alias(\"課程\")\n",
    ")\n",
    "\n",
    "exploded.show()\n",
    "\n",
    "# 陣列相關函數\n",
    "array_operations = students_df.select(\n",
    "    \"姓名\",\n",
    "    \"選修課程\",\n",
    "    size(\"選修課程\").alias(\"課程數\"),\n",
    "    array_contains(\"選修課程\", \"數學\").alias(\"是否選修數學\")\n",
    ")\n",
    "\n",
    "array_operations.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb50581",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 📚 第七章：資料品質與清理\n",
    "### 1️⃣7️⃣ 處理 NULL 值\n",
    "\n",
    "🎈 概念解釋：\n",
    "NULL 值處理是資料清理的重要環節。\n",
    "\n",
    "🎯 常用方法：\n",
    "fillna, dropna, coalesce, nvl\n",
    "\n",
    "#### 📌 範例 17: NULL 值處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba11916",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 建立包含 NULL 的資料\n",
    "data_with_null = [\n",
    "    (1, \"Alice\", 75000, \"工程部\"),\n",
    "    (2, \"Bob\", None, \"行銷部\"),\n",
    "    (3, \"Charlie\", 68000, None),\n",
    "    (4, None, 65000, \"人資部\"),\n",
    "]\n",
    "\n",
    "df_null = spark.createDataFrame(\n",
    "    data_with_null,\n",
    "    [\"員工編號\", \"姓名\", \"薪資\", \"部門\"]\n",
    ")\n",
    "\n",
    "df_null.show()\n",
    "\n",
    "# 方法 1: 填充預設值\n",
    "filled = df_null.fillna({\n",
    "    \"姓名\": \"未知\",\n",
    "    \"薪資\": 60000,\n",
    "    \"部門\": \"待分配\"\n",
    "})\n",
    "\n",
    "filled.show()\n",
    "\n",
    "# 方法 2: 刪除包含 NULL 的列\n",
    "dropped = df_null.dropna()\n",
    "\n",
    "dropped.show()\n",
    "\n",
    "# 方法 3: 使用 COALESCE\n",
    "coalesced = df_null.select(\n",
    "    \"員工編號\",\n",
    "    coalesce(col(\"姓名\"), lit(\"未知\")).alias(\"姓名\"),\n",
    "    coalesce(col(\"薪資\"), lit(60000)).alias(\"薪資\"),\n",
    "    coalesce(col(\"部門\"), lit(\"待分配\")).alias(\"部門\")\n",
    ")\n",
    "\n",
    "coalesced.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84daa24",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "\n",
    "### 1️⃣8️⃣ 資料去重\n",
    "\n",
    "🎈 概念解釋：\n",
    "找出並移除重複的記錄。\n",
    "\n",
    "🎯 方法：\n",
    "distinct, dropDuplicates, 視窗函數\n",
    "\n",
    "#### 📌 範例 18: 資料去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112c71a7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# 建立包含重複資料\n",
    "duplicate_data = [\n",
    "    (1, \"Alice\", \"工程部\"),\n",
    "    (2, \"Bob\", \"行銷部\"),\n",
    "    (1, \"Alice\", \"工程部\"),  # 完全重複\n",
    "    (3, \"Alice\", \"人資部\"),  # 姓名重複\n",
    "]\n",
    "\n",
    "df_dup = spark.createDataFrame(\n",
    "    duplicate_data,\n",
    "    [\"員工編號\", \"姓名\", \"部門\"]\n",
    ")\n",
    "\n",
    "df_dup.show()\n",
    "\n",
    "# 方法 1: 完全去重\n",
    "dedup_all = df_dup.distinct()\n",
    "dedup_all.show()\n",
    "\n",
    "# 方法 2: 按特定欄位去重（保留第一筆）\n",
    "dedup_by_name = df_dup.dropDuplicates([\"姓名\"])\n",
    "dedup_by_name.show()\n",
    "\n",
    "# 方法 3: 使用視窗函數保留特定規則的記錄\n",
    "windowSpec = Window.partitionBy(\"姓名\").orderBy(\"員工編號\")\n",
    "dedup_window = df_dup.withColumn(\"rn\", row_number().over(windowSpec)) \\\n",
    "                     .filter(col(\"rn\") == 1) \\\n",
    "                     .drop(\"rn\")\n",
    "\n",
    "dedup_window.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99a76eb",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 📚 第八章：進階分析函數\n",
    "### 1️⃣9️⃣ NTILE - 資料分組\n",
    "\n",
    "🎈 概念解釋：\n",
    "NTILE 將資料分成 N 個大致相等的組。\n",
    "\n",
    "🎯 應用場景：\n",
    "客戶分層、ABC 分析、分位數分析\n",
    "\n",
    "#### 📌 範例 19: NTILE - 薪資四分位數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738d2299",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "windowSpec = Window.orderBy(\"薪資\")\n",
    "\n",
    "ntile_result = employees_df.withColumn(\n",
    "    \"薪資分組\",\n",
    "    ntile(4).over(windowSpec)\n",
    ").withColumn(\n",
    "    \"分組說明\",\n",
    "    when(col(\"薪資分組\") == 1, \"低薪組\")\n",
    "    .when(col(\"薪資分組\") == 2, \"中低薪組\")\n",
    "    .when(col(\"薪資分組\") == 3, \"中高薪組\")\n",
    "    .otherwise(\"高薪組\")\n",
    ")\n",
    "\n",
    "ntile_result.orderBy(\"薪資\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8894eb",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "\n",
    "### 2️⃣0️⃣ PERCENT_RANK - 百分位排名\n",
    "\n",
    "🎈 概念解釋：\n",
    "PERCENT_RANK 計算值在資料集中的相對位置（0 到 1 之間）。\n",
    "\n",
    "🎯 應用場景：\n",
    "績效評估、成績分析、相對排名\n",
    "\n",
    "#### 📌 範例 20: PERCENT_RANK - 薪資百分位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fe5bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "windowSpec = Window.orderBy(\"薪資\")\n",
    "\n",
    "percent_result = employees_df.withColumn(\n",
    "    \"薪資百分位\",\n",
    "    round(percent_rank().over(windowSpec) * 100, 2)\n",
    ").withColumn(\n",
    "    \"說明\",\n",
    "    concat(\n",
    "        lit(\"薪資超過 \"),\n",
    "        round(percent_rank().over(windowSpec) * 100, 0).cast(\"int\"),\n",
    "        lit(\"% 的員工\")\n",
    "    )\n",
    ")\n",
    "\n",
    "percent_result.orderBy(\"薪資\").select(\"姓名\", \"薪資\", \"薪資百分位\", \"說明\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4f9f0e",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "# 🎓 課程總結\n",
    "## 🎊 恭喜！你已經完成了 SQL 進階課程！\n",
    "📝 進階技能總結：\n",
    "\n",
    "1️⃣ 視窗函數：\n",
    "   - ROW_NUMBER, RANK, DENSE_RANK：排名與編號\n",
    "   - LEAD, LAG：訪問前後列資料\n",
    "   - SUM/AVG OVER：累計計算與移動平均\n",
    "   - NTILE, PERCENT_RANK：分組與百分位\n",
    "\n",
    "2️⃣ 通用表表達式 (CTE)：\n",
    "   - WITH 子句：提高查詢可讀性\n",
    "   - 多層 CTE：分解複雜邏輯\n",
    "   - 遞迴 CTE：處理層級關係\n",
    "\n",
    "3️⃣ 子查詢優化：\n",
    "   - 避免相關子查詢\n",
    "   - 使用 JOIN 替代\n",
    "   - 使用視窗函數優化\n",
    "\n",
    "4️⃣ 進階 JOIN：\n",
    "   - CROSS JOIN：笛卡爾積\n",
    "   - SELF JOIN：自我連接\n",
    "   - SEMI/ANTI JOIN：高效過濾\n",
    "\n",
    "5️⃣ 效能優化：\n",
    "   - Broadcast Join：廣播小表\n",
    "   - 分區與分桶：資料組織\n",
    "   - 快取與持久化：避免重複計算\n",
    "\n",
    "6️⃣ 複雜資料轉換：\n",
    "   - PIVOT/UNPIVOT：行列轉換\n",
    "   - 陣列與結構：處理複雜型態\n",
    "   - 資料展開與聚合\n",
    "\n",
    "7️⃣ 資料品質：\n",
    "   - NULL 值處理\n",
    "   - 資料去重\n",
    "   - 資料驗證\n",
    "\n",
    "💡 進階學習建議：\n",
    "- 理解執行計畫，學會效能分析\n",
    "- 掌握資料分區策略\n",
    "- 熟悉 Spark UI 的使用\n",
    "- 實踐 Delta Lake 進階功能\n",
    "- 學習 Spark Streaming\n",
    "\n",
    "🚀 下一步：\n",
    "- 深入研究 Spark 內部機制\n",
    "- 學習分散式系統原理\n",
    "- 實踐大規模資料處理專案\n",
    "- 探索機器學習與 Spark MLlib\n",
    "\n",
    "記住：進階技能需要大量實踐，多做專案、多解決實際問題才能真正掌握！\n",
    "# 📚 進階教學文件結束 - 繼續精進你的技能！🚀\n",
    "\n",
    "關閉 Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6840a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,region_name,-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
